---
date: 2012-08-04
url: /blog/2012/08/04/autocomplete.html
title: Толерантный автокомплит
layout: post
tags: [algorithms]
math: true
---

Автокомплит вещь удобная. Он позволяет экономить время на наборе текста, когда множество значений поля закрыто. Хороший автокомплит отличается следующими качествами:

* он должен быть быстрый. Если мы хотим экономить силы пользователя, то мы должны ему предложить вариант как можно быстрее;
* он не должен предлагать к вводу варианты которые заведомо неверны;
* _он должен быть толерантен к пользовательскому вводу_. В том числе прощать опечатки.

Вот о последнем качестве autocomplete алгоритмов я и хочу сегодня поговорить.

Если вы хотите создать приложение толерантное к вводимой пользователем информации, то исправление опечаток в том или ином виде должно быть. Человек может опечататься в первой букве, может попросту не знать как правильно пишется слово. Поэтому, не очень хорошо что первая же неверно введенная пользователем буква приведет к тому что он не сможет выбрать правильный вариант.

Я значительную часть рабочего дня провожу в IntelliJ IDEA. Отличная среда, пожалуй лучшая. А еще, я [использую guava][ref-functional-java], хорошая библиотека. И есть там такой метод `Closeables.closeQuietly()`, который закрывает `Closeable` объект подавляя все исключения. Так вот, когда вы пишете имя метода которого нет в текущем классе, IDEA автоматически предлагает вам сделать `static import` подходящего метода из других классов.

![IntelliJ IDEA static import](/images/autocomplete/idea-good.png)

Прекрасная фича, которая не работает если вы опечатались. А я ну никак не могу запомнить как правильно пишется слово "quietly".

![IntelliJ IDEA static import reject](/images/autocomplete/idea-bad.png)

В этом случае IDE "фейлит" и предлагает мне создать новый метод. Примерно то же самое происходит и с автодополнением. Стоит мне ввести хотя бы один не тот символ, как тут же "No suggestions". Я считаю, IDEA могла бы быть гораздо более толерантна к девелоперу на этапе ввода кода. Это же не какой-нибудь там NetBeans :)

Этой же проблемой страдает большое количество продуктов.

{{< fig "Встроенный в Mac OS X поиск по позиция системного меню" >}}
![Mac OS X Help Search](/images/autocomplete/mac-help.png)
{{</ fig >}}

{{< fig "Mac OS X Spotlight" >}}
![Mac OS X Spotlight Search](/images/autocomplete/mac-spotlight.png)
{{</ fig >}}

{{< fig "Money — ПО для учета персональных финансов" >}}
![Money](/images/autocomplete/money.png)
{{</ fig >}}

Такая ситуация выглядит для меня довольно странно, так как алгоритмы позволяющие разруливать большую часть опечаток при вводе довольно банальны.

Конечно, разработка первоклассного spellchecker'а дело не простое. Оно требует обучения не тривиальных статистических моделей. Но есть несколько простых подходов, которые позволяют в большинстве приложений сделать автокомплит достаточно толерантным, чтобы пользователи были счастливы в 80% случаев.

## Подход №1: Редакционное расстояние

Самое простое решение заключается в том чтобы сортировать все позиции словаря по возрастанию редакционного расстояния и показывать только первые несколько позиций. В качестве редакционного расстояния можно взять [расстояние Левенштейна][ref-levenshtein-distance]. Расстояние Левенштейна – это минимальное количество операций вставок/удаления/изменения символов необходимое для того чтобы преобразовать исходную строку в целевую.

{{< notice note >}}
Список слов близких к слову "пазор" и их редакционные расстояния:
* позор &rarr; 1
* позер &rarr; 2
* дозор &rarr; 2
* помор &rarr; 2
* побор &rarr; 2
* подзор &rarr; 2
* покер &rarr; 3
* покос &rarr; 3
{{</ notice >}}

Существует статистика что в пределах расстояния Левенштейна 2 находится более 90% опечаток, так что можно показывать только их, чтобы избежать совсем уж неадекватных исправлений.

### Достоинства

* довольно проста в реализации. Имплементацию расчёта расстояния Левенштейна можно найти практически для любого языка программирования.

### Недостатки

* Довольно ресурсоемкая. Алгоритмическая сложность алгоритма O(n<sup>2</sup>). Если в вашем словаре много позиций или у вас много посетителей на данное решение может не хватить аппаратных ресурсов.
* в данном виде этот подход не позволяет использовать знания о языковой модели, что не дает корректно исправлять некоторые типичные опечатки. Например опечатка "corola" находится на расстоянии 1 и от "corolla" и от "corona", соответственно две эти замены будут равновероятны. Но пропуск дублирующей "l" гораздо более вероятная опечатка чем замена "l" на "n".

В целом, решение на основании расстояния Левенштейна будет вполне сносно работать в первом приближении. Его очень просто реализовать, что делает данный подход отличным кандидатом на роль прототипа.

## Подход №2: Коэффициент Жаккара и K-gram индекс

Альтернативный подход заключается в том чтобы использовать [коэффициент Жаккара][ref-jaccard-coeff] как метрику расстояния между строками. Коэффициент Жаккара (Jaccard index) это индекс сходства двух множеств который определяется как отношение мощности пересечения этих множеств к мощности их объединения.

![Sets](/images/autocomplete/sets.png)

$$ J = \frac{\mid A \cap B \mid} {\mid A \cup B \mid} $$

При коэффициенте Жаккара равном 1 множества равны, при 0 не имеют ни одного общего элемента. Эту метрику можно использовать для оценки близости вводимых пользователем слов к словам из нашего словаря по которому мы делаем autocomplete.

Но для того чтобы иметь возможность расчитать коэффициент Жаккара для двух строк нам надо преобразовать их в множества. Самый простой вариант, разбить строку на символы. Но обычно берут не символы строки, а k-граммы строки (также известны как n-граммы). [K-грамма][ref-ngram] – это непрерывная уникальная последовательность из k символов строки. Дублирующие k-граммы пропускают. Например, в слове "клоун" 3 триграммы (k = 3): "кло", "лоу" и "оун". На практике, k выбирают равным в диапазоне от двух до четырех, в зависимости от характера данных.

Cформировав из строк множество k-грамм, мы можем расчитать коэффициент Жаккара между этими строками. Например, триграммный коэффициент Жаккара для пары "corolla" и "corola" будет выше, чем для пары "corona" и "corola". В первом случае совпадают три триграммы: "cor", "oro" и "rol", а во втором случае только две: "cor" и "oro".

В контексте алгоритма автодополнения, для того чтобы иметь возможность быстро подбирать кандидатов на пользователский запрос, необходимо построить [k-gram индекс][ref-k-gram-index] по словарю автодополнения. K-gram индекс это инвертированный индекс из k-граммы в слова содержащие эту k-грамму. K-gram индекс позволяет  быстро находить элементы с наиболее высоким коэффициентом Жаккара предварительно проиндексировав их.

{{< notice note >}}
Биграммный индекс (k = 2) для слов: "топор", "компот" и "оптом" выглядит следующим образом:

* то &rarr; оптом, топор
* оп &rarr; оптом, топор
* по &rarr; компот, топор
* ор &rarr; топор
* ко &rarr; компот
* ом &rarr; компот, оптом
* мп &rarr; компот
* от &rarr; компот
* пт &rarr; оптом
{{</ notice >}}

Имея k-gramm индекс алгоритм автодополнения сводится к следующим шагам:

1. разбить на k-граммы строку введенную пользователем;
1. для каждой k-граммы получить список слов в которых она встречается (posting list);
1. сделать merge posting list'ов с подсчетом какое количество раз каждое слово встречается в них. По факту это количество совпавших k-грамм между строкой пользователя и позицией словаря.
1. отсортировать этот список по убыванию количества совпавших k-грамм;
1. вернуть первые N позиций пользователю.

### Достоинства

* данная подход позволяет эффективно находить строки синтаксически близкие к целевой (с максимальным количеством пересекающихся k-грамм). Существенно быстрее чем редакционное расстояние.

### Недостатки

* как и редакционное расстояние, не позволяет использовать информацию языковой модели, что не позволяет исправлять сложные типы опечаток;
* более сложна в реализации чем подход с редакционным расстоянием.

## Tips & tricks

Некоторые замечания которые могут быть полезны, если вы захотите имплементировать толерантный автокомплит.

* K-gram индекс можно хранить в любой удобной для вас inverted index системе. Например, Apache Lucene;
* подходы №1 и №2 не являются взаимоисключающими. Очень часто K-gram индекс из за его эффективности используют как предварительный фильтр для других, более сложных алгоритмов;
* может иметь смысл предварительно убирать из текста все символы не несущие смысловой нагрузки. Например, дефисы, слэши и т.д;
* при построении k-gram из слова/фразы имеет смысл добавить в исходную строку какой-нибудь специальный символ (например, "$") на границе слов. Это сделает результаты более точным, за счет дополнительной совпадающей k-граммы на границах.

## В заключениe
Мы рассмотрели два основных инструмента которые позволяют находить и исправлять опечатки. Они могут быть использованы как в чистом виде, так и в составе более сложных стратегий автодополнения. Надеюсь, эта информация даст вам общее представление о том как сделать более толерантное к опечаткам автодополнение.

[ref-jaccard-coeff]: http://ru.wikipedia.org/wiki/Коэффициент_Жаккара
[ref-functional-java]: /blog/2012/05/12/functional-java.html
[ref-levenshtein-distance]: http://ru.wikipedia.org/wiki/Расстояние_Левенштейна
[ref-ngram]: http://en.wikipedia.org/wiki/N-gram
[ref-k-gram-index]: http://nlp.stanford.edu/IR-book/html/htmledition/k-gram-indexes-for-spelling-correction-1.html
