---
date: 2012-06-05
url: /blog/2012/06/05/classification.html
title: О задачах классификации
layout: post
tags: [information rerieval, classification]
---
В этом и следующих постах, я хочу на пальцах описать процесс создания простого классификатора текстовых документов, а также рассказать о некоторых нетипичных с обывательской точки зрения подходах используемых при классификации документов.

[Классификатор][ref-classifier] – это алгоритм соотносящий некие входные данные с одним или несколькими классами. В отличие от алгоритмов [кластеризации][ref-clustering] эти классы должны быть определены заранее. 

Возможно, кому-то это определение покажется слишком общими или академическим, поэтому лучше наверное рассмотреть задачу классификации на примерах. А примеров хоть отбавляй.

## Они повсюду

Пожалуй самый яркий пример автоматической классификации – это фильтрация спама. Каждый день на мой ящик падает десятки если не сотни спам-писем, которые автоматически отфильтровываются из моего inbox'а.

![Mail Spam](/images/classification/spam.png)

Современные коммерческие системы способны успешно фильтровать спам с точностью превышающей 99%[^google-spam-filter-performance]. Другим довольно типичным примером классификации служит автоматическое определение тематики того или иного текста. Некоторые новостные аггрераторы используют подобный подход для группировки новостей в направления: экономика, политика, общественная жизнь и т.д.

Зачастую классификация является фундаментом на котором строятся алгоритмы решения более сложных задач.
Например, классификация используется при создании рекомендательных систем и в частности при реализации [коллаборативной фильтрации][ref-collaborative-filtering].

![Amazon recommendations](/images/classification/recommendation.png)

Safari Reader Mode является еще одним примером где используются алгоритмы классификации для достижения конечной цели. Суть этого режима работы браузера заключается в том что он позволяет автоматически убрать со страницы всю шелуху не имеющую отношения к сути контента страницы[^boilerplate-paper].

![Safari Reader Mode](/images/classification/reader-mode.png)

Так же классификация используется в задачах face detection'а и face recognition'а.

![Face Detection in Aperture](/images/classification/face-detection.png)

Классификация используется как инструмент для решения множества других задач:

* снятие омонимии при обработке натуральных языков;
* в поисковых системах – для ограничения области поиска в целях повышения точности (вертикальный поиск);
* автоматическое определение языка на котором написан текст;
* анализ тональности (определение эмоциональной окраски текста).

Этот список можно продолжать еще долго. Например, в медицине алгоритмы классификации используются для реконструирования 3D модели головного мозга по серии МРТ снимков[^mri-3d], а также для диагностики пациентов страдающих синдромом Альцгеймера[^alzheimer].

## Традиционные подходы

### Rule based classification

Если говорить о задаче классификации текстов, то пожалуй ее традиционным решением является классификация основная на правилах (rule based classification). Вы имплементируете правила определения класса документа по его тексту в виде `if-then-else` выражений (код на Scala).

```scala
def classify(text: String) =
	if (text.contains("виагра") || text.contains("бухгалтер")) "SPAM" else "NOT SPAM"
```

Этот подход может быть хорошим вариантом если вы работаете с небольшой коллекцией документов которую вы способны охватить и тщательно проанализировать. Просто потому что вы четко контролируете правила по которым классификатор принимает решения. Но есть у этого подхода и очевидные минусы:

* для того чтобы выбрать значимые для классификации слова необходимо обладать экспертными знаниями в предметной области. Есть ли у вас например соображения по поводу ключевых слов которые хорошо отличают документы посвященные финансовой тематике от документов экономической? У меня очень смутные;
* отнюдь не всегда факт наличия или отсутствия какого-либо одного слова является решающим фактором для принятия решения.

Поподробней остановлюсь на последнем пункте. Если вернуться к задаче определения спама и немного подумать о том какие слова являются хорошими классификационными признаками (classifying feature), то станет понятно что нет такого слова наличие которого гарантировало бы что сообщение является спамом. Возможно, в пределах компании производящей [силденафил][ref-sildenafil] в промышленных масштабах слово "виагра" не является показательным признаком спам-сообщения, кто знает.

В общем, суть такова: _любое из известных спам-слов пусть редко но встречается в повседневной жизни._ Поэтому, принимать окончательно решение основываясь на факте наличия или отсутствия какого-либо одного слова идея контрпродуктивная. Мы можем усложнять правила добавляя вложенные `if`'ы. Но довольно быстро вы поймете что возможности человека в формулировании таких правил очень ограничены, потому что _сложность правил растет экспоненциально с количеством выбранных для классификации слов_.

### Weight based classification

Мы можем пойти другим путем. Мы можем для каждого слова выбрать некий вес, который будет означать насколько вероятно что сообщение с этим словом является спамом (0 – никогда не является спамом, 1 – всегда спам).


|           | spam | not spam |
|-----------|------|----------|
| бухгалтер | 0.99 |   0.01   |
| виагра    | 0.99 |   0.01   |
| выгодное  | 0.70 |   0.30   |
| github    | 0.01 |   0.99   |

В этой таблице перечислены гипотетические веса для четырех слов. Сумма значений в строке должна быть равна единице. Тогда наша классификация может выглядеть следующим образом:

```scala
def classify(text: String) = {
	val weights = Map("бухгалтер"->0.9, "виагра"->0.99, "выгодное"->0.7, "github"->0.01)
	val words = text.split(' ').filter(weights.contains(_))
	val P_spam = words.map(weights(_)).reduce(_ * _)
	val P_not_spam = words.map(1 - weights(_)).reduce(_ * _)
	if (P_spam > P_not_spam) "SPAM" else "NOT SPAM"
}
```

Мы берем каждое слово и определяем суммарный вес документа отдельно для класса "спам" и класса "не спам". Суммарный вес определяется как произведение весов всех известных слов документа. Слова для которых у нас нет веса мы пропускаем при классификации. Какой суммарный вес оказался больше тот класс и побеждает.

Это более разумный подход, так как он более гибок и принимает решение на основании всех известных слов в тексте. Так же его гораздо проще сопровождать чем полотна `if`'ов. 

## Метод машинного обучения

А теперь очень важное замечание. _Если у нас будет некий способ автоматически подобрать оптимальные веса слов, то данный подход можно считать методом [машинного обучения][ref-machine-learning]_. Сильно упрощенный, возможно даже гипертрофированный, но по своей сути это именно метод машинного обучения.

Если быть более точным, то описанный мною метод является зародышем [наивного байесовского классификатора][ref-nbc]. Но не позволяйте названию обмануть вас, NBC (Naïve Bayes Classifier) если не самый, то один из самых часто используемых классификаторов. Тому есть ряд причин:

* он прост в имплементации и тестировании;
* процесс обучения довольно эффективен по сравнению с другими более сложными классификаторами;
* на небольших корпусах документов разница между NBC и другими гораздо более сложными алгоритмами классификации зачастую несущественна, а иногда NBC может оказаться и более точным[^on-bayes-optimality].

В последующих заметках я более детально опишу вопросы связанные с созданием и тестированием подобного классификатора. Подписывайтесь на [RSS][ref-rss].

[^google-spam-filter-performance]: [How Gmail Blocks Spam](http://googlesystem.blogspot.com/2007/10/how-gmail-blocks-spam.html)
[^mri-3d]: [Automatic classification of MRI images for three-dimensional volume reconstruction by using general regression neural networks](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=1352574&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F9356%2F29717%2F01352574.pdf%3Farnumber%3D1352574)
[^alzheimer]: [Automatic classification of patients with Alzheimer's disease from structural MRI: a comparison of ten methods using the ADNI database](http://www.ncbi.nlm.nih.gov/pubmed/20542124)
[^boilerplate-paper]: [Boilerplate Detection using Shallow Text Features](http://www.l3s.de/~kohlschuetter/publications/wsdm187-kohlschuetter.pdf)
[^on-bayes-optimality]: [On the Optimality of the Simple Bayesian Classifier under Zero-One Loss](http://www.cc.gatech.edu/fac/Charles.Isbell/classes/reading/papers/bayes-opt.pdf)

[ref-nbc]: http://ru.wikipedia.org/wiki/Наивный_байесовский_классификатор
[ref-collaborative-filtering]: http://ru.wikipedia.org/wiki/Коллаборативная_фильтрация
[ref-sildenafil]: http://ru.wikipedia.org/wiki/Силденафил
[ref-classifier]: http://ru.wikipedia.org/wiki/Классификация_документов
[ref-clustering]: http://ru.wikipedia.org/wiki/Кластерный_анализ
[ref-machine-learning]: http://ru.wikipedia.org/wiki/Машинное_обучение
[ref-rss]: /rss.xml
