<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Суровая реальность</title>
    <link>http://bazhenov.me</link>
    <atom:link href="http://bazhenov.me/rss.xml" rel="self" type="application/rss+xml" />
    <description>Суровая реальность</description>
    <language>ru-ru</language>
    <pubDate>Tue, 23 Apr 2013 22:12:42 +1100</pubDate>
    <lastBuildDate>Tue, 23 Apr 2013 22:12:42 +1100</lastBuildDate>
    
    <item>
      <title>Классификация методом максимальной энтропии</title>
      <link>http://bazhenov.me/blog/2013/04/23/maximum-entropy-classifier.html</link>
      <pubDate>Tue, 23 Apr 2013 00:00:00 +1100</pubDate>
      <author>Denis Bazhenov (dotsid@gmail.com)</author>
      <guid isPermaLink="true">http://bazhenov.me/blog/2013/04/23/maximum-entropy-classifier.html</guid>
      <description>&lt;p&gt;&lt;a href=&quot;http://bazhenov.me/blog/2012/06/11/naive-bayes.html&quot;&gt;Наивный байесовский классификатор&lt;/a&gt;, о котором я уже писал, один из самых простых классификационных алгоритмов. В этой заметке я опишу более сложный алгоритм — &lt;a href=&quot;http://en.wikipedia.org/wiki/Principle_of_maximum_entropy&quot;&gt;метод максимальной энтропии&lt;/a&gt;, который, в ряде случаев, может оказаться существенно более точным. К своему удивлению, я не нашел в рунете более менее полного описания этого алгоритма классификации. Поэтому, считаю полезным поделиться этими знаниями.&lt;/p&gt;

&lt;!-- excerpt --&gt;

&lt;p&gt;Как и в предыдущем случае, описание разбито на две части: &lt;a href=&quot;#theory&quot;&gt;теорию&lt;/a&gt; и &lt;a href=&quot;#practice&quot;&gt;практику&lt;/a&gt;. В теоретической части описываются вопросы связанные с математической моделью классификатора. В практической части, содержится описание, как максимально быстро получить работающий классификатор для практических нужд на Java. Исходный код работающего примера &lt;a href=&quot;https://github.com/bazhenov/maxent-example&quot;&gt;доступен на github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;Мотивация к использованию&lt;/h2&gt;

&lt;p&gt;У байеса есть несомненные преимущества: он эффективен в обучении, может использовать параллелизм современного железа. Но есть у него и минусы. Именно они могут побудить вас использовать более сложные алгоритмы. Таких минусов несколько:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;одна из систематических ошибок байеса связана с предположением условной независимости классификационных признаков (слов, в случае классификации документов). На практике, это предположение часто оказывается ложным. Например, пара слов &amp;ldquo;курильский&amp;rdquo; и &amp;ldquo;бобтейл&amp;rdquo; гораздо чаще встречается вместе, чем &amp;ldquo;бобтейл&amp;rdquo; и &amp;ldquo;турбонаддув&amp;rdquo; или &amp;ldquo;курильский&amp;rdquo; и &amp;ldquo;кальян&amp;rdquo;. Не учитывая эту информацию, байесовский классификатор имеет тенденцию отклонения в сторону классов с высоким уровнем зависимости между словами (так называемая, проблема evidence double-counting&amp;rsquo;а).&lt;/li&gt;
  &lt;li&gt;вероятностные оценки, которые дает байес могут быть сильно искажены для редких документов (подробнее об этом ниже).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Метод максимальной энтропии очень тесно связан с другим распространенным алгоритмом машинного обучения, – &lt;a href=&quot;http://ru.wikipedia.org/wiki/Логистическая_регрессия&quot;&gt;логистической регрессией&lt;/a&gt;. Люди, знакомые с логистической или линейной регрессией, найдут много знакомого в этом описании.&lt;/p&gt;

&lt;h2 id=&quot;theory&quot;&gt;Теоретические основы&lt;/h2&gt;
&lt;p&gt;Байесовский классификатор оперирует на основании таблицы весов, которые он получает на этапе обучения из частотности классификационных признаков в пределах классов, – по одному параметру на каждую пару классификационный признак+класс. То есть, всего таких весов n*k, где n–количество классов, а k–количество классификационных признаков.&lt;/p&gt;

&lt;p&gt;На этапе классификации maxent очень похож на байесовский классификатор. Он основывается на такой же таблице весов и экспоненциальной модели классификации, которая используется в байесе для формирования вероятностного пространства из логарифмических оценок.&lt;/p&gt;

&lt;p&gt;Для дальнейшего повествования введем следующую нотацию. Допустим, у нас есть n классов c&lt;sub&gt;1&lt;/sub&gt;, c&lt;sub&gt;2&lt;/sub&gt;,…, c&lt;sub&gt;n&lt;/sub&gt;, и k классификационных признаков: F&lt;sup&gt;I&lt;/sup&gt;,F&lt;sup&gt;II&lt;/sup&gt;,…,F&lt;sup&gt;k&lt;/sup&gt;. ) &lt;em&gt;Классификационный признак – это бинарная функция над документом.&lt;/em&gt; Например:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;F = function(d) { d.contains(&quot;бухгалтерия&quot;) }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;На основании классификационных признаков генерируются классификационные индикаторы f&lt;sub&gt;1&lt;/sub&gt;, f&lt;sub&gt;2&lt;/sub&gt;,…, f&lt;sub&gt;k*n&lt;/sub&gt;, по одному индикатору на каждую пару признак+класс. &lt;em&gt;Классификационный индикатор – это булева функция над парой документ+класс.&lt;/em&gt; Она истинна только когда истинен соответствующий классификационный признак и совпадает переданный класс.&lt;/p&gt;

&lt;table class=&quot;center&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&amp;nbsp;&lt;/th&gt;
      &lt;th&gt;C&lt;sub&gt;1&lt;/sub&gt;&lt;/th&gt;
      &lt;th&gt;C&lt;sub&gt;2&lt;/sub&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;F&lt;sup&gt;I&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;f&lt;sub&gt;1&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;f&lt;sub&gt;2&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;F&lt;sup&gt;II&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;f&lt;sub&gt;3&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;f&lt;sub&gt;4&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;F&lt;sup&gt;III&lt;/sup&gt;&lt;/td&gt;
      &lt;td&gt;f&lt;sub&gt;5&lt;/sub&gt;&lt;/td&gt;
      &lt;td&gt;f&lt;sub&gt;6&lt;/sub&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p class=&quot;description&quot;&gt;Взаимосвязь классов, классификационных признаков и индикаторов&lt;/p&gt;

&lt;p&gt;Пример классификационного индикатора:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;f = function(c, d) { с == &quot;SPAM&quot; &amp;amp;&amp;amp; F(d) }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Обратите внимание, классификационные индикаторы  нумеруются по сквозному принципу. Индикаторы, дающие единицу как остаток от деления на количество классов, срабатывают (возвращают истину) только на первый класс, кратные двойке на второй и т.д. Такой подход не является обязательным при реализации классификатора, но для осознания теории важно понимать разницу между признаком и индикатором, а также разницей в их нумерации.&lt;/p&gt;

&lt;p&gt;Непосредственно классификация происходит по формуле:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(c\mid d,\lambda)=\frac
{\exp\sum_i^{n \times k}{\lambda_i}f_i\left(c,d\right )}
{\sum_{\tilde{c}\in C}{\exp\sum_i^{n \times k}{\lambda_i}f_i\left(\tilde{c},d\right )}}&lt;/script&gt;

&lt;p class=&quot;description&quot;&gt;Классификационная формула метода максимальной энтропии (Формула №1)&lt;/p&gt;

&lt;p&gt;В этой формуле:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;f_i&lt;/script&gt; — i-ый классификационный индикатор (значение 0 или 1);&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt; — вес i-го классификационного индикатора &lt;script type=&quot;math/tex&quot;&gt;f_i&lt;/script&gt;;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt; — класс-гипотеза; &lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt; — множество всех возможных классов;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;d&lt;/script&gt; — классифицируемый документ.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;У каждого индикатора &lt;script type=&quot;math/tex&quot;&gt;f_i&lt;/script&gt; есть свой вес &lt;script type=&quot;math/tex&quot;&gt;\lambda_i&lt;/script&gt;, который описывает взаимосвязь между соответствующим классификационным признаком и классом. Чем больше вес, тем сильнее связь. Таким образом, числитель дроби описывает экспоненту весов для класса-гипотезы, а знаменатель нормирует значение по единице. Самое сложная часть этой формулы – набор весов &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;, который приходится путем численной оптимизации, о которой мы поговорим позже.&lt;/p&gt;

&lt;p&gt;Результатом этого отношения является не просто классификационное решение, а значение вероятности для заданного класса. Один из плюсов maxent-классификации заключается в том, что она гораздо более точно моделирует вероятностное распределение классов.&lt;/p&gt;

&lt;p&gt;На этом сходства заканчиваются и начинаются различия. Maxent-классификатор имеет несколько существенных отличий от байесовского:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;он является conditional, а не joint-классификатором;&lt;/li&gt;
  &lt;li&gt;он основывается на взаимосвязи энтропии вероятностного распределения и его равномерности;&lt;/li&gt;
  &lt;li&gt;он использует итеративные алгоритмы подбора параметров модели, которые сложнее в реализации и более ресурсоемкие.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Все три различия необходимо рассмотреть более подробно.&lt;/p&gt;

&lt;h3 id=&quot;joint-vs-conditional-&quot;&gt;Joint vs. Conditional-модели&lt;/h3&gt;
&lt;p&gt;Байес для классификации использует следующую формулу: p(d|c)p(c), что является совместным распределением (&lt;a href=&quot;http://en.wikipedia.org/wiki/Joint_probability_distribution&quot;&gt;joint distribution&lt;/a&gt;) документов и классов: p(d∩c). В отличии от байеса, классификатор максимальной энтропии моделирует распредение p(c|d) напрямую, используя обучающую выборку для подбора оптимальных параметром распределения. Есть две причины, по которым этот подход может быть предпочтителен.&lt;/p&gt;

&lt;p&gt;Во-первых, joint-классификаторы игнорируют вероятность документа как такового. Это правомерно, если вы хотите получить только классификацию, но не вероятностное распределение классов.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Классификацией называется список классов отсортированный по убыванию их вероятностей. При этом, вы не знаете их численные значения, только порядок. Классификации, впрочем, достаточно для большинства прикладных задач.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Вероятностные оценки восстанавливаемые из логарифмических оценок байеса являются апроксимацией. Причем, чем меньше априорная вероятность документа, тем сильнее будет искажена восстановленная оценка.&lt;/p&gt;

&lt;p&gt;Во-вторых, conditional-модели позволяют более адекватно моделировать зависимости между словами. Представьте, обучающую выборку из следующих двух документов:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;class   text
-----------------------
USA     San Francisco
China   Beijing
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Для такого корпуса будут построена следующие таблицы весов:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        Naïve Bayes                    MaxEnt
                USA  China                  USA  China
--------------------------    ------------------------
San          -0.847 -1.609    San         0.639 -0.859
Francisco    -0.847 -1.609    Francisco   0.639 -0.859
Beijing      -1.946 -0.511    Beijing    -2.234  0.762
--------------------------    ------------------------
Total        -3.640 -3.729    Total      -0.956 -0.956
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Обратите внимание, что у байеса сумма столбцов по двум классам не равна. Оценка в пределах класса &lt;code&gt;USA&lt;/code&gt; немного выше. Поэтому, если вы попытаетесь классифицировать документ &amp;ldquo;San Francisco Beijing&amp;rdquo;, он попадет в класс &lt;code&gt;USA&lt;/code&gt;. Так происходит из-за высокой зависимости слов &amp;ldquo;San&amp;rdquo; и &amp;ldquo;Francisco&amp;rdquo;. Байес считает их дважды. По этой причине документ и попадает в класс &lt;code&gt;USA&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Maxent-классификатор способен определить такого рода зависимость между классификационными признаками и демпировать их вес, чтобы избежать проблемы double-counting&amp;rsquo;а. В этом случае, он корректно определит вероятность классов как 50/50%.&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;Принцип максимальной энтропии&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;http://ru.wikipedia.org/wiki/Информационная_энтропия&quot;&gt;Информационная энтропия&lt;/a&gt; — это мера неопределенности вероятностного распределения.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(X)=-\sum_{x\in\Omega}p(x)\log p(x)&lt;/script&gt;

&lt;p class=&quot;description&quot;&gt;Формула расчета энтропии для дискретного вероятностного распределения&lt;/p&gt;

&lt;p&gt;В бытовом понимании, энтропия говорит о том, насколько тяжело делать предсказания о событиях описываемых вероятностным распределением. Чем больше энтропия, тем тяжелее делать предсказания.&lt;/p&gt;

&lt;p&gt;На первый взгляд может показаться, что энтропию надо минимизировать. Давайте разберем небольшой пример, показывающий почему это не так.&lt;/p&gt;

&lt;p class=&quot;image&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/maxent/entropy.png&quot; alt=&quot;Энтропия произвольной монеты&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;description&quot;&gt;График энтропии для монеты с произвольным значением p(H)&lt;/p&gt;

&lt;p&gt;На этом графике показана энтропия &lt;a href=&quot;http://ru.wikipedia.org/wiki/Распределение_Бернулли&quot;&gt;распределения Бернулли&lt;/a&gt; в зависимости от его единственного параметра. Это распределение описывает процесс кидания монетки с заданной вероятностью выпадения орла или решки. Обратите внимание, в точках 0 и 1 энтропия равна нулю. Нет никакой неопределенности, всегда выпадает одна и та же сторона монеты. Максимальна же энтропия в точке 0.5, когда выпадение орла и решки равновероятно. Этот пример иллюстрирует одну важную особенность энтропии, чем больше энтропия распределения, тем распределение более равномерно.&lt;/p&gt;

&lt;p&gt;Равномерность распределения – очень полезное качество в задачах классификации. Из всех распределений соответствующих эмпирическим данным, &lt;em&gt;следует выбирать распределение обладающее наибольшей равномерностью&lt;/em&gt; (и, как следствие, наибольшей энтропией). В противном случае, вы &lt;em&gt;выбираете распределение которое содержит информацию, подтверждения которой нет в обучающем корпусе документов.&lt;/em&gt; Именно поэтому maxent-классификатор максимизирует энтропию.&lt;/p&gt;

&lt;p&gt;Проиллюстрируем это на примере. Допустим, вы кидаете монету три раза и все три раза она падает орлом вверх. Существуют ли объективные причины подозревать монету в нечестности (p(H)≠p(T))? С наивной точки зрения, вероятность выпадения орла составляет 100%, что является экстремумом нечестности. Но трех бросков определенно недостаточно, чтобы статистическая вероятность события была репрезентативна в отношении истинной вероятности. В статистике эта проблема решается при помощи доверительных интервалов.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Доверительный интервал – это интервал оцениваемого параметра, такой, что в него попадает истинное значение параметра с заданной вероятностью, которую называют уровень доверия. Если не указано явно, подразумевается, что доверительный интервал обладает уровнем доверия 0.95.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Если мы посчитаем доверительный интервал вероятности выпадения решки (здесь я для расчета использую &lt;a href=&quot;http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval&quot;&gt;доверительный интервал Вильсона&lt;/a&gt;), то он составит: 0-79%, 0-66% и 0-56% для первого, второго и третьего броска соответственно. Даже после третьего броска диапазон истинной вероятности довольно широкий.&lt;/p&gt;

&lt;p class=&quot;image&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/maxent/entropy-constrained.png&quot; alt=&quot;Энтропия произвольной монеты после третьего броска&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;description&quot;&gt;Доверительный интервал после трех бросков монеты&lt;/p&gt;

&lt;p&gt;Так какую вероятность нам выбрать для моделирования такой монеты, если результаты 3-х бросков всё что у нас есть? Принцип максимальной энтропии говорит – 0.5. Мы не получили достаточно информации, чтобы утверждать, что целевое распределение не равномерно. А пока таковой нет, мы должны отталкиваться от предположения равномерности истинного распределения.&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;Выбор параметров модели&lt;/h3&gt;
&lt;p&gt;Последний вопрос, который требует обсуждения заключается в том, как выбрать оптимальный набор весов &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; для нашей формулы классификации?&lt;/p&gt;

&lt;p&gt;Существует несколько способов получения оптимального набора параметров. Один из них – максимизация функции правдоподобия. Для заданного набора параметров модели – это произведение вероятностей всех документов по их классам.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\log p(C|D,\lambda)
=\sum_{(c,d)\in(C,D)}\log p(c|d,\lambda)
=\sum_{(c,d)\in(C,D)}\log\frac
{\exp\sum_i^{n \times k}{\lambda_i}f_i\left(c,d\right )}
{\sum_{\tilde{c}\in C}{\exp\sum_i^{n \times k}{\lambda_i}f_i\left(\tilde{c},d\right )}}&lt;/script&gt;

&lt;p class=&quot;description&quot;&gt;Функция условного правдоподобия maxent-классификатора над корпусом документов &lt;script type=&quot;math/tex&quot;&gt;(C,D)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Набор параметров при котором достигается максимум значения этой функции и является оптимальным. Имеем типичную задачу численной оптимизации.&lt;/p&gt;

&lt;p&gt;Обычно, для того, чтобы найти максимум функции, оперируют её градиентом. Если градиент известен, можно использовать различные алгоритмы для нахождения максимума, самый простой из которых &lt;a href=&quot;http://ru.wikipedia.org/wiki/Градиентный_спуск&quot;&gt;градиентный спуск&lt;/a&gt;. Сам по себе, алгоритм градиентного спуска довольно прост. Он заключается в итеративном рассчете градиента функции в текущей точке и смещения в направлении градиента на некий шаг. Так до тех пор, пока не будет достигнута цель оптимизации. Полное описание этого алгоритма выходит за пределы заметки.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;a href=&quot;http://ru.wikipedia.org/wiki/Градиент&quot;&gt;Градиент&lt;/a&gt; – это вектор, указывающий направление в котором функция растет наиболее интенсивно.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p class=&quot;image&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/maxent/gradient-descend-visualized.png&quot; alt=&quot;Визуализация произвольной функции методов градиентного спуска&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;description&quot;&gt;Визуализация процесса оптимизации произвольной функции двух аргументов&lt;br /&gt;(показаны изолинии функции и апроксимационные шаги алгоритма)&lt;/p&gt;

&lt;p&gt;Частные производные функции правдоподобия, которые составляют градиент, выглядят следующим образом:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial\log p(C|D,\lambda)}{\partial\lambda_i}=
\underbrace{\sum_{(c,d)\in(C,D)}{f_i(c,d)}}_{empirical \: count}-
\underbrace{\sum_{d\in D}{\sum_{c\in C}{p(c|d,\lambda)f_i(c,d)}}}_{predicted\:count}&lt;/script&gt;

&lt;p class=&quot;description&quot;&gt;Частная производная функции условного правдоподобия&lt;/p&gt;

&lt;p&gt;В этой формуле я хотел бы обратить ваше внимание на две группы, выделенные фигурными скобками. Первая – это количество срабатываний классификационного признака на обучающем корпусе (empirical count). Вторая – это предсказанное нашей моделью количество количество срабатываний классификационного признака на том же корпусе (predicted count).&lt;/p&gt;

&lt;p&gt;В точке экстремума все частные производные, а следственно и градиент, равны нулю. Таким образом, вся эта нетривиальная математика приводит нас к очень простому выводу: &lt;em&gt;оптимальная модель – это модель, предсказания которой совпадают с эмпирическим количеством срабатываний классификационных признаков в обучающем корпусе документов.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;В заключение, несколько фактов относительно функции правдоподобия, её градиента и процесса оптимизации:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;оптимальное распределение и максимум функции правдоподобия гарантированно существуют;&lt;/li&gt;
  &lt;li&gt;у функции правдоподобия один глобальным максимум;&lt;/li&gt;
  &lt;li&gt;найденные данным методом параметры описывают распределение с максимальной возможной энтропией.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;practice&quot;&gt;Пример использования OpenNLP Maxent&lt;/h2&gt;

&lt;p&gt;Корректная реализация собственного классификатора максимальной энтропии требует определенного опыта в области машинного обучения. Поэтому, если вам надо получить конкретные результаты, то лучше воспользоваться готовыми решениями. Для Java, есть два проекта, которые предлагают готовый инструмент: классификатор максимальной энтропии из состава проекта &lt;a href=&quot;http://opennlp.apache.org&quot;&gt;OpenNLP&lt;/a&gt; и логистическая регрессия в &lt;a href=&quot;http://mahout.apache.org&quot;&gt;Apache Mahout&lt;/a&gt;. Здесь я опишу, как использовать OpenNLP. Исходный код примера доступен на &lt;a href=&quot;https://github.com/bazhenov/maxent-example&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Проще всего будет тем, кто использует Maven для управления зависимостями. Достаточно прописать следующую зависимость в проекте:&lt;/p&gt;

&lt;pre class=&quot;code&quot;&gt;&lt;code&gt;&amp;lt;dependency&amp;gt;
  &amp;lt;groupId&amp;gt;org.apache.opennlp&amp;lt;/groupId&amp;gt;
  &amp;lt;artifactId&amp;gt;opennlp-maxent&amp;lt;/artifactId&amp;gt;
  &amp;lt;version&amp;gt;3.0.2-incubating&amp;lt;/version&amp;gt;
&amp;lt;/dependency&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Затем, необходимо реализовать класс, который будет извлекать из текста классификационные признаки в формате OpenNLP. Для этого необходимо реализовать интерфейс &lt;code&gt;EventStream&lt;/code&gt;. Этот интерфейс представляет собой итератор по объектам типа &lt;code&gt;Event&lt;/code&gt;. Каждый &lt;code&gt;Event&lt;/code&gt; является обучающим семплом, который содержит в себе массив классификационных признаков и результирующий класс (context и outcome в терминологии OpenNLP). Классификационный признак – это просто напросто уникальная строка.&lt;/p&gt;

&lt;p&gt;Обучение запускается методом &lt;code&gt;GIS.trainModel()&lt;/code&gt;, который принимает на вход итератор по &lt;code&gt;Event&lt;/code&gt;&amp;lsquo;ам, а также некоторые параметры процесса обучения, из которых стоит отметить следующие:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;iterations&lt;/code&gt; — лимит количества итераций процесса обучения;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;cutoff&lt;/code&gt; – минимальное количество срабатываний классификационного признака, при котором признак допускается к обучению. Признаки с частотностью меньше cutoff, игнорируются;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;smoothing&lt;/code&gt;— булевый флаг включающий сглаживание. Сглаживание в алгоритмах машинного обучения тема отдельная. Вкратце, сглаживание может дать более качественную модель, но за счет более сложного процесса обучения. До тех пор, пока у вас есть техническая возможность лучше его использовать.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Результирующую модель можно сохранить в файл при помощи следующего кода:&lt;/p&gt;

&lt;pre class=&quot;code&quot;&gt;&lt;code&gt;SuffixSensitiveGISModelWriter writer = new SuffixSensitiveGISModelWriter(model, new File(&quot;maxent.model&quot;));
writer.persist();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;а прочитать из файла при помощи следующего:&lt;/p&gt;

&lt;pre class=&quot;code&quot;&gt;&lt;code&gt;SuffixSensitiveGISModelReader reader = new SuffixSensitiveGISModelReader(new File(&quot;maxent.model&quot;));
reader.getModel();
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Если у вас остались вопросы, оставляйте из в комментариях. Подписывайтесь на &lt;a href=&quot;http://feeds.feedburner.com/severe-reality&quot;&gt;RSS&lt;/a&gt; для того чтобы получать обновления. Если вам понравилась эта заметка, плюсуйте или поделитесь ссылкой в социальных сетях.&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;Ссылки по теме&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kamalnigam.com/papers/maxent-ijcaiws99.pdf&quot;&gt;Using Maximum Entropy for Text Classification&lt;/a&gt; — Kamal Nigam, John Lafferty, Andrew McCallum;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://bulba.sdsu.edu/~malouf/papers/conll02.pdf&quot;&gt;A comparison of algorithms for maximum entropy parameter estimation&lt;/a&gt; — Robert Malouf;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.stanford.edu/class/cs124/lec/Maximum_Entropy_Classifiers.pdf&quot;&gt;Maxent Models and Discriminative Estimation. Generative vs. Discriminative models&lt;/a&gt; — Christopher Manning&lt;/li&gt;
&lt;/ol&gt;

</description>
    </item>
    
    <item>
      <title>Размер линейного счетчика</title>
      <link>http://bazhenov.me/blog/2013/04/14/linear-counter-bitmask-size.html</link>
      <pubDate>Sun, 14 Apr 2013 00:00:00 +1100</pubDate>
      <author>Denis Bazhenov (dotsid@gmail.com)</author>
      <guid isPermaLink="true">http://bazhenov.me/blog/2013/04/14/linear-counter-bitmask-size.html</guid>
      <description>&lt;p&gt;Для использования &lt;a href=&quot;http://bazhenov.me/blog/2012/12/12/linear-counter.html&quot;&gt;линейного счетчика&lt;/a&gt; необходимо заранее знать приблизительное количество уникальных элементов в потоке. На основании этого количества, а также необходимого вам уровня точности, вычисляется длина битовой маски счетчика.&lt;/p&gt;

&lt;!-- excerpt --&gt;

&lt;p&gt;Допустим, вы хотите создать линейный счетчик для оценки количества элементов в потоке, с максимальным количеством уникальных элементов равным 10 миллионам. Какой длины должна быть битовая маска? Ответ на этот вопрос позволяет найти следующее неравенство:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;m &gt; \max\left(5, \frac{1}{(\epsilon t)^2}\right) \left(e^t - t - 1\right)&lt;/script&gt;

&lt;p&gt;Минимальное положительное целое m, для которого выполняется это неравенство, является длиной маски при которой обеспечивается необходимая точность.&lt;/p&gt;

&lt;p&gt;В этой формуле:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;m&lt;/script&gt; — длина битовой маски;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; — необходимая точность оценки в виде доли (например, 0.01 для точности 1%);&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;t&lt;/script&gt; — так называемый, load factor (&lt;script type=&quot;math/tex&quot;&gt;t=\frac{n}{m}&lt;/script&gt;). Отношение количества уникальных элементов в потоке к длине битовой маски.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;К сожалению, у этого неравенства нет алгебраического решения, – из него нельзя выразить m. Для прикладного применения ответ можно получить несколькими способами.&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;Таблицы&lt;/h2&gt;

&lt;p&gt;В &lt;a href=&quot;http://dblab.kaist.ac.kr/Publication/pdf/ACM90_TODS_v15n2.pdf&quot;&gt;оригинальной публикации&lt;/a&gt; приведены таблицы для точности 1 и 10%. Приведу здесь часть таблицы для полноты повествования.&lt;/p&gt;

&lt;table class=&quot;center&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;n&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;1%&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;10%&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;sup&gt;2&lt;/sup&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5034&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;80&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;sup&gt;3&lt;/sup&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5329&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;268&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;sup&gt;4&lt;/sup&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7960&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1709&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;sup&gt;5&lt;/sup&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;26729&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;12744&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;sup&gt;6&lt;/sup&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;154171&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;100880&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;sup&gt;7&lt;/sup&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1096582&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;831809&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Если вам нужна оценка для точности 1 или 10%, её можно узнать интерполяцией двух промежуточных значений из таблицы.&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;Определение длины маски численным методом&lt;/h2&gt;

&lt;p&gt;Если табличный метод не подходит, решение можно получить числено. Пожалуй, самый простой и вместе с тем довольно эффективный способ заключается в слегка модифицированном бинарном поиске.&lt;/p&gt;

&lt;p&gt;Использование такого подхода возможно, так как правая часть неравенства хоть и не является дифференцируемой на всём промежутке значений, тем не менее, можно показать, что она монотонно убывает при &lt;script type=&quot;math/tex&quot;&gt;m\to\infty&lt;/script&gt; (так как данная заметка носит прикладной характер, доказательство остается на совести читателя). Это означает, что есть такое вещественное значение &lt;script type=&quot;math/tex&quot;&gt;m_0&lt;/script&gt;, которое делит всю область допустимых значений на две: справа неравенство выполняется, а слева не выполняется. Округлив это значение до ближайшего целого сверху мы получим искомый ответ.&lt;/p&gt;

&lt;pre class=&quot;code&quot;&gt;&lt;code&gt;int computeRequiredBitMaskLength(double n, double eps) {
	if (eps &amp;gt;= 1 || eps &amp;lt;= 0) {
		throw new IllegalArgumentException(&quot;Epsilon should be in (0, 1) range&quot;);
	}
	if (n &amp;lt;= 0) {
		throw new IllegalArgumentException(&quot;Cardinality should be positive&quot;);
	}
	int fromM = 1;
	int toM = 100000000;
	int m;
	double eq;
	do {
		m = (toM + fromM) / 2;
		eq = precisionInequalityRV(n / m, eps);
		if (m &amp;gt; eq) {
			toM = m;
		} else {
			fromM = m + 1;
		}
	} while (toM &amp;gt; fromM);
	return m &amp;gt; eq ? m : m + 1;
}

double precisionInequalityRV(double t, double eps) {
	return max(1.0 / pow(eps * t, 2), 5) * (exp(t) - t - 1);
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Как и любой другой бинарный поиск, этот выполняется за &lt;script type=&quot;math/tex&quot;&gt;O(\log d)&lt;/script&gt; (где d — размер диапазона по которому происходит поиск). Таким образом, этот подход позволяет находить ответ за несколько десятков итераций даже для больших значений d. Этого более чем достаточно для большинства прикладных задач.&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>Архитектура поисковых систем</title>
      <link>http://bazhenov.me/blog/2013/01/08/search-architecture.html</link>
      <pubDate>Tue, 08 Jan 2013 00:00:00 +1100</pubDate>
      <author>Denis Bazhenov (dotsid@gmail.com)</author>
      <guid isPermaLink="true">http://bazhenov.me/blog/2013/01/08/search-architecture.html</guid>
      <description>&lt;p&gt;Так уж получилось, что последние несколько лет я занимаюсь вопросами, связанными с поиском. Один из проектов, завершенных в прошлом году, был связан с модифицированием архитектуры нашей поисковой системы. В итоге мы получили результаты, которыми, как я считаю, имеет смысл поделиться. So, here we go.&lt;/p&gt;

&lt;!-- excerpt --&gt;

&lt;h2 id=&quot;section&quot;&gt;Функциональность поисковой системы&lt;/h2&gt;
&lt;p&gt;Но для начала было бы неплохо конкретизировать, что именно может и для каких целей используется наша поисковая система.&lt;/p&gt;

&lt;p&gt;На один из наших проектов каждый день заходит более сотни тысяч пользователей в поисках интересующих их товаров и услуг. Именно обслуживание этих запросов является основной задачей поисковой системы, и именно под эту задачу она и проектировалась.&lt;/p&gt;

&lt;p&gt;Под поисковыми запросами в данном случае я понимаю не только запросы на поиск по ключевым словам, но также и атрибутивный поиск (по цене, типу товара и т.д.)&lt;/p&gt;

&lt;p&gt;Наша система способна выполнять следующие типы запросов:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;атрибутивные (на равенство, на вхождение во множество и на вхождение в диапазон для целочисленных значений);&lt;/li&gt;
  &lt;li&gt;полнотекстовые;&lt;/li&gt;
  &lt;li&gt;запросы на построение фасетов.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Типичный запрос к поисковой системе выглядит следующим образом: &amp;ldquo;&lt;code&gt;все автомобили производителя Toyota или Nissan, с типом трансмиссии Автоматическая, во Владивостоке, дешевле 500 000 рублей&lt;/code&gt;&amp;rdquo;. В дополнении к самим документам, удовлетворяющим критериям, поисковая система может вернуть фасеты. Фасет – это список значений указанного поля с указанием количества документов с этим значением поля. Типичные примеры фасетного поиска вы можете увидеть кругом в Интернете.&lt;/p&gt;

&lt;p class=&quot;image photo&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/search-architecture/amazon-facets.png&quot; alt=&quot;Фасеты на примере amazon.com&quot; width=&quot;799&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Также система способна выполнять сортировку (и последующий пейджинг) документов по значению одного из атрибутов или по релевантности.&lt;/p&gt;

&lt;p&gt;Кроме того, система включает в себя более сложный функционал, который относится к информационному поиску: механизмы обработки синонимов, алгоритмы расширения пользовательского запроса для обеспечения полноты и другой функционал, о котором в пределах этой заметки я говорить не буду. Это отдельная тема.&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;Ретроспектива&lt;/h2&gt;

&lt;p&gt;Когда обсуждают архитектуру систем, более важным, чем текущее состояние, на мой взгляд, является ретроспектива развития. Проблемы, которые испытывал проект в прошлом, являются одной из основных движущих сил, определяющих его будущее. Поэтому, я считаю важным рассказать, как поиск работал раньше, и какие проблемы заставили нас сделать то, что мы сделали.&lt;/p&gt;

&lt;p&gt;Поиск работал очень просто. Все документы складывались в отдельную MySQL таблицу, по которой и осуществлялась необходимая фильтрация. Полнотекстовый поиск реализовывался &lt;a href=&quot;http://dev.mysql.com/doc/refman/5.1/en/fulltext-search.html&quot;&gt;встроенными средствами MySQL&lt;/a&gt;. От этого решения мы очень быстро ушли, так как полнотекстовый поиск в MySQL очень ограничен с точки зрения функциональности. Полнотекстовый индекс мы реализовали на &lt;a href=&quot;http://lucene.apache.org/&quot;&gt;Apache Lucene&lt;/a&gt; в виде отдельного сервиса. Во время записи данных в таблицу копия документа отправлялась в этот сервис для индексации текста. При выполнении полнотекстового поиска вычислялось пересечение результатов двух поисков (по MySQL и по Lucene), и результат отдавался клиенту.&lt;/p&gt;

&lt;p&gt;Некоторые инкрементальные изменения в логике хранения были сделаны по ходу эксплуатации. Мы ушли от схемы одна таблица на все предметные области, так как эта таблица получалась очень разряженная (например, у сотовых телефонов нет атрибутов характерных для автомобилей и наоборот), в сторону схемы одна таблица для общих атрибутов + одна таблица на каждую отдельную предметную область. Это позволило немного сэкономить на времени выполнения full scan&amp;rsquo;ов даже не смотря на появившиеся join&amp;rsquo;ы таблиц.&lt;/p&gt;

&lt;p&gt;В этом виде поиск проработал несколько лет. И никто бы его наверное и не трогал, но примерно в тот момент, когда количество актуальных предложений перевалило за миллион, частота возникновения различных проблем заставила нас задуматься о том, что срок жизни этого решения подходит к концу.&lt;/p&gt;

&lt;p&gt;Проблем было несколько:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;intersect join делался путем отправки всех идентификаторов, полученных от Lucene в качестве дополнительного критерия фильтрации в MySQL. Это ставило ограничение на количество документов, которые мы принимаем к фильтрации от полнотекстового индекса, так как у MySQL есть лимит на длину SQL-запроса;&lt;/li&gt;
  &lt;li&gt;так как вариативность условий фильтрации таблицы очень высокая, невозможно было подобрать covered index&amp;rsquo;ы для всех возможных запросов. Фактически, в наших условиях full scan&amp;rsquo;ы были неизбежны. Самым эффективным зачастую оказывался индекс, позволяющий обойти данные в порядке сортировки с фильтрацией не подходящих кортежей на лету. На первый взгляд такой обход кажется крайне не эффективным. Тем не менее, так как все запросы содержат инструкцию &lt;code&gt;LIMIT&lt;/code&gt;, позволяющую БД досрочно прервать выполнение запроса когда будет накоплено достаточное количество предложений, этот подход выигрывал у фильтрации по наиболее подходящему индексу и последующей сортировке;&lt;/li&gt;
  &lt;li&gt;из-за высокой вариативности запросов кеширование принципиально не влияло на latency системы, которое росло линейно с ростом датасета;&lt;/li&gt;
  &lt;li&gt;добавление новых индексов в систему стало очень не тривиальной процедурой из-за необходимости делать блокирующий ALTER TABLE по таблице большого размера.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Таким путем мы пришли к пониманию того, что реляционная БД не может в полной мере удовлетворить наши потребности. И мы начали смотреть по сторонам. Но для начала надо было определится что именно мы хотим получить в итоге.&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;Основные силы, определяющие архитектуру&lt;/h2&gt;

&lt;p&gt;Архитектура определяется потребностями, тем, какой мы хотим видеть систему в будущем, и в каком направлении мы будем ее развивать. Поэтому не лишним будет описать основные факторы, определившие созданное нами решение.&lt;/p&gt;

&lt;h3 id=&quot;transactional--data-&quot;&gt;Возможность transactional и data мастшабирования&lt;/h3&gt;
&lt;p&gt;В момент когда мы формировали облик системы, у нас уже была относительно немаленькая транзакционная нагрузка (несколько сотен поисковых запросов в секунду), и учитывая ретроспективу проекта она должна была расти с известной нам скоростью. Имеющееся у нас на тот момент решение уже было вполне способно масштабироваться по пропускной способности за счет репликации.&lt;/p&gt;

&lt;p&gt;Но было известно также, что существенные ресурсы будут вкладываться в увеличение объема контента, поэтому нам было необходимо решение которое сможет масштабироваться не только по пропускной способности, но и по размеру корпуса документов. Репликация, к сожалению, не является адекватным решением масштабирования по объему данных.&lt;/p&gt;

&lt;p&gt;Также мы хотели получить систему, на операционные проблемы которой (в плане нагрузки) не надо будет отвлекать программистов. Это довольно типичный синдром, нагрузка на систему вырастает, и программистам сразу же приходится бросать свои текущие задачи, чтобы вернуть систему в стабильное состояние. Нам нужна была система, в которой мы могли бы компенсировать нагрузку железом, а не временем разработчиков. Возможность компенсации железом если не решает проблему нагрузки полностью, то позволяет команде не заниматься fire fighting&amp;rsquo;ом. Она дает драгоценное время, чтобы спокойно обдумать сложившуюся ситуацию, после чего без спешки реализовать, протестировать и ввести в эксплуатацию адекватное решение.&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;Отказоустойчивость поиска&lt;/h3&gt;
&lt;p&gt;Оборудование выходит из строя как планово так и внепланово, но люди должны иметь возможность искать интересующие их товары и услуги в любое время дня и ночи вне зависимости от проведения плановых работ или возникновения операционных инцидентов. Естественным ответом на эту потребность является резервирование всех жизненно важных ролей для системы. Если без какого-то звена системы поиск не возможен, это звено должно быть минимум дублировано.&lt;/p&gt;

&lt;p&gt;Требование доступности определило важную архитектурную особенность системы, которая отличает ее от современных open source аналогов, таких как Sphinx, Solr и Elastic Search. Позже я объясню в чем она заключается.&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;Гибкость в реализации сценариев поиска&lt;/h3&gt;
&lt;p&gt;Но самое главное качество поиска заключается не в его скорости, масштабируемости или отказоустойчивости, а в том, что он позволяет пользователям находить интересующие их предложения. Это не так просто как кажется на первый взгляд. Пользователи не всегда точно знают, что они ищут, равно как и не все знают как эффективно пользоваться поиском, чтобы найти нужную информацию. Иногда, они указывают слишком узкие критерии поиска и получают пустую выборку. Это ситуация из которой человеку не просто выбраться. Мы заранее знали, что нам прийдется реализовывать более экзотичные виды поиска.&lt;/p&gt;

&lt;p&gt;Такое положение вещей диктовало дизайну и архитектуре системы еще одно требование. С точки зрения дизайна вся логика, определяющая сценарий поиска, должна соответствовать принципу &lt;a href=&quot;http://en.wikipedia.org/wiki/Open/closed_principle&quot;&gt;OCP&lt;/a&gt;, чтобы ее легко можно было расширять и реализовывать новые сценарии, а архитектурно исполнение кода сценария должно быть отделено от самого индекса, над которым сценарий выполняется.&lt;/p&gt;

&lt;p&gt;Перечисленные выше требования по большей части и сформировали архитектуру всей системы.&lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;Существующие решения&lt;/h3&gt;
&lt;p&gt;Конечно же в первую очередь мы смотрели на уже существующие open source решения и прикидывали, можем ли мы использовать что-нибудь готовое. Таких решений несколько: &lt;a href=&quot;http://sphinxsearch.com&quot;&gt;Sphinx&lt;/a&gt;, &lt;a href=&quot;http://lucene.apache.org/solr/&quot;&gt;Solr&lt;/a&gt; и &lt;a href=&quot;http://www.elasticsearch.org&quot;&gt;Elastic Search&lt;/a&gt;. В документации у них заявлено, что это distributed и fault-tolerant системы. На практике же и Sphinx и Solr это скорее инструменты для создания такой системы, но сами по себе они таковыми не являются.&lt;/p&gt;

&lt;p&gt;Во-первых, ни одна из вышеперечисленных систем не сохраняет исходные документы. Это означает, что на выходе вы получаете набор идентификаторов, а не документы, которые вы отправляли на индексацию. По этой причине необходимо также обеспечивать и отказоустойчивость базы данных, в которой вы храните первичные данные. В противном случае, поиск вроде как fault-tolerant, но как только БД с первичкой становится недоступна, оказывается что система не в состоянии обслуживать поисковые запросы. По-крайней мере, в том смысле, в котором это понимает пользователь, а именно – показать документы, подпадающие под указанные критерии. Стоит отметить, что физически Solr умеет хранить данные в Lucene-индексе, но сами разработчики настоятельно не рекомендуют этого делать. Связано это в первую очередь с существенными издержками на I/O&lt;sup id=&quot;fnref:ref-solr-stored-performance&quot;&gt;&lt;a href=&quot;#fn:ref-solr-stored-performance&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. &lt;/p&gt;

&lt;p&gt;Во-вторых, ни Sphinx, ни Solr не имплементируют раздельной индексации. То есть подготовки отдельных индексов для различных партиций. Документация по обоим проектам перекладывает эту ответственность на клиента&lt;sup id=&quot;fnref:ref-sphinx-distributed-search&quot;&gt;&lt;a href=&quot;#fn:ref-sphinx-distributed-search&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; &lt;sup id=&quot;fnref:ref-solr-distributed-search&quot;&gt;&lt;a href=&quot;#fn:ref-solr-distributed-search&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;. Насколько мне известно, раздельную индексацию поодерживают только Elastic Search и довольно новый проект &lt;a href=&quot;http://wiki.apache.org/solr/SolrCloud&quot;&gt;SolrCloud&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;И последнее, но не менее важное. На момент принятия решения у нас уже был существенный опыт эксплуатации Apache Lucene. Он у нас использовался в тот момент для обслуживания полнотекстовых запросов. Причем опыт этот был сугубо позитивный. Lucene отлично справлялся с возложенной на него задачей, и довольно быстро появилось осознание того что область его применения гораздо более обширная, чем просто fulltext поиск.&lt;/p&gt;

&lt;p&gt;В итоге мы решили реализовать собственный интеграционный слой поверх Apache Lucene и &lt;a href=&quot;http://www.oracle.com/technetwork/products/berkeleydb/overview/index-093405.html&quot;&gt;Berkeley DB JE&lt;/a&gt; в качестве БД для документов, который бы предоставил все требуемые нам возможности.&lt;/p&gt;

&lt;p&gt;На Berkeley DB выбор пал по нескольким причинам. Предполагаемые паттерны доступа были очень простые (только по первичному ключу), поэтому мы сразу начали оценивать целесообразность применения NoSQL решения для хранения документов. Упрощенная модель данных, если подходит под задачу, при прочих равных дает целый ряд преимуществ: меньше сюрпризов с производительностью, простая диагностика проблем, относительно дешевая отказоустойчивость. Berkeley DB хорошо подошел под наши сценарии использования. Это CP система, у нее мигрирующий мастер и запись по кворуму, что позволяет и читать, и писать документы до тех пор, пока в кластере осталось хотя бы N/2+1 машин. С точки зрения диагностики потенциальных проблем этот вариант для нас тоже был предпочтительным, так как будучи встраиваемой БД мы знали что сможем контролировать гораздо больше аспектов её поведения. К тому же, так как система реализовывалась на Java, Berkeley DB JE был удобен еще и с точки зрения стоимости тестирования.&lt;/p&gt;

&lt;h2 id=&quot;section-6&quot;&gt;Архитектура поисковой системы&lt;/h2&gt;

&lt;p&gt;С высоты птичьего полета наша поисковая система выглядит следующим образом.&lt;/p&gt;

&lt;p class=&quot;image photo&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/search-architecture/overview.png&quot; alt=&quot;Overview&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Полупрозрачным цветом обозначены звенья системы, количество которых может быть подобрано исходя из нагрузки.&lt;/p&gt;

&lt;p&gt;Основные роли:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Index Node (IN) — машины, выполняющие запросы над инвертированным индексом, группируются в партиции (Apache Lucene);&lt;/li&gt;
  &lt;li&gt;Index Master (IM) — машина, обновляющая инвертированный индекс;&lt;/li&gt;
  &lt;li&gt;Document Storage (DS) — машины занимающиеся хранением исходных документов (Berkeley DB);&lt;/li&gt;
  &lt;li&gt;Coordinator (CN) — машины, являющиеся фасадом к поисковой системе для клиентов. Они инициируют и координируют деятельность внутри кластера, связанную с обработкой пользовательского запроса (REST-сервис);&lt;/li&gt;
  &lt;li&gt;Notification Broker — система обмена сообщениями, которая используется нами для синхронизации состояния кластера (ActiveMQ)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Возможность transactional и data масштабирования мы реализовали путем партиционирования индекса на несколько частей и поддержанием нескольких реплик для каждой партции. Весь индекс разбивается на не пересекающиеся куски примерно одинакового размера. Каждый кусок отдается на обработку отдельной партиции. Партиция состоит из нескольких машин (реплик), которые хранят копию одной и той же части индекса и обрабатывают пользовательские запросы над этой частью индекса.&lt;/p&gt;

&lt;p&gt;Таким образом, когда у нас появляются проблемы с транзакционной нагрузкой, мы добавляем реплики в партиции. Когда появляются проблемы с тем, что одна машина не способна эффективно обрабатывать имеющийся у нее индекс из за того что он слишком большой, мы добавляем новую партицию. При добавлении новой партиции индекс переразбивается на большее количество частей, и каждая партиция получает индекс меньшего размера.&lt;/p&gt;

&lt;h3 id=&quot;section-7&quot;&gt;Поиск&lt;/h3&gt;
&lt;p&gt;Index Node&amp;rsquo;ы делают всю грязную работу, связанную с поиском. У них есть доступ к индексу, а наружу они предоставляют довольно простой интерфейс. На вход от координаторов они принимают детализированный поисковый запрос (в Lucene-формате), параметры сортировки и фасетирования, а на выходе возвращают Top N документов удовлетворяющих запросу. Такой простой интерфейс позволяет реализовывать различные поисковые сценарии, не меняя контракт взаимодействия с Index Node.&lt;/p&gt;

&lt;p class=&quot;image photo&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/search-architecture/communication.png&quot; alt=&quot;Overview&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Index Node&amp;rsquo;ы периодически реплицируют индекс с Index Master&amp;rsquo;а, проверяют его целостность и делают его основным индексом, по которому выполняются поиски.&lt;/p&gt;

&lt;p&gt;Еще одна важная деталь касающаяся Index Node&amp;rsquo;ов заключается в том, что мы разогреваем индекс перед тем как опубликовать его. Каждый Index Node имеет доступ к live потоку текущих запросов. Перед тем как пустить поисковые запросы на новый индекс, по индексу прогоняются production-запросы до тех пор, пока время ответа на этом индексе не стабилизируется. Это защищает пользователей от проблем &amp;ldquo;холодного индекса&amp;rdquo;. То же самое происходит при старте индексных машин.&lt;/p&gt;

&lt;p class=&quot;image photo&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/search-architecture/warmup.png&quot; alt=&quot;Warm up индекса&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Всем процессом поиска управляют координаторы, которые тоже резервируются. При поиске координатор выбирает по одной реплике из каждой партиции и передает запрос им. После того как реплики ответят, координатор объединяет частичные результаты, полученные от реплик, выполняет пейджинг, получает необходимые документы из БД и отдает ответ клиенту. Координаторы не имеют собственного состояния и содержат всю логику, связанную со сценариями поиска.&lt;/p&gt;

&lt;p&gt;После успешного выполнения запроса, сам запрос публикуется на общую шину для последующей обработки. Примерами такой обработки являются: архивирование логов, упомянутый ранее разогрев индекса и сбор различной статистики.&lt;/p&gt;

&lt;p&gt;Обобщение процесса поиска проиллюстрировано на следующей схеме. Процессы, происходящие асинхронно и не блокирующие обработку поискового запроса, отображены пунктиром.&lt;/p&gt;

&lt;p class=&quot;image photo&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/search-architecture/search-read.png&quot; alt=&quot;Data flow при поиске&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;section-8&quot;&gt;Обновление индекса и хранение данных&lt;/h3&gt;
&lt;p&gt;Одно из решений которое, отличает нашу систему от open source аналогов вроде Solr, заключается в том, что мы храним не только индекс, но и исходные документы, которые предоставляет клиент на индексацию. Системы же вроде Solr на выходе выдают дают набор идентификаторов, и вы должны дополнительно преобразовать их в данные, которые собираетесь показать пользователю.&lt;/p&gt;

&lt;p&gt;В качестве БД мы используем Berkeley DB Java Edition. Это key-value log-oriented база данных. Так как она является встраиваемой БД, все данные доступны in-memory, что очень удобно при умеренных размерах корпуса документов. Нагрузочное тестирование показало, что на корпусе в 50 миллионов документов BDB не является узким звеном и отлично справляется со своими задачами.&lt;/p&gt;

&lt;p&gt;На схеме ниже отображен data-flow при записи документа в поисковую службу:&lt;/p&gt;

&lt;p class=&quot;image photo&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/search-architecture/search-write.png&quot; alt=&quot;Data flow при записи&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Berkeley DB периодически рапортует информацию о состоянии кластера координаторам, чтобы те знали, на какую из машин кластера отсылать запросы на запись.&lt;/p&gt;

&lt;p&gt;Обновляется индекс отдельной машиной кластера – Index Master&amp;rsquo;ом. Структура инвертированого индекса сильно оптимизирована под эффективное чтение, поэтому его обновление операция относительно не дешёвая. В задачи Index Master&amp;rsquo;а входит:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;обновление инвертированного индекса в соответствии с поступающими на индексацию документами и топологией кластера (количеством партиций и их конфигурацией);&lt;/li&gt;
  &lt;li&gt;поддержание индекса в компактном состоянии для минимизации накладных расходов поисковых реплик.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Именно Index Master определяет в какую партицию попадет какой документ.&lt;/p&gt;

&lt;p class=&quot;image photo&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/search-architecture/im-partitions.png&quot; alt=&quot;Взаимодействие Index Master и партиций&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;section-9&quot;&gt;Проблемы и компромиссы&lt;/h2&gt;

&lt;p&gt;Рассказ был бы неполный, если бы я не упомянул о проблемах, с которыми нам пришлось столкнуться, и о компромиссах на которые нам пришлось пойти.&lt;/p&gt;

&lt;p&gt;Одно из спорных решений, которое мы приняли заключается в том, что мы равномерно распределяем документы по партициям кластера на основании идентификатора документа&lt;sup id=&quot;fnref:ref-ii-partitionong&quot;&gt;&lt;a href=&quot;#fn:ref-ii-partitionong&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. Плюс этого подхода заключается, в том что мы получаем очень равномерное распределение документов и запросов, а следственно и нагрузки, между партициями. Минус же заключается в том, что этот подход не дает абсолютно никакой локализации (data locality). На фазе поиска мы не можем сделать никаких предположений о том, где расположены целевые документы, поэтому мы вынуждены опрашивать все партиции и делать scatter-gather. Это вызывает ряд неудобств.&lt;/p&gt;

&lt;p&gt;Первое неудобство заключается в том, что координатор вынужден ждать ответа от всех партиций. Соответственно, общая производительность системы ограничивается производительностью самой медленной партиции. В случае большого количества партиций, производительность отдельной партиции может существенно отличаться от производительности системы в целом.&lt;/p&gt;

&lt;p class=&quot;image&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/search-architecture/cdf-latency.png&quot; alt=&quot;CDF времени ответа для систем с различным количеством партиций&quot; width=&quot;467&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Как видно, чем больше партиций, тем ниже производительность. Если каждая отдельная партиция с вероятностью 25% отвечает за 100 миллисекунд, то система, ждущая ответа 4-х партиций, не сможет отвечать за 100 миллисекунд в 25% случаев. Следует сказать, что этот вывод основывается на предположении взаимной независимости времени ответа между партициями, что далеко не всегда является правдой. Но в распределенных системах к этому качеству всячески стремятся, так как оно полезно во многих контекстах.&lt;/p&gt;

&lt;p&gt;Также, scatter-gather на большое количество партиций при определенных обстоятельства может приводить к моментальному выеданию сетевого канала и/или TCP-incast&amp;rsquo;у&lt;sup id=&quot;fnref:ref-tcp-incast&quot;&gt;&lt;a href=&quot;#fn:ref-tcp-incast&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;. Суть в следующем: пейджинг можно делать только после того, как объединены поисковые результаты всех партиций. Поэтому Index Node не может делать пейджинг на своей стороне и вынужден отсылать на координатор не только страницу с запрошенными результатами, но и все документы, предшествующие ей. Таким образом, если человек запросил 1000-ую страницу, а на каждой странице по 100 документов, то каждая партиция должна отослать на координатор 100 000 совпавших документов. Легко посчитать, что при среднем размере записи в 30 байт на документ (вместе с идентификатором возвращается служебная информация) и при 10 партициях суммарный размер ответа будет близок к 30 мегабайтам. В худшем случае это означает прибавку к latency в сотни миллисекунд (на гигабитном канале) и не более 4-5 запросов в секунду на одной машинке.&lt;/p&gt;

&lt;p&gt;С другой стороны, польза от просмотра 1000-ой страница довольно сомнительная. По нашей статистике до 10-й страницы доходит менее одного процента пользователей. Поэтому выход из этой ситуации для нас был очень простой, – мы просто не обслуживаем запросы дальше определенной страницы. Такое же поведение вы можете заметить у многих других систем (Яндекс, Google, eBay).&lt;/p&gt;

&lt;p&gt;Еще одна причина, почему мы решили не заморачиваться с data locality, состоит в том, что в противном случае нам пришлось бы самим реализовывать Distributed IDF для Lucene&lt;sup id=&quot;fnref:ref-distributed-idf&quot;&gt;&lt;a href=&quot;#fn:ref-distributed-idf&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;. В общих чертах, IDF — это статистическая информация по корпусу документов, которая используется для ранжирования. В случае если документы распределяются равновероятно между партициями, а не на основании их контента, мы считаем что локальный IDF партиции является приемлемой апроксимацией глобального IDF.&lt;/p&gt;

&lt;p&gt;Вообще, схема оптимального разбиения корпуса документов – это отдельная очень большая и интересная тема, которую я не могу осветить сейчас.&lt;/p&gt;

&lt;h2 id=&quot;section-10&quot;&gt;Полученные результаты&lt;/h2&gt;

&lt;p&gt;Описанная в данной заметке система работает уже около полугода, и у нас появился эксплуатационный опыт, позволяющий судить о ее качествах. Немного цифр:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;транзакционная нагрузка (номинальная 300 tps, пиковая 1000 tps, 17 миллионов запросов в сутки);&lt;/li&gt;
  &lt;li&gt;размер текущего корпуса документов — 8 миллионов;&lt;/li&gt;
  &lt;li&gt;пропускная способность индексации — 1000 документов в секунду;&lt;/li&gt;
  &lt;li&gt;время ответа: среднее — 50ms, 9-й дециль — 150ms.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Система горизонтально масштабируется по размеру корпуса. Мы это проверили на практике, за последние пол года он вырос более чем в два раза. Добавление новых партиций позволило держать время ответа системы стабильным, не отвлекая от работы разработчиков.&lt;/p&gt;

&lt;p&gt;Результаты capacity-тестирования говорят, что с минорными изменениями система способна обеспечить приемлемое время ответа на корпусе в 50 миллионов документов. По нашим оценкам, это дает нам около года спокойной работы без оглядки на проблемы с производительностью поиска, что нас вполне устраивает.&lt;/p&gt;

&lt;h2 id=&quot;section-11&quot;&gt;Ссылки по теме&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:ref-solr-stored-performance&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://wiki.apache.org/solr/SolrPerformanceFactors#stored_fields&quot;&gt;Solr Performance Factors&lt;/a&gt;&lt;a href=&quot;#fnref:ref-solr-stored-performance&quot; rel=&quot;reference&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ref-sphinx-distributed-search&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://sphinxsearch.com/docs/1.10/distributed.html&quot;&gt;Sphinx Documentation: Distributed Searching&lt;/a&gt;&lt;a href=&quot;#fnref:ref-sphinx-distributed-search&quot; rel=&quot;reference&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ref-solr-distributed-search&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://wiki.apache.org/solr/DistributedSearch#Distributed_Indexing&quot;&gt;Solr Distributed Indexing&lt;/a&gt;&lt;a href=&quot;#fnref:ref-solr-distributed-search&quot; rel=&quot;reference&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ref-ii-partitionong&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.62.1613&quot;&gt;Effect of Inverted Index Partitioning Schemes on Performance of Query Processing in Parallel Text Retrieval Systems&lt;/a&gt;&lt;a href=&quot;#fnref:ref-ii-partitionong&quot; rel=&quot;reference&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ref-tcp-incast&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.eecs.berkeley.edu/Pubs/TechRpts/2012/EECS-2012-40.pdf&quot;&gt;Understanding TCP Incast and Its Implications for Big Data Workloads&lt;/a&gt;&lt;a href=&quot;#fnref:ref-tcp-incast&quot; rel=&quot;reference&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:ref-distributed-idf&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://lucene.472066.n3.nabble.com/solrCloud-Distributed-IDF-scoring-in-the-cloud-td2526179.html&quot;&gt;Solr Mailing Lists: Distributed IDF - scoring in the cloud&lt;/a&gt;&lt;a href=&quot;#fnref:ref-distributed-idf&quot; rel=&quot;reference&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Линейный счетчик</title>
      <link>http://bazhenov.me/blog/2012/12/12/linear-counter.html</link>
      <pubDate>Wed, 12 Dec 2012 00:00:00 +1100</pubDate>
      <author>Denis Bazhenov (dotsid@gmail.com)</author>
      <guid isPermaLink="true">http://bazhenov.me/blog/2012/12/12/linear-counter.html</guid>
      <description>&lt;p&gt;Допустим, вам необходимо рассчитать количество уникальных строк в файле. Причем, файл большой – миллионы или десятки миллионов строк. Типичный shell&amp;rsquo;овский однострочник который позволяет решить эту задачу выглядит следующим образом:&lt;/p&gt;

&lt;pre class=&quot;shell&quot;&gt;&lt;code&gt;sort | uniq | wc -l
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;И все бы хорошо, но есть одна проблема. Имя ей &lt;code&gt;sort&lt;/code&gt;.&lt;/p&gt;

&lt;!-- excerpt --&gt;

&lt;p&gt;Сортировка &lt;script type=&quot;math/tex&quot;&gt;O(n \log n)&lt;/script&gt; по времени и &lt;script type=&quot;math/tex&quot;&gt;O(n)&lt;/script&gt; по памяти, поэтому время её работы очень быстро перерастает все разумные пределы. Как вариант, можно хранить не сами строки, а их контрольные суммы. Это снизит как требования по памяти, так и асимптотическую оценку по времени до &lt;script type=&quot;math/tex&quot;&gt;O(n)&lt;/script&gt;. Но хранение контрольных сумм подразумевает наличие коллизий, поэтому вы можете получить не точную оценку, а приблизительную. В случаях когда приблизительной оценки достаточно мы можем разменять точность алгоритма на его скорость и требования к памяти.&lt;/p&gt;

&lt;p&gt;Существует один очень простой алгоритм который позволяет вычислять оценку количества уникальных объектов в потоке с довольно высокой точностью за линейное время, используя &lt;em&gt;0.1 бита на один уникальный объект&lt;/em&gt;. Да да, вы не ослышались в одном бите хранится информация о десяти уникальных объектах.&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;Алгоритм линейного счетчика&lt;/h2&gt;

&lt;p&gt;Ладно, я вас обманул. Невозможно в одном бите хранить информацию о десяти объектах. Но можно хранить тот факт что мы уже встречали хотя бы один из объектов привязанных к этому биту. Именно эта идея лежит в основе алгоритма.&lt;/p&gt;

&lt;p&gt;Представьте, что у вас есть битовый вектор из 1000 бит, и вы устанавливаете в нем 1000 случайных бит (некоторые биты будут установлены более одного раза). Если вы теперь посчитаете сколько битов установлено в единицу, то получите число близкое к 630. Если вам интересно почему в итоге получается именно такое число, то об этом &lt;a href=&quot;http://bazhenov.me/blog/2012/04/16/one-in-a-million.html&quot;&gt;я уже писал&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Верно и обратное. Если мы встречаем битовый вектор длины &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; в котором примерно 63% бит установлено в единицу, это значит что надо этим вектором было произведено примерно &lt;script type=&quot;math/tex&quot;&gt;n&lt;/script&gt; операций установки бита в единицу, при условии что биты выбирались равновероятно.&lt;/p&gt;

&lt;p&gt;Таким, довольно нехитрым, способом можно восстановить оценку количества уникальных объектов на основании population count битового вектора (количества бит установленных в единицу). Сделать это можно используя следующую формулу: &lt;script type=&quot;math/tex&quot;&gt; l \log(\frac{l}{N_f}) &lt;/script&gt;, где &lt;script type=&quot;math/tex&quot;&gt;l&lt;/script&gt; — длина вектора, а &lt;script type=&quot;math/tex&quot;&gt;N_f&lt;/script&gt; количество свободных (нулевых) бит (логарифм обязан быть натуральным).&lt;/p&gt;

&lt;p&gt;На графике ниже приведена &lt;a href=&quot;https://gist.github.com/4267208&quot;&gt;симуляция&lt;/a&gt; для вектора длины 10000. В него индексировалось указанное количество объектов, затем получалась доля установленных бит и на её основании восстанавливалась оценка количества добавленных объектов.&lt;/p&gt;

&lt;p class=&quot;image&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/linear-counter/fig1.png&quot; alt=&quot;Тестирование линейного счетчика&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Точность оценки естественным образом связана с отношением количества уникальных объектов к длине битового вектора. Оказывается, что даже если количество добавляемых объектов превышает длину вектора в десять раз, вы по-прежнему можете получать достаточно точные оценки. Погрешность при этом составляет порядка 1%. Это означает, что если в потоке 100 миллионов уникальных объектов, для получения более менее приемлемой оценки достаточно 10 мегабайт памяти.&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;Реализация&lt;/h2&gt;

&lt;p&gt;Реализация этого алгоритма довольно простая. Самое главное, это хеш-функция с равномерным распределением. Любая криптографическая функция вполне подойдет.&lt;/p&gt;

&lt;p&gt;Консольная утилита способная делать быстрый estimate количества уникальных строк в pipe&amp;rsquo;е иногда очень кстати. Поэтому, я реализовал &lt;a href=&quot;https://github.com/bazhenov/linear-counter&quot;&gt;линейный счетчик на C++&lt;/a&gt;. Вы можете использовать этот код в качестве примера. Если же вы пишете на Java, то есть замечательный проект &lt;a href=&quot;https://github.com/clearspring/stream-lib&quot;&gt;stream-lib&lt;/a&gt; в котором помимо линейного счетчика есть масса других вероятностных алгоритмов которые могут оказаться очень полезными при работе с большими массивами данных.&lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;Ссылки по теме&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Kyu Y. Whang, Brad T. Vander Zanden, and Howard M. Taylor. &lt;a href=&quot;http://dblab.kaist.ac.kr/Publication/pdf/ACM90_TODS_v15n2.pdf&quot;&gt;A linear-time probabilistic counting algorithm for database applications.&lt;/a&gt; ACM Trans. Database Syst., 15(2):208–229, 1990;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://highlyscalable.wordpress.com/2012/05/01/probabilistic-structures-web-analytics-data-mining/&quot;&gt;Probabilistic Data Structures for Web Analytics and Data Mining&lt;/a&gt; — Ilya Katsov.&lt;/li&gt;
&lt;/ul&gt;

</description>
    </item>
    
    <item>
      <title>Feature selection в алгоритмах классификации</title>
      <link>http://bazhenov.me/blog/2012/12/10/feature-selection.html</link>
      <pubDate>Mon, 10 Dec 2012 00:00:00 +1100</pubDate>
      <author>Denis Bazhenov (dotsid@gmail.com)</author>
      <guid isPermaLink="true">http://bazhenov.me/blog/2012/12/10/feature-selection.html</guid>
      <description>&lt;p&gt;Существует один очень простой и эффективный способ улучшения алгоритмов классификации, который называется &lt;a href=&quot;http://en.wikipedia.org/wiki/Feature_selection&quot;&gt;feature selection&lt;/a&gt; (выбор классификационных признаков). Этот метод позволяет при построении модели выбрать только самые показательные признаки (например, слова) и отсеять остальные.&lt;/p&gt;

&lt;!-- excerpt --&gt;

&lt;p&gt;Что такое показательные признаки? Если мы говорим о задаче классификации текстовых документов, то это слова которые несут информацию о классе к которому относится документ. Например, в контексте автомобильной тематики слово &amp;ldquo;Nissan&amp;rdquo;, скорее всего будет показательным признаком, а вот слово &amp;ldquo;новый&amp;rdquo; вряд ли. Использование для классификации не всех слов, а только показательных дает несколько преимуществ.&lt;/p&gt;

&lt;p&gt;Во-первых, feature selection позволяет существенно уменьшить количество параметров модели (используемых для классификации слов), и как следствие снижает требования к объему памяти требуемой для классификации.&lt;/p&gt;

&lt;p&gt;Во-вторых, feature selection может повысить точность алгоритма за счет удаления из модели слов с низким соотношением сигнал/шум. Представьте, что слово &amp;ldquo;аскетичный&amp;rdquo; встретилось в обучающей выборке 3 раза, – 1 раз в рамках класса &amp;ldquo;Авто&amp;rdquo; и 2 раза в рамках класса &amp;ldquo;Одежда&amp;rdquo;. Формально оно говорит в пользу класса &amp;ldquo;Одежда&amp;rdquo;, но вряд ли это слово можно считать хорошим классификационным признаком. Скорее всего, перевес в сторону одежды это случайность.&lt;/p&gt;

&lt;p&gt;Но &lt;em&gt;сам факт наличия такого слова в обучающей выборке не случайность&lt;/em&gt;. Это следствие &lt;a href=&quot;http://ru.wikipedia.org/wiki/Закон_Ципфа&quot;&gt;закона Зипфа&lt;/a&gt; (Zipf&amp;rsquo;s law), который описывает распределение частот слов в натуральном языке. Простым языком это можно описать следующим образом, если вы возьмете любой достаточно большой корпус документов и посчитаете сколько в нем слов встречающихся ровно 1 раз, 2 раза, 3 раза и т.д., то окажется что большинство слов встречаются 1 раз. Это не должно быть большим сюрпризом, – в повседневной жизни мы используем довольно небольшое количество слов (высокочастотники), а большую часть слов мы практически не используем (низкочастотники).&lt;/p&gt;

&lt;h2 id=&quot;mutual-information&quot;&gt;Mutual Information&lt;/h2&gt;

&lt;p&gt;В этой заметке я опишу один, пожалуй самый простой способ оценки показательности классификационных признаков, – &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html&quot;&gt;метод взаимной информации&lt;/a&gt; (Mutual Information). Идея метода очень проста. Зная как часто слово употребляется в пределах документов класса и за его пределами, мы можем сказать насколько статистически сильно связаны слово и класс.&lt;/p&gt;

&lt;p&gt;Для того чтобы рассчитать численное значение взаимной информации необходимо составить матрицу цитирумости. Матрица цитируемости – это матрица 2x2 которая показывает взаимоотношение конкретного слова с конкретным классов. Возьмем, к примеру слово &amp;ldquo;Toyota&amp;rdquo; и класс &amp;ldquo;Авто&amp;rdquo;.&lt;/p&gt;

&lt;table align=&quot;center&quot;&gt;
	&lt;tr&gt;
		&lt;th /&gt;
		&lt;th&gt;C&lt;sub&gt;Авто&lt;/sub&gt;=1&lt;/th&gt;
		&lt;th&gt;C&lt;sub&gt;Авто&lt;/sub&gt;=0&lt;/th&gt;
	&lt;/tr&gt;
	&lt;tr&gt;
		&lt;th&gt;W&lt;sub&gt;Toyota&lt;/sub&gt;=1&lt;/th&gt;
		&lt;td&gt;N&lt;sub&gt;11&lt;/sub&gt;=65 342&lt;/td&gt;
		&lt;td&gt;N&lt;sub&gt;10&lt;/sub&gt;=143&lt;/td&gt;
	&lt;/tr&gt;
	&lt;tr&gt;
		&lt;th&gt;W&lt;sub&gt;Toyota&lt;/sub&gt;=0&lt;/th&gt;
		&lt;td&gt;N&lt;sub&gt;01&lt;/sub&gt;=45 342&lt;/td&gt;
		&lt;td&gt;N&lt;sub&gt;00&lt;/sub&gt;=897 657&lt;/td&gt;
	&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Из этой матрицы видно что слово &amp;ldquo;Toyota&amp;rdquo; было встречено в 65 342 документах в контексте класса &amp;ldquo;Авто&amp;rdquo;, а также в 143 документах за переделами класса &amp;ldquo;Авто&amp;rdquo;. Имея на руках такую матрицу мы можем рассчитать взаимную информацию по формуле:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; MI =
\frac{N_{11}}{N} \log_2{ \frac{N N_{11}}{N_{1.} N_{.1}} } +
\frac{N_{01}}{N} \log_2{ \frac{N N_{01}}{N_{0.} N_{.1}} } + 
\frac{N_{10}}{N} \log_2{ \frac{N N_{10}}{N_{1.} N_{.0}} } +
\frac{N_{00}}{N} \log_2{ \frac{N N_{00}}{N_{0.} N_{.0}} } &lt;/script&gt;

&lt;p&gt;где, &lt;script type=&quot;math/tex&quot;&gt; N &lt;/script&gt; — сумма всей матрицы, &lt;script type=&quot;math/tex&quot;&gt; N_{0.} = N_{00} + N_{01} &lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt; N_{.1} = N_{01} + N_{11} &lt;/script&gt; и т.д.&lt;/p&gt;

&lt;p&gt;Взаимная информация всегда находится в диапазоне от нуля до единицы. Чем выше значение, тем сильнее связь между наличием/отсутствием слова и наличием/отсутствием класса. Рассчитав взаимную информацию для всех пар слово-класс мы можем оставить для каждого класса первые N слов, объединить эти слова в общий словарь и теперь только их использовать для классификации. Количество слов необходимых для оптимальной классификации зависит от конкретной задачи и его необходимо подбирать эмпирически, постоянно проверяя результаты на тестовой выборке.&lt;/p&gt;

&lt;p&gt;На тех задач на которых я тестировал этот метод, он давал прирост точности от 3 до 10%. В Introduction to Infromation Retrieval указано что на практике возможен прирост в десятки процентов по F&lt;sub&gt;1&lt;/sub&gt;&lt;sup id=&quot;fnref:ref-mi-performance&quot;&gt;&lt;a href=&quot;#fn:ref-mi-performance&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Все зависит от вашей конкретной задачи, а также классификационного алгоритма.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:ref-mi-performance&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html#fig:mccallum&quot;&gt;Introduction to Information Retrieval, 2008 Cambridge University Press&lt;/a&gt;&lt;a href=&quot;#fnref:ref-mi-performance&quot; rel=&quot;reference&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel> 
</rss>