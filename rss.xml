<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Суровая реальность</title>
    <link>http://bazhenov.me</link>
    <atom:link href="http://bazhenov.me/rss.xml" rel="self" type="application/rss+xml" />
    <description>Суровая реальность</description>
    <language>ru-ru</language>
    <pubDate>Mon, 07 May 2012 12:37:36 +1100</pubDate>
    <lastBuildDate>Mon, 07 May 2012 12:37:36 +1100</lastBuildDate>
    
    <item>
      <title>Гармоническое среднее</title>
      <link>http://bazhenov.me/blog/2012/05/05/harmonic-mean.html</link>
      <pubDate>Sat, 05 May 2012 00:00:00 +1100</pubDate>
      <author>dotsid@gmail.com (Denis Bazhenov)</author>
      <guid>http://bazhenov.me/blog/2012/05/05/harmonic-mean</guid>
      <description><p>Сегодня я хочу обсудить следующую проблему. Как мониторить CPU usage на многопроцессорной машине? Конечно же мониторить метрики выдываемые <code>mpstat</code>. Эта программа выдает процент времени который процессор проводит в различныйх состояниях (<code>user</code>, <code>system</code>, <code>iowait</code>, <code>idle</code> и т.д.).</p>

<!--excerpt-->

<pre class="shell"><code>$ mpstat 1
Linux 2.6.32-200.13.1.el5uek (search-personal2.vfarm.loc)		05/05/2012 	_x86_64_	(16 CPU)

11:35:52 AM  CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest   %idle
11:35:53 AM  all    8.34    0.00    0.59    0.06    0.00    0.12    0.00    0.00   90.89
11:35:54 AM  all    7.27    0.00    0.86    0.00    0.00    0.36    0.00    0.00   91.51
11:35:55 AM  all    7.80    0.00    0.45    0.06    0.00    0.17    0.00    0.00   91.53
11:35:56 AM  all    5.33    2.17    0.84    0.00    0.00    0.14    0.00    0.00   91.52
11:35:57 AM  all    5.92    0.00    0.40    0.06    0.00    0.06    0.00    0.00   93.57
11:35:58 AM  all    4.71    0.07    0.42    0.00    0.00    0.14    0.00    0.00   94.67
</code></pre>

<p>В ретроспективе это выглядит следующим образом:</p>

<p class="image photo"><img src="http://bazhenov.me/images/harmonic-mean/fig1.png" alt="CPU usage" /></p>

<p>Может показаться что у этого сервера нет никаких проблем с CPU. Тем не менее надо учитывать что машина многопроцессорная и может оказаться что нагрузка на ядра не симмитрична. <code>mpstat</code> же показывает арифметическое среднее, поэтому если вы на 16 процессорной машине видите CPU utilization 6% это может означать:</p>

<ul>
  <li>машина загружена симметрично и у каждого ядра есть еще масса свободного времени;</li>
  <li>машина загружена не симметрично – одно ядро работает на 100% все остальные курят бамбук.</li>
</ul>

<p>Конечно же последний случай это явная проблема и система мониторинга должна позволять находить такие ситуации. Но что мониторить чтобы находить ассиметричную нагрузку на различные ядра?</p>

<h2 id="section">Мониторинг</h2>

<h3 id="load-average">Load average</h3>
<p><a href="http://en.wikipedia.org/wiki/Load_(computing)">Load average</a> (<code>uptime</code>, <code>w</code>) не позволяет отследить подобную ассиметричность в нагрузке, так как ее следствием является работа, которая не может быть выполнена параллельно. В этом случае в системе не будет длинной очереди CPU scheduler&rsquo;а, а это именно то что и показывает load average.</p>

<h3 id="section-1">Мониторить отдельно каждое ядро</h3>
<p>Можно отслеживать ассиметричность имея информацию по CPU usage для каждого отдельного ядра (на подобии той которая приведена в начале заметки). Но это, как вы можете догадаться довольно напряжно. Слишком много данных, которые надо пропустить через мозг чтобы получить информацию.</p>

<h3 id="cpu-utilization">Экстремальные значения CPU utilization</h3>
<p>Можно мониторить например максимальное значение CPU utilization. Это позволит понять какая утилизация у самого загруженного ядра в системе. Благодаря этому можно отследить ситуацию ассиметричной нагрузки по разнице между арифметическим средним и максимальным значением утилизации.</p>

<p>Мы мониторим <a href="http://en.wikipedia.org/wiki/Harmonic_mean">гармоническое среднее</a>. <em>Гармоническое среднее, в отличии арифметического стремится к нулю когда хотя бы одно из значений стремится к нулю.</em> Считается оно тоже довольно просто — количество значений деленное на сумму обратных значений:</p>

<script type="math/tex; mode=display">\frac{n}{\sum\limits_{i=1}^n \frac{1}{x_i}}</script>

<p>То есть для двух процессоров idle которых равен 3 и 100, гармоническое среднее равно: <script type="math/tex">\frac{2}{\frac{1}{3} + \frac{1}{100}} \approx 5.8</script></p>

<p>Если добавить на график который я привел выше, гармоническое среднее утилизации, то мы получим следующее:</p>

<p class="image photo"><img src="http://bazhenov.me/images/harmonic-mean/fig2.png" alt="CPU Harmonic Utilization" /></p>

<p>Здесь видно что большую часть времени нагрузка распределяется равномерно (светло синей области практически не видно). Тем не менее в период с 1:10 до 1:20 нагрузка на CPU ассиметрична, что говорит о выполнении задачи которая не может быть распараллелена.</p>

<h2 id="section-2">Что может быть причиной?</h2>

<p>Это может быть любая активность которая не может быть эффективно распараллелена. Например, банальный <code>grep</code> по большому объему данных (если он не упрется в I/O), сжатие, потоковое кодирование видео, шифрование, некоторые алгоритмы GC в JVM однопоточные по своей природе.</p>

<p>Более подробно об этом явлении я уже писал ранее в заметке &ldquo;<a href="http://bazhenov.me/blog/2009/01/13/moores-law-a-la-finita.html">Конец эры закона Мура</a>&rdquo;.</p>

<h2 id="section-3">Что делать в этой ситуации?</h2>
<p>Во-первых, надо понять является ли это проблемой. Вполне возможно, что эта ситуация может быть вызвана какой-нибудь background задачей, которая ни коим образом не затрагивает пользователей. Если же данная ситуация влияет на качество сервиса предоставляемого пользователям, то определенно надо более точно локализовать проблему и попытаться разрешить ее.</p>

<p>На ОС Linux в диагностике подобного рода проблем вам могут помочь следующие инструменты:</p>

<ul>
  <li>команда <code>pidstat -u -t -p $PID 1</code> позволяет выяснить какие потоки указанного процесса кушают CPU наиболее активно. Эта команда может быть использована в том числе и для диагностики проблем JVM приложений, так как потоки JVM напрямую соотносятся с потоками ОС;</li>
  <li>для получения информации по потокам JVM может быть полезна команда <code>jstack $PID</code>, которая делает thread dump JVM приложения (<code>jstack</code> является частью JDK и не входит в комплект поставки JRE);</li>
  <li>для получения трейса конкретного потока можно воспользоваться командами <code>strace</code>/<code>ltrace</code>. Они показывают трейс системных вызовов и библиотечных вызовов соответственно.</li>
</ul>

<p>Надеюсь эти инструменты помогут вам диагностировать подобные ситуации быстро и безболезненно.</p>

</description>
    </item>
    
    <item>
      <title>Один на миллион</title>
      <link>http://bazhenov.me/blog/2012/04/16/one-in-a-million.html</link>
      <pubDate>Mon, 16 Apr 2012 00:00:00 +1100</pubDate>
      <author>dotsid@gmail.com (Denis Bazhenov)</author>
      <guid>http://bazhenov.me/blog/2012/04/16/one-in-a-million</guid>
      <description><p>Вам наверное приходилось слышать что-то вроде: &ldquo;Этого не может быть. Вероятность этого менее одной миллионной&rdquo;. В бытовом понимании одна миллионная это некая несвершимая вероятность. Другими словами, этого просто не может произойти. Но насколько это вeрятное событие, если вы повторите эксперимент миллион раз? Конечно, вероятность наблюдать событие растет если вы повторяете эксперимент много раз подряд. Но все равно мы зачастую подсознательно пытаемся игнорировать такие события считая их невероятными.</p>

<!-- excerpt -->

<p>Тем не менее, в современных Интернет-системах которые обрабатывают миллионы запросов в сутки не заметить в течение дня событие с вероятностью в одну миллионную довольно сложно. И вот почему. Вероятность что интересующее нас событие (с исходной вероятностью <code>1/n</code>) не проявит себя ни в одном из <code>n</code> независимых экспериментов равно:</p>

<script type="math/tex; mode=display">(1-\frac{1}{n})^n</script>

<p>В этой формуле $\frac{1}{n}$ — вероятность события которое реализуется в одном эксперименте из <script type="math/tex">n</script>. <script type="math/tex">1-\frac{1}{n}</script> соответственно вероятность что это событие не реализуется в ходе эксперимента. Ну и не тяжело догодаться что $(1-\frac{1}{n})^n$ это вероятность с которой это событие не реализуется в ходе <script type="math/tex">n</script> независимых экспериментов. Для n = 1 000, если вы его подставите в формулу, итоговое значение вероятности составит 0.368 или 36.8%. Для n = 1 000 000, значение вероятности составит&hellip; те же самые 36.8%! <em>Редкость события всегда уравновешивается частотой повторения испытаний.</em></p>

<p>Обоснование этого возможно контринтуитивного результата кроется во <a href="http://ru.wikipedia.org/wiki/Замечательные_пределы#.D0.92.D1.82.D0.BE.D1.80.D0.BE.D0.B9_.D0.B7.D0.B0.D0.BC.D0.B5.D1.87.D0.B0.D1.82.D0.B5.D0.BB.D1.8C.D0.BD.D1.8B.D0.B9_.D0.BF.D1.80.D0.B5.D0.B4.D0.B5.D0.BB">втором замечательном пределе</a>, который большинству из нас преподавали в ходе курса высшей математики.</p>

<script type="math/tex; mode=display">\lim_{n \to +\infty}(1+\frac{1}{n})^n = e</script>

<p>Второй замечательный предел интенсивно используется при решении дифференциальных уравнений, но нас интересует одно из следствий этого предела.</p>

<p>Этот предел говорит нам о том что при растущем <code>n</code>, значение выражения <script type="math/tex">(1-\frac{1}{n})^n</script> стремится к значению <script type="math/tex">\frac{1}{e}</script>, которое является той самой константой приблизительно равной 0.368.</p>

<blockquote>
  <p>Я не буду здесь приводить доказательство этого следствия второго замечательного предела. Предел разумного количества формул на один пост достигнут :).</p>
</blockquote>

<p>Теперь давайте соберем все это вместе. Предположим что у вас есть некий баг (скажем, race condition в многопоточном приложении), который воспроизводится с вероятностью один на миллион. Фактически что говорит этот предел – если вы обслужите за день миллион запросов то вероятность того что этот баг не порявит себя ни в одном из запросов ~37%. То есть с вероятностью ~63% этот баг даст о себе знать <em>хотя бы один раз</em> за сутки.</p>

<p>Из этого можно сделать еще один вывод. Чем больше запросов вы обрабатываете за сутки, тем более редкие явления вы будете наблюдать в течение суток. 100 000 запросов в сутки? Тогда вероятность <script type="math/tex">\frac{1}{1 000 00}</script> для вас не редкость. 10 000 000? Тогда и одной десятимиллионной вас не удивишь.</p>

<p>Да пребудет с вами нормальное распределение!</p>

</description>
    </item>
    
    <item>
      <title>О высокой нагрузке</title>
      <link>http://bazhenov.me/blog/2012/02/26/highload.html</link>
      <pubDate>Sun, 26 Feb 2012 00:00:00 +1100</pubDate>
      <author>dotsid@gmail.com (Denis Bazhenov)</author>
      <guid>http://bazhenov.me/blog/2012/02/26/highload</guid>
      <description><p>В последнее время все чаще говорят о <em>высоконагруженных</em> приложениях. Нельзя не заметить что это теперь очень популярная, можно даже сказать модная, область знаний. Сам же термин &ldquo;высоконагруженная система&rdquo; при этом не имеет в нашей отрасли четкого определения. В этой заметке я хочу привести свои рассуждения по этому вопросу. Я не ставлю перед собой цель дать исчерпывающее определение этого термина. Моя цель, предоставить читателю ключевую информацию о системах такого рода, которая определяет стиль мышления при работе с ними.</p>

<!-- excerpt -->

<p>Итак, что такое высоконагруженная система? Ответ на этот вопрос стоит начать с описания качественных свойств такого рода систем.</p>

<h2 id="section">Традиционные качества высоконагруженной системы</h2>
<p>Как правило, к таким качествам относят большое количество пользователей и данных. В целом это правда, но тут есть несколько загвоздок:</p>

<ul>
  <li>это не вся правда;</li>
  <li>приведенные факторы являются количественными, а не качественными.</li>
</ul>

<p>Ниже на основании предпосылки &ldquo;много пользователей, много данных&rdquo; я сформирую список качественных факторов присущих высоконагруженным системам. Начнем с самого простого.</p>

<h3 id="section-1">Многопользовательская система</h3>
<p>Конечно же высоконагруженное приложение в первую очередь является многопользовательским. То есть в один момент времени с ним работает более чем один человек. Сейчас, в эру стремительного развития Интернета, это тысячи и сотни тысяч человек.</p>

<p>Устойчивая ассоциация высоконагруженных систем с большим количеством пользователей в нашей индустрии появилась давным давно. Ничего принципиально неверного в такой связи нет. Но если высокая нагрузка подразумевает большое количество пользователей, то большое количество пользователей совсем не обязательно подразумевает высоконагруженную систему.</p>

<p>Если посмотреть на статистику Московского метрополитена за 2010 год, то окажется что средняя часовая нагрузка на систему максимальна в диапазоне от 8 до 9 часов утра<sup id="fnref:ref-metro-stat"><a href="#fn:ref-metro-stat" rel="footnote">1</a></sup>. За этот час через турникеты проходят приблизительно 720 тысяч человек. Что порождает необходимость не менее 200 раз в секунду проверять статус предъявленных проездных и принимать решение о пропуске того или иного человека через турникет. В Интернете существует масса высоконагруженных ресурсов с подобными показателями пропускной способности. Например, статистика по StackOverflow за тот же 2010-й год показывает что их средняя пропускная способность находится в диапазоне 100-150 хитов в секунду<sup id="fnref:ref-stackoverflow-stat"><a href="#fn:ref-stackoverflow-stat" rel="footnote">2</a></sup>.</p>

<p>Определенно у метрополитена более высокие требования к пропускной способности. Но значит ли это что Московский метрополитен можно считать более &ldquo;высоконагруженным&rdquo; чем StackOverflow? Вряд ли, в частности потому что эти две системы оперируют несравнимыми объемами информации, о чем мы поговорим позже.</p>

<p>Я намерено в обоих случаях привел оценку пропускной способности, так как она дает больше информации о нагрузке чем количество пользователей системы. Две разные системы могут подталкивать пользователей к разным паттернам их использования. Это может приводить к абсолютно разным требованиям по пропускной способности для этих систем. Пропускная способность точнее описывает количество работы которую должна уметь выполнять система в единицу времени.</p>

<h3 id="section-2">Распределенная система</h3>
<p>Высоконагруженные системы являются системами распределенными, то есть работают более чем на одном сервере. Зачастую это десятки и сотни серверов. Требование распределенности вытекает из следующих причин:</p>

<ul>
  <li>необходимости обрабатывать возрастающие объемы данных;</li>
  <li>необходимости &ldquo;живучести&rdquo; системы в случаях отказа части серверов.</li>
</ul>

<p>Но обо всем по порядку…</p>

<p>Я наверное не ошибусь если скажу что большинство высоконагруженных приложений являются Интернет-приложениями (позже мы еще вернемся к состоятельности этой гипотезы). А отличительной особенностью современного Интернета основанного на концепции <a href="http://ru.wikipedia.org/wiki/Веб_2.0">Web 2.0</a> является тот факт, что сами пользователи генерируют данные, которые они сами же в итоге и потребляют. Это приводит к тому, что чем больше у вас пользователей, тем больше потенциальный объем хранимых данных.</p>

<p>Требование обработки больших объемов данных может существенно осложнить жизнь. Под &ldquo;большим объемом&rdquo; я подразумеваю такой объем информации, который не может эффективно обработать одна машина. В большинстве случаев, это объем превышающий объем доступной на сервере оперативной памяти. То есть приходится тем или иным образом распределять данные между несколькими машинами, каждая из которых обрабатывает свой небольшой кусочек данных, но делает это эффективно, без page fault&rsquo;ов (не используя swap) и прочих неприятностей. Необходимость эффективной обработки данных диктуется другим очень важным качеством высоконагруженных систем, – интерактивностью, о котором будет сказано ниже.</p>

<p>Но большие объемы данных – это не все. Ко всему к этому хочется чтобы система работала в режиме 24x7 без остановок и перерывов. Но вот незадача, любое даже самое надежное оборудование иногда выходит из строя. Встает естественная задача обеспечения доступности системы в случаях отказа оборудования. </p>

<p>Тут мы вступаем в область знания распределенных систем, эксплуатация которых редко бывает безоблачной, даже когда вы используете готовые решения. Тем не менее распределенные системы, не смотря на сложность их разработки и поддержки, пожалуй единственный подход позволяющий обеспечить вышеизложенные требования в полной мере.</p>

<h3 id="section-3">Позитивная динамика по пользователям и данным</h3>
<p>Если ваше приложение представляет хоть какой-то интерес, то даже если ничего не делать, ваша аудитория будет расти просто с ростом аудитории Интернета. Поэтому характерной чертой высоконагруженных систем является не просто большое количество пользователей, но и <em>позитивная динамика</em> количества пользователей.</p>

<p>В контексте реалий Web 2.0 растущее количество пользователей может привести к тому, что <em>такую же позитивную динамику вы будете иметь и по данным</em>. Поэтому в контексте высоконагруженных систем корректней говорить не о <em>большом</em>, а о <em>растущем</em> количестве пользователей и данных.</p>

<h3 id="section-4">Интерактивность</h3>
<p>И тут мы подходим к своеобразной финальной ноте в аккорде качеств highload систем, если позволите так выразиться. Интерактивность – одно из основополагающих качеств высоконагруженной системы. Интерактивность подразумевает, что пользователь после того как послал запрос сидит и ждет ответа, а люди как известно ждать не любят. При этом большинство Интернет-приложений о которых мы говорим не являются критическими важными в жизни людей. Twitter, Flickr, Facebook и т.д. это все круто конечно, но если они будут отвечать непомерно долго, я найду чем занятся еще. Наша жизнь от них не зависит (прогрессирующие формы задротства у некоторых особо аддиктивных индивидов не в счет), а это значит что эти приложения должны занимать минимум нашего времени. То есть отвечать за приемлимое время. Это контрастирует, например, с системами пакетной обработки данных, в которых время ответа системы вторично.</p>

<p>Из этого правила безусловно есть исключения. Представьте что вы только что совершили покупку в Интернете сообщив свою платежную информацию третьим лицам. Скорее всего вы дождетесь ответа системы, даже если она будет отвечать дольше минуты. Но как известно исключение из правила лишний раз подверждает само правило.</p>

<h2 id="section-5">Управление ресурсами</h2>
<p>Качество интерактивности очень важно для понимания сути высоконагруженных приложений, потому что по вышеобозначенным причинам разрабатывая такую систему вы должны быть уверены вот в чем:</p>

<blockquote>
  <p>Каждый раз когда приложение получает очередной запрос, у него  должно быть достаточно свободных ресурсов (CPU, память, диск, сеть) для обработки запроса за приемлимое время.</p>
</blockquote>

<p>Возможно это и звучит тривиально, но именно данное требование приводит нас к основному посылу данной заметки:</p>

<blockquote>
  <p>Контроль за ресурсами является неотъемлимой частью работы с высоконагруженным проектом.</p>
</blockquote>

<p>Необходимо также учитывать тот факт, что из-за позитивной динамики <em>свободных ресурсов становится меньше с течением времени</em>. В этом заключается &ldquo;парадокс&rdquo; высоконагруженных систем. Если вы возьмете высоконагруженный проект и заморозите его разработку (отправите всех разработчиков в бессрочный отпуск), то <em>рано или поздно он перестанет работать</em>.</p>

<blockquote>
  <p>Высоконагруженная система – это интерактивная распределенная система которая требует постоянного контроля за собственными ресурсами. В противном случае она перестает работать.</p>
</blockquote>

<p>Это противоречит обывательскому опыту. Как может само по себе поломаться то, чего не меняли? У программного кода нет срока годности. Причина в том, что со временем системе просто перестанет хватать ресурсов. А нехватка ресурсов провоцируется факторами, часть из которых мы уже рассмотрели:</p>

<ul>
  <li>рост аудитории;</li>
  <li>рост объема данных;</li>
  <li>изменения паттернов поведения аудитории, в том числе и сезонные.</li>
</ul>

<p>Если посмотреть темы докладов на конференциях посвященных тематике разработки высоконагруженных систем (например, тот же <a href="http://highload.ru/">highload</a>), то большинство тем с которыми выступают докладчики можно свести к двум основополагающим направлениям:</p>

<ul>
  <li>как решать существующие задачи используя меньше ресурсов (практически все NoSQL БД, неблокирующий I/O, оптимизация и тюнинг);</li>
  <li>как имея больше ресурсов решать пропорционально больший объем задач (message queueing, распределенные вычисления, распределенные БД, параллелизм).</li>
</ul>

<p>Эту дихотомию я уже затрагивал ранее в заметке <a href="http://bazhenov.me/blog/2009/06/28/performance-versus-scalability.html">Performance vs. Scalability</a>.</p>

<p>Фактически речь на таких конференциях идет в основном о различных способах адекватного управления аппаратными ресурсами в контексте баз данных, языков программирования, операционных систем, алгоритмов, моделей ввода/вывода, вычислительных парадигм и т.д.</p>

<p>Просто подумайте над этим. Большая часть пропогандируемых в настоящее время баз данных под эгидой NoSQL, не предоставляет качественно новых возможностей для разработчиков. Реляционная модель позволяет относительно легко все это реализовать не выходя за рамки одной парадигмы. В этом, я полагаю, и заключается одна из причин почему реляционки приобрели такую популярность и до сих остаются самым распространенным типом БД.
Так называемые, пост-реляционные базы данных не являются инструментом для решения качественно новых задач, всего лишь инструментом для более эффективного решения существующих. Именно важность эффективной траты ресурсов стала катализатором популярности NoSQL тематики.</p>

<h2 id="section-6">Высоконагруженный проект – это Интернет приложение</h2>
<p>Если исходить из того что неотъемлемой частью высоконагруженного проекта является постоянный рост данных и аудитории, то становится понятно почему высоконагруженные проекты – это поголовно Интернет приложения.</p>

<p>В реальной жизни всегда есть некий предел, который не позволяет использовать систему все возрастающему количеству людей. В том же метрополитене человеку требуется некоторое время, чтобы пройти через турникет. Исходя из этого времени, а также общего количества турникетов можно достаточно точно расчитать верхний предел возможной нагрузки. Выглядит очень невероятным чтобы за секунду через один турникет могло пройти 10 человек.</p>

<p>В сфере High Performance Computations приложения могут выполнять просто гигансткое количество операций в единицу времени. Больше чем любое Интернет-приложение. Но это количество зависит только от объема входных данных, а также алгоритма обработки (например, от точности моделирования, если речь идет о моделировании динамических систем). Тяжело придумать причину почему нагрузка на такую систему может вырасти сама по себе без вмешательства персонала ее сопровождающего.</p>

<p>Похоже, что Интернет-приложения это единственная сфера в которой нагрузка есть переменная не имеющая верхнего предела.</p>

<h2 id="highload-">Работая в Highload проекте</h2>
<p>Но не стоит возводить данный тип проектов на какой-то особый пьедестал. Highload проект это в первую очередь такой же самый проект как и все остальные. Проект в котором бывают и гонки за фичами и deadline&rsquo;ы и сложности межличностных отношений и все прочие &ldquo;прелести&rdquo; процесса разработки ПО. Но говоря о &ldquo;высоконагруженной составляющей&rdquo; проекта, работа над ней сводится к тому, что постоянно приходится искать ответы на ресурсно-ориентированные вопросы:</p>

<ul>
  <li>какие bottleneck&rsquo;и есть в системе по ресурсам и как их устранить?</li>
  <li>каков запас системы по ресурсам учитывая естественный рост аудитории/данных?</li>
  <li>что делать при нехватке того или иного ресурса? (например, можно за счет RAM сэкономить CPU).</li>
</ul>

<p>Не удивительно что в такого рода проектах гигантское значение имеет система мониторинга. Она предоставляет массу информации для ответа на такого рода вопросы. Сопровождать и развивать высоконагруженный проект без мониторинга хотя бы основных метрик по всему серверному парку это безрассудство.</p>

<p>Это приводит нас к cписку первостепенных задач которые стоят перед разработчиками в контексте высокой нагрузки:</p>

<ul>
  <li>создание инфраструктуры предоставляющей качественную обратную связь по утилизации аппаратных ресурсов системы (мониторинг, self-аудит приложений);</li>
  <li>определение звеньев системы нуждающихся в масштабировании в ближайшем времени и выбор стратегии масштабирования этих звеньев; </li>
  <li>определение toolbox&rsquo;а позволяющего решать типовые задачи эффективно по аппаратным ресурсам (базы данных, очереди сообщений, языки программирования, библиотеки и т.д.).</li>
</ul>

<h2 id="section-7">В состоянии гонки</h2>
<p>У вас как у инженера нет прямого влияния на количество пользователей и есть опосредованое влияние на объемы данных. В конце концов, чем больше данных и чем шире аудитория, тем лучше для бизнеса. Больше пользователей – больше возможностей для монетизации. Больше данных – больше конкурентное преимущество, а так же потенциально более высокие темпы роста проекта. Необходимо смирится с непрерывным ростом этих двух метрик.</p>

<p>Что же остается? Не так много на самом деле. Сделать так чтобы возможности системы были всегда на шаг впереди ее потребностей.</p>

<div class="footnotes">
  <ol>
    <li id="fn:ref-metro-stat">
      <p><a href="http://www.mosmetro.ru/documents/11931/2010.pdf">Московский Метрополитен 2010: Основные показатели работы метрополитена</a><a href="#fnref:ref-metro-stat" rel="reference">&#8617;</a></p>
    </li>
    <li id="fn:ref-stackoverflow-stat">
      <p><a href="http://msug.vn.ua/Posts/Details/4149">Microsoft User Group Community: Немного статистики от StackOverflow за 2010 год</a><a href="#fnref:ref-stackoverflow-stat" rel="reference">&#8617;</a></p>
    </li>
  </ol>
</div>
</description>
    </item>
    
    <item>
      <title>Deploy и прочие неприятности</title>
      <link>http://bazhenov.me/blog/2011/06/10/deploy.html</link>
      <pubDate>Fri, 10 Jun 2011 00:00:00 +1100</pubDate>
      <author>dotsid@gmail.com (Denis Bazhenov)</author>
      <guid>http://bazhenov.me/blog/2011/06/10/deploy</guid>
      <description><p>В продолжение <a href="http://bazhenov.me/blog/2011/04/09/build.html">предыдущего поста</a> хочу немного рассказать о том как у нас происходит deploy. В прошлый раз мы закончили на том что артефакт доставлен на production и готов к развертыванию. Начинается самое интересное, процесс деплоя.</p>

<!--excerpt-->

<p>Но для начала надо немного описать платформу на которую мы деплоим наши приложения. В качестве servlet-container&rsquo;а мы используем <a href="http://jetty.codehaus.org/jetty/">Jetty</a>. Есть два основных типа приложений, которые мы пишем и сопровождаем: web-приложения, которые имеют некий web-интерфейс (это может быть как UI, так и REST интерфейс), а также background сервисы, которые как правило работают асинхронно (мы активно используем <a href="http://activemq.apache.org/">ActiveMQ</a>). И здесь нужно подметить первую особенность, мы используем Jetty для обоих типов приложений. Это может показатся странным, так как для background-сервисов мы получаем явный overhead связанный с запуском и работой servlet-container&rsquo;а. Но этот overhead просто мизерный по сравнению с тем какие плюсы мы получаем. А именно:</p>

<ul>
  <li>единые утилиты управления lifecycle&rsquo;ом приложения вне зависимости от его типа (старт, останов, перезапуск);</li>
  <li>стандартные способы задания конфигурации приложения вне зависимости от его типа;</li>
  <li>стандартные форматы распространения приложения вне зависимости от его типа (в нашем случае <code>.war</code>).</li>
</ul>

<p>Вобщем, стандартизация на лицо.</p>

<p>У нас своя сборка Jetty в которой кроме самого Jetty, есть еще несколько часто используемых нами библиотек (например: <code>slf4j</code>, <code>logback</code>, <code>mysql</code> драйвер, библиотеки connection pooling&rsquo;а и некоторые другие). Сборка представлена в виде rpm пакета, что позволяет очень быстро подготавливать новую машину: <code>yum install jdk jetty</code>, готово.</p>

<p>Вас может удивило почему мы ставим на production машины JDK, а не JRE. Это связано с наличием в JDK очень полезных для диагностики утилит, таких как: <code>jmap</code>, <code>jstack</code>, <code>jvisualvm</code> и других. Это выгодно отличает платформу Java от множества других, &ndash; масса утилит для диагностики, которые в большинстве случаев позволяют довольно быстро понять что не так с приложением.</p>

<p>Деплой приложения состоит из следующих шагов:</p>

<ul>
  <li>распространение бинарника с приложением, а также конфигурации на целевые машины;</li>
  <li>перезапуск servlet container&rsquo;а;</li>
  <li>deployment тестирование.</li>
</ul>

<p>Естественно второй и третий шаг выполняются для всех машин по очереди.</p>

<h2 id="section">Распространение артефакта и конфигурации</h2>
<p>Тут все просто, <code>scp</code> доставляет файлы на каждую машину по очереди и ложит каждый в нужное место. Доставляются артефакты, а также конфигурационные файлы.</p>

<h3 id="servlet-container">Перезапуск servlet container&rsquo;а</h3>
<p>Мы не используем hot redeploy, так как он вызывает ряд проблем. В частности, небезизвестную <a href="http://www.jroller.com/agileanswers/entry/preventing_java_s_java_lang">OutOfMemoryError: PermGen</a>. Для перезапуска используются скрипты входящие в стандартную поставку Jetty с некоторыми несущественными дополнениями.</p>

<h3 id="deployment-">Deployment тестирование</h3>
<p>Этот момент стоит описать поподробнее. После того как Jetty запустился было бы неплохо проверить что приложение способно выполнять основные функции. Тот кто деплоит приложение может опечататься в конфигурации, системные администраторы могут забыть &ldquo;пропилить дырку&rdquo; в firewall&rsquo;е для необходимого приложению сервиса, мало ли что еще может произойти.</p>

<p>Задача deployment тестирования проверить что у приложения есть доступ ко всем необходимым сервисам для того чтобы оно выполняло свою работу. Это могут быть базы данных, очереди сообщений, сетевые файловые системы и другое middleware ПО. К счастью для нас, Spring Beans контейнер практически все делает за нас. Если во время инициализации контейнера (частью которого являются подключения к БД и т.д.) происходит ошибка, об этом легко узнать послав <code>GET</code> запрос на любой url, который обрабатывается непосредственно spring&rsquo;ом. У нас есть соглашение что url <code>/status</code> не занимается приложением и служит для deployment тестирования. Этот url также сообщает какая версия приложения запушена в данный момент. Таким образом, deploy скрипт после того как перезапускает servlet container начинает опрашивать приложение. Если приложение возвращает 200-й статус код, то можно переходить к обновлению следующей машины. Если нет, то процесс деплоя считается неуспешным и прерывается. Тот кто делает deploy может инициировать процедуру отката. Откат мы делаем руками, так как процент &ldquo;битых&rdquo; релизов у нас не велик и смысла автоматизировать эту процедуру пока что смысла нет.</p>

<h3 id="rollout--">Rollout распределенных приложений</h3>
<p>Когда приложение запущено в нескольких экземплярах (на нескольких машинах), то после запуска приложения на одном ноде имеет смысл немного подождать прежде чем переходить к следующей машине. Здесь играют свою роль как особенности приложения, так и особенности платформы. Java, как русские &ndash; &ldquo;долго запрягает, но быстро едет&rdquo; (начальная загрузка байт кода, создание пула соединений, JIT компилятор etc). Нас такой tradeoff вполне устраивает, поэтому наш deploy скрипт может быть сконфигурирован таким образом, чтобы давать только что запущенному ноду &ldquo;разогреться&rdquo; перед тем как принять на себя нагрузку его еще не обновленных коллег. Как правило, 3-5 секунд достаточно.</p>

<p>Деплой на несколько машин (особенно учитывая их &ldquo;разогрев&rdquo;) порождает интересную проблему. Во время обновления кластера, на нем работает <em>две версии приложения</em>. Некоторых людей этот факт ставит в ступор. Они не могут ужиться с тем, что у приложения в любой момент времени нет строго определенной версии.</p>

<p>Суровая реальность заключается в том, что требование работать одновременно в &ldquo;нескольких версиях&rdquo; порождается природой web-приложений. Мы стремимся к тому чтобы наше система всегда была online. Это имеет одно очень важное последствие для процесса deploy&rsquo;я:</p>

<blockquote>
  <p>Мы всегда должны иметь возможность откатиться на предыдущую версию системы, так как пусть редко, но бывает что ошибка все же проходит сквозь все рубежи тестирования и попадает на production. Причем rollback должен осуществлятся не медленнее чем rollout.</p>
</blockquote>

<p>Это приводит нас к следующему заключению: <em>следующая версия системы всегда должна быть обратно-совместима с предыдущей</em>. Это касается не только кода, но и схемы БД, данных в кеше и т.д.</p>

<p>Оказывается, что когда новая версия системы обратно совместима с текущей, нет ничего страшного в том чтобы некоторое время они поработали вместе.</p>

<h2 id="section-1">На практике</h2>
<p>Итак, как это все происходит на практике. Разработчик заходит через <code>ssh</code> на машину-координатор, куда автоматически доставляются артефакты приложений. У каждого приложения есть отдельная директория содержимое которой выглядит примерно следующим образом:</p>

<pre class="shell"><code>-rw-rw-r-- 1 tech tech      101 Jun 24 16:42 .config
-rw-rw-r-- 1 tech tech     1134 Jun  9 13:05 config.properties
-rw-r--r-- 1 tech tech      163 May 16 16:11 .jettyrc
-rw-r--r-- 1 tech tech 30298960 Jun  3 19:41 search-web-frontend-1.0.118.war
-rw-r--r-- 1 tech tech 30299011 Jun  3 21:25 search-web-frontend-1.0.119.war
-rw-r--r-- 1 tech tech 30298949 Jun 13 21:51 search-web-frontend-1.0.120.war
-rw-r--r-- 1 tech tech 30297647 Jun 14 12:38 search-web-frontend-1.0.121.war
-rw-r--r-- 1 tech tech 30297689 Jun 15 11:50 search-web-frontend-1.0.122.war
-rw-r--r-- 1 tech tech 30298356 Jun 15 12:13 search-web-frontend-1.0.123.war
-rw-r--r-- 1 tech tech 30298678 Jun 15 12:32 search-web-frontend-1.0.124.war
-rw-r--r-- 1 tech tech 30732203 Jun 23 11:45 search-web-frontend-1.0.125.war
-rw-r--r-- 1 tech tech 30732204 Jun 23 11:55 search-web-frontend-1.0.126.war
</code></pre>

<p>Здесь есть несколько файлов о которых стоит рассказать поподробнее. Файл <code>.config</code> это обыкновенный <code>ini</code>-файл хранящий имена всех машин на которых установлено приложение, а также некоторые другие настройки развертывания. <code>config.properties</code> — это <code>properties</code>-файл, который содержит настройки приложения. <code>.jettyrc</code> содержит startup опции виртуальной машины на которой стартует Jetty.</p>

<p>За довольно длительное время мы перепробовали различные способы работы с конфигурационными файлами приложений. Мы хранили их в development системе контроля версий. Мы создавали для них отдельный репозиторий в зоне production. Текущая схема нам нравится больше всего. Она позволяет автоматически бекапить все конфигурационные файлы приложений, что безусловно необходимо, а также не связываться с излишей сложностью VCS систем для управления &ldquo;двумя файлами из 10 строчек каждый&rdquo;.</p>

<p>Если запустить команду deploy без аргументов, то она выведет текущее состояние нодов:</p>

<pre class="shell"><code>$ deploy 
http://search-service1:8080 - Ok com.farpost.search:search-web-frontend 1.0.126
http://search-service2:8080 - Ok com.farpost.search:search-web-frontend 1.0.126
http://search-service3:8080 - Ok com.farpost.search:search-web-frontend 1.0.126
</code></pre>

<p>Благодаря <a href="http://maven.apache.org/">maven</a> во все наши сборки автоматически попадает информация о версии, а также номере билда.</p>

<p>Если же передать команде deploy имя артефакта, то начнется его развертывание на production машинах.</p>

<h2 id="section-2">Дальнейшие соображения</h2>
<p>Текущий процесс нас вполне устраивает, тем не менее у нас есть идеи как сделать его еще лучше.</p>

<h3 id="runtime---">Runtime обновление конфигурации логгирования</h3>
<p>Мы используем <a href="http://logback.qos.ch/">logback</a> в качестве библиотеки логгирования. Существенным ее плюсом является то что она позволяет <a href="http://logback.qos.ch/manual/configuration.html#autoScan">менять конфигурацию логгирования на лету</a>. Достаточно просто поменять XML файл с конфигурацией. Было бы неплохо иметь возможность распространять конфигурацию логгирования на машины без перезагрузки самого приложения.</p>

<h3 id="section-3">Дифференцирование конфигурации различных нодов</h3>
<p>В распределенной системе разные ноды могут иметь идентичную сборку, но разную конфигурацию, обуславливающую требуемую разность в их поведении. Сейчас у нас пока что нет возможности задать разную конфигурацию для разных нодов. Учитывая что конфигурация задается в виде <code>properties</code> файлов, сделать такого рода дифференциорание не сложно.</p>

<h3 id="partial-deploy">Partial deploy</h3>
<p>Иногда бывает необходимо обновить не весь кластер, а только один нод из кластера. В будущем, я думаю мы сделаем такую возможность.</p>

<p>Вот пожалуй и все. А как деплоите приложения вы? ;)</p>

</description>
    </item>
    
    <item>
      <title>Fair lock</title>
      <link>http://bazhenov.me/blog/2011/04/17/fair-lock.html</link>
      <pubDate>Sun, 17 Apr 2011 00:00:00 +1100</pubDate>
      <author>dotsid@gmail.com (Denis Bazhenov)</author>
      <guid>http://bazhenov.me/blog/2011/04/17/fair-lock</guid>
      <description><p>Не так давно у нас на собеседовании был кандидат, который произвел довольно хорошее впечатление, поэтому было решено предложить ему более сложную задачу, которую обычно мы не спрашиваем. Вот ее немного видоизмененный вариант.</p>

<!-- excerpt -->

<blockquote>
  <p>Переделайте следующий код оставив его многопоточным таким образом, чтобы лампочки зажигались и гасли строго по очереди и в любой момент времени должна быть включена только одна лампочка:</p>
</blockquote>

<pre class="code"><code>package me.bazhenov.bulb;

public class Main {

	public static void main(String[] args) {
		new Thread(new Bulb("first")).start();
		new Thread(new Bulb("seconds")).start();
	}
}

public class Bulb implements Runnable {

	private final String name;

	public Bulb(String name) {
		this.name = name;
	}

	public void run() {
		Thread self = currentThread();
		while(!self.isInterrupted()) {
			System.out.println(name + " bulb is on");
			try {
				sleep(300);
			} catch (InterruptedException e) {
				self.interrupt();
			}
			System.out.println(name + " bulb is off");
		}
	}
}
</code></pre>

<p>Кандидат предложил использовать <a href="http://download.oracle.com/javase/1.5.0/docs/api/java/util/concurrent/locks/ReentrantLock.html"><code>ReentrantLock</code></a> в <code>FairSync</code> режиме. В первом приближении эта идея может показаться рабочей. Передать общий лок в оба оъекта типа Bulb и синхронизироватся там на нем. Тем не менее, этот подход не работает. Если мы заглянем в документацию к классу, то увидим следующее описание:</p>

<blockquote>
  <p>The constructor for this class accepts an optional fairness parameter. When set true, under contention, locks favor granting access to the longest-waiting thread. Otherwise this lock does not guarantee any particular access order. [&hellip;] Note however, that fairness of locks does not guarantee fairness of thread scheduling.</p>
</blockquote>

<p><code>FairSync</code> не гарантирует отсутствие race condition&rsquo;а между потоками. Единственное что он гарантирует это то, что лок возьмет поток который ждал на локе дольше всего. Отсутствие контроля за CPU шедулером не дает нам гарантии что в момент когда поток отпускает лок его оппонент уже попытался сделать acquire на этом же локе (что необходимо для того чтобы сработал FairSync в этой задаче).</p>

<p>К сожалению у меня нет под рукой соответствующего железа, но я подозреваю что на однопроцессорной машине разницы между FairSync и NonfairSync вообще не будет, так как у параллельного потока не будет возможности поставить в очередь заявку на acquire, чтобы при следующем unlock&rsquo;е его заявка была обслужена.</p>

<p>Правильное же решение задачи остается на совести читателя :)</p>

</description>
    </item>
    
  </channel> 
</rss>