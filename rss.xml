<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Суровая реальность</title>
    <link>http://bazhenov.me</link>
    <atom:link href="http://bazhenov.me/rss.xml" rel="self" type="application/rss+xml" />
    <description>Суровая реальность</description>
    <language>ru-ru</language>
    <pubDate>Tue, 11 Dec 2012 20:41:49 +1100</pubDate>
    <lastBuildDate>Tue, 11 Dec 2012 20:41:49 +1100</lastBuildDate>
    
    <item>
      <title>Feature selection в алгоритмах классификации</title>
      <link>http://bazhenov.me/blog/2012/12/10/feature-selection.html</link>
      <pubDate>Mon, 10 Dec 2012 00:00:00 +1100</pubDate>
      <author>Denis Bazhenov (dotsid@gmail.com)</author>
      <guid isPermaLink="true">http://bazhenov.me/blog/2012/12/10/feature-selection.html</guid>
      <description>&lt;p&gt;Существует один очень простой и эффективный способ улучшения алгоритмов классификации, который называется &lt;a href=&quot;http://en.wikipedia.org/wiki/Feature_selection&quot;&gt;feature selection&lt;/a&gt; (выбор классификационных признаков). Этот метод позволяет при построении модели выбрать только самые показательные признаки (например, слова) и отсеять остальные.&lt;/p&gt;

&lt;!-- excerpt --&gt;

&lt;p&gt;Что такое показательные признаки? Если мы говорим о задаче классификации текстовых документов, то это слова которые несут информацию о классе к которому относится документ. Например, в контексте автомобильной тематики слово &amp;ldquo;Nissan&amp;rdquo;, скорее всего будет показательным признаком, а вот слово &amp;ldquo;новый&amp;rdquo; вряд ли. Использование для классификации не всех слов, а только показательных дает несколько преимуществ.&lt;/p&gt;

&lt;p&gt;Во-первых, feature selection позволяет существенно уменьшить количество параметров модели (используемых для классификации слов), и как следствие снижает требования к объему памяти требуемой для классификации.&lt;/p&gt;

&lt;p&gt;Во-вторых, feature selection может повысить точность алгоритма за счет удаления из модели слов с низким соотношением сигнал/шум. Представьте, что слово &amp;ldquo;аскетичный&amp;rdquo; встретилось в обучающей выборке 3 раза, – 1 раз в рамках класса &amp;ldquo;Авто&amp;rdquo; и 2 раза в рамках класса &amp;ldquo;Одежда&amp;rdquo;. Формально оно говорит в пользу класса &amp;ldquo;Одежда&amp;rdquo;, но вряд ли это слово можно считать хорошим классификационным признаком. Скорее всего, перевес в сторону одежды это случайность.&lt;/p&gt;

&lt;p&gt;Но &lt;em&gt;сам факт наличия такого слова в обучающей выборке не случайность&lt;/em&gt;. Это следствие &lt;a href=&quot;http://ru.wikipedia.org/wiki/Закон_Ципфа&quot;&gt;закона Зипфа&lt;/a&gt; (Zipf&amp;rsquo;s law), который описывает распределение частот слов в натуральном языке. Простым языком это можно описать следующим образом, если вы возьмете любой достаточно большой корпус документов и посчитаете сколько в нем слов встречающихся ровно 1 раз, 2 раза, 3 раза и т.д., то окажется что большинство слов встречаются 1 раз. Это не должно быть большим сюрпризом, – в повседневной жизни мы используем довольно небольшое количество слов (высокочастотники), а большую часть слов мы практически не используем (низкочастотники).&lt;/p&gt;

&lt;h2 id=&quot;mutual-information&quot;&gt;Mutual Information&lt;/h2&gt;

&lt;p&gt;В этой заметке я опишу один, пожалуй самый простой способ оценки показательности классификационных признаков, – &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html&quot;&gt;метод взаимной информации&lt;/a&gt; (Mutual Information). Идея метода очень проста. Зная как часто слово употребляется в пределах документов класса и за его пределами, мы можем сказать насколько статистически сильно связаны слово и класс.&lt;/p&gt;

&lt;p&gt;Для того чтобы рассчитать численное значение взаимной информации необходимо составить матрицу цитирумости. Матрица цитируемости – это матрица 2x2 которая показывает взаимоотношение конкретного слова с конкретным классов. Возьмем, к примеру слово &amp;ldquo;Toyota&amp;rdquo; и класс &amp;ldquo;Авто&amp;rdquo;.&lt;/p&gt;

&lt;table align=&quot;center&quot;&gt;
	&lt;tr&gt;
		&lt;th /&gt;
		&lt;th&gt;C&lt;sub&gt;Авто&lt;/sub&gt;=1&lt;/th&gt;
		&lt;th&gt;C&lt;sub&gt;Авто&lt;/sub&gt;=0&lt;/th&gt;
	&lt;/tr&gt;
	&lt;tr&gt;
		&lt;th&gt;W&lt;sub&gt;Toyota&lt;/sub&gt;=1&lt;/th&gt;
		&lt;td&gt;N&lt;sub&gt;11&lt;/sub&gt;=65 342&lt;/td&gt;
		&lt;td&gt;N&lt;sub&gt;10&lt;/sub&gt;=143&lt;/td&gt;
	&lt;/tr&gt;
	&lt;tr&gt;
		&lt;th&gt;W&lt;sub&gt;Toyota&lt;/sub&gt;=0&lt;/th&gt;
		&lt;td&gt;N&lt;sub&gt;01&lt;/sub&gt;=45 342&lt;/td&gt;
		&lt;td&gt;N&lt;sub&gt;00&lt;/sub&gt;=897 657&lt;/td&gt;
	&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;Из этой матрицы видно что слово &amp;ldquo;Toyota&amp;rdquo; было встречено в 65 342 документах в контексте класса &amp;ldquo;Авто&amp;rdquo;, а также в 143 документах за переделами класса &amp;ldquo;Авто&amp;rdquo;. Имея на руках такую матрицу мы можем рассчитать взаимную информацию по формуле:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; MI =
\frac{N_{11}}{N} \log_2{ \frac{N N_{11}}{N_{1.} N_{.1}} } +
\frac{N_{01}}{N} \log_2{ \frac{N N_{01}}{N_{0.} N_{.1}} } + 
\frac{N_{10}}{N} \log_2{ \frac{N N_{10}}{N_{1.} N_{.0}} } +
\frac{N_{00}}{N} \log_2{ \frac{N N_{00}}{N_{0.} N_{.0}} } &lt;/script&gt;

&lt;p&gt;где, &lt;script type=&quot;math/tex&quot;&gt; N &lt;/script&gt; — сумма всей матрицы, &lt;script type=&quot;math/tex&quot;&gt; N_{0.} = N_{00} + N_{01} &lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt; N_{.1} = N_{01} + N_{11} &lt;/script&gt; и т.д.&lt;/p&gt;

&lt;p&gt;Взаимная информация всегда находится в диапазоне от нуля до единицы. Чем выше значение, тем сильнее связь между наличием/отсутствием слова и наличием/отсутствием класса. Рассчитав взаимную информацию для всех пар слово-класс мы можем оставить для каждого класса первые N слов, объединить эти слова в общий словарь и теперь только их использовать для классификации. Количество слов необходимых для оптимальной классификации зависит от конкретной задачи и его необходимо подбирать эмпирически, постоянно проверяя результаты на тестовой выборке.&lt;/p&gt;

&lt;p&gt;На тех задач на которых я тестировал этот метод, он давал прирост точности от 3 до 10%. В Introduction to Infromation Retrieval указано что на практике возможен прирост в десятки процентов по F&lt;sub&gt;1&lt;/sub&gt;&lt;sup id=&quot;fnref:ref-mi-performance&quot;&gt;&lt;a href=&quot;#fn:ref-mi-performance&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Все зависит от вашей конкретной задачи, а также классификационного алгоритма.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:ref-mi-performance&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/mutual-information-1.html#fig:mccallum&quot;&gt;Introduction to Information Retrieval, 2008 Cambridge University Press&lt;/a&gt;&lt;a href=&quot;#fnref:ref-mi-performance&quot; rel=&quot;reference&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Толерантный автокомплит</title>
      <link>http://bazhenov.me/blog/2012/08/04/autocomplete.html</link>
      <pubDate>Sat, 04 Aug 2012 00:00:00 +1100</pubDate>
      <author>Denis Bazhenov (dotsid@gmail.com)</author>
      <guid isPermaLink="true">http://bazhenov.me/blog/2012/08/04/autocomplete.html</guid>
      <description>&lt;p&gt;Автокомплит вещь удобная. Он позволяет экономить время на наборе текста, когда множество значений поля закрыто. Хороший автокомплит отличается следующими качествами:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;он должен быть быстрый. Если мы хотим экономить силы пользователя, то мы должны ему предложить вариант как можно быстрее;&lt;/li&gt;
  &lt;li&gt;он не должен предлагать к вводу варианты которые заведомо неверны;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;он должен быть толерантен к пользовательскому вводу&lt;/em&gt;. В том числе прощать опечатки.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Вот о последнем качестве autocomplete алгоритмов я и хочу сегодня поговорить.&lt;/p&gt;

&lt;!-- excerpt --&gt;

&lt;p&gt;Если вы хотите создать приложение толерантное к вводимой пользователем информации, то исправление опечаток в том или ином виде должно быть. Человек может опечататься в первой букве, может попросту не знать как правильно пишется слово. Поэтому, не очень хорошо что первая же неверно введенная пользователем буква приведет к тому что он не сможет выбрать правильный вариант.&lt;/p&gt;

&lt;p&gt;Я значительную часть рабочего дня провожу в IntelliJ IDEA. Отличная среда, пожалуй лучшая. А еще, я &lt;a href=&quot;http://bazhenov.me/blog/2012/05/12/functional-java.html&quot;&gt;использую guava&lt;/a&gt;, хорошая библиотека. И есть там такой метод &lt;code&gt;Closeables.closeQuietly()&lt;/code&gt;, который закрывает &lt;code&gt;Closeable&lt;/code&gt; объект подавляя все исключения. Так вот, когда вы пишете имя метода которого нет в текущем классе, IDEA автоматически предлагает вам сделать &lt;code&gt;static import&lt;/code&gt; подходящего метода из других классов.&lt;/p&gt;

&lt;p class=&quot;image photo&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/autocomplete/idea-good.png&quot; alt=&quot;IntelliJ IDEA static import&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Прекрасная фича, которая не работает если вы опечатались. А я ну никак не могу запомнить как правильно пишется слово &amp;ldquo;quietly&amp;rdquo;.&lt;/p&gt;

&lt;p class=&quot;image photo&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/autocomplete/idea-bad.png&quot; alt=&quot;IntelliJ IDEA static import reject&quot; /&gt;&lt;/p&gt;

&lt;p&gt;В этом случае IDE &amp;ldquo;фейлит&amp;rdquo; и предлагает мне создать новый метод. Примерно то же самое происходит и с автодополнением. Стоит мне ввести хотя бы один не тот символ, как тут же &amp;ldquo;No suggestions&amp;rdquo;. Я считаю, IDEA могла бы быть гораздо более толерантна к девелоперу на этапе ввода кода. Это же не какой-нибудь там NetBeans :)&lt;/p&gt;

&lt;p&gt;Этой же проблемой страдает большое количество продуктов.&lt;/p&gt;

&lt;p class=&quot;image photo&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/autocomplete/mac-help.png&quot; alt=&quot;Mac OS X Help Search&quot; /&gt;&lt;/p&gt;
&lt;p class=&quot;description&quot;&gt;Встроенный в Mac OS X поиск по позиция системного меню&lt;/p&gt;

&lt;p class=&quot;image photo&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/autocomplete/mac-spotlight.png&quot; alt=&quot;Mac OS X Spotlight Search&quot; /&gt;&lt;/p&gt;
&lt;p class=&quot;description&quot;&gt;Mac OS X Spotlight&lt;/p&gt;

&lt;p class=&quot;image photo&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/autocomplete/money.png&quot; alt=&quot;Money&quot; /&gt;&lt;/p&gt;
&lt;p class=&quot;description&quot;&gt;Money — ПО для учета персональных финансов&lt;/p&gt;

&lt;p&gt;Такая ситуация выглядит для меня довольно странно, так как алгоритмы позволяющие разруливать большую часть опечаток при вводе довольно банальны.&lt;/p&gt;

&lt;p&gt;Конечно, разработка первоклассного spellchecker&amp;rsquo;а дело не простое. Оно требует обучения не тривиальных статистических моделей. Но есть несколько простых подходов, которые позволяют в большинстве приложений сделать автокомплит достаточно толерантным, чтобы пользователи были счастливы в 80% случаев.&lt;/p&gt;

&lt;h2 id=&quot;section&quot;&gt;Подход №1: Редакционное расстояние&lt;/h2&gt;

&lt;p&gt;Самое простое решение заключается в том чтобы сортировать все позиции словаря по возрастанию редакционного расстояния и показывать только первые несколько позиций. В качестве редакционного расстояния можно взять &lt;a href=&quot;http://ru.wikipedia.org/wiki/Расстояние_Левенштейна&quot;&gt;расстояние Левенштейна&lt;/a&gt;. Расстояние Левенштейна – это минимальное количество операций вставок/удаления/изменения символов необходимое для того чтобы преобразовать исходную строку в целевую.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Список слов близких к слову &amp;ldquo;пазор&amp;rdquo; и их редакционные расстояния: позор &amp;rarr; 1, позер &amp;rarr; 2, дозор &amp;rarr; 2, помор &amp;rarr; 2, побор &amp;rarr; 2, подзор &amp;rarr; 2, покер &amp;rarr; 3, покос &amp;rarr; 3.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Существует статистика что в пределах расстояния Левенштейна 2 находится более 90% опечаток, так что можно показывать только их, чтобы избежать совсем уж неадекватных исправлений.&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;Достоинства&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;довольно проста в реализации. Имплементацию расчёта расстояния Левенштейна можно найти практически для любого языка программирования.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-2&quot;&gt;Недостатки&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Довольно ресурсоемкая. Алгоритмическая сложность алгоритма O(n&lt;sup&gt;2&lt;/sup&gt;). Если в вашем словаре много позиций или у вас много посетителей на данное решение может не хватить аппаратных ресурсов.&lt;/li&gt;
  &lt;li&gt;в данном виде этот подход не позволяет использовать знания о языковой модели, что не дает корректно исправлять некоторые типичные опечатки. Например опечатка &amp;ldquo;corola&amp;rdquo; находится на расстоянии 1 и от &amp;ldquo;corolla&amp;rdquo; и от &amp;ldquo;corona&amp;rdquo;, соответственно две эти замены будут равновероятны. Но пропуск дублирующей &amp;ldquo;l&amp;rdquo; гораздо более вероятная опечатка чем замена &amp;ldquo;l&amp;rdquo; на &amp;ldquo;n&amp;rdquo;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;В целом, решение на основании расстояния Левенштейна будет вполне сносно работать в первом приближении. Его очень просто реализовать, что делает данный подход отличным кандидатом на роль прототипа.&lt;/p&gt;

&lt;h2 id=&quot;k-gram-&quot;&gt;Подход №2: Коэффициент Жаккара и K-gram индекс&lt;/h2&gt;

&lt;p&gt;Альтернативный подход заключается в том чтобы использовать &lt;a href=&quot;http://ru.wikipedia.org/wiki/Коэффициент_Жаккара&quot;&gt;коэффициент Жаккара&lt;/a&gt; как метрику расстояния между строками. Коэффициент Жаккара (Jaccard index) это индекс сходства двух множеств который определяется как отношение мощности пересечения этих множеств к мощности их объединения.&lt;/p&gt;

&lt;p class=&quot;image&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/autocomplete/sets.png&quot; alt=&quot;Sets&quot; /&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; J = \frac{\mid A \cap B \mid} {\mid A \cup B \mid} &lt;/script&gt;

&lt;p&gt;При коэффициенте Жаккара равном 1 множества равны, при 0 не имеют ни одного общего элемента. Эту метрику можно использовать для оценки близости вводимых пользователем слов к словам из нашего словаря по которому мы делаем autocomplete.&lt;/p&gt;

&lt;p&gt;Но для того чтобы иметь возможность расчитать коэффициент Жаккара для двух строк нам надо преобразовать их в множества. Самый простой вариант, разбить строку на символы. Но обычно берут не символы строки, а k-граммы строки (также известны как n-граммы). &lt;a href=&quot;http://en.wikipedia.org/wiki/N-gram&quot;&gt;K-грамма&lt;/a&gt; – это непрерывная уникальная последовательность из k символов строки. Дублирующие k-граммы пропускают. Например, в слове &amp;ldquo;клоун&amp;rdquo; 3 триграммы (k = 3): &amp;ldquo;кло&amp;rdquo;, &amp;ldquo;лоу&amp;rdquo; и &amp;ldquo;оун&amp;rdquo;. На практике, k выбирают равным в диапазоне от двух до четырех, в зависимости от характера данных.&lt;/p&gt;

&lt;p&gt;Cформировав из строк множество k-грамм, мы можем расчитать коэффициент Жаккара между этими строками. Например, триграммный коэффициент Жаккара для пары &amp;ldquo;corolla&amp;rdquo; и &amp;ldquo;corola&amp;rdquo; будет выше, чем для пары &amp;ldquo;corona&amp;rdquo; и &amp;ldquo;corola&amp;rdquo;. В первом случае совпадают три триграммы: &amp;ldquo;cor&amp;rdquo;, &amp;ldquo;oro&amp;rdquo; и &amp;ldquo;rol&amp;rdquo;, а во втором случае только две: &amp;ldquo;cor&amp;rdquo; и &amp;ldquo;oro&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;В контексте алгоритма автодополнения, для того чтобы иметь возможность быстро подбирать кандидатов на пользователский запрос, необходимо построить &lt;a href=&quot;http://nlp.stanford.edu/IR-book/html/htmledition/k-gram-indexes-for-spelling-correction-1.html&quot;&gt;k-gram индекс&lt;/a&gt; по словарю автодополнения. K-gram индекс это инвертированный индекс из k-граммы в слова содержащие эту k-грамму. K-gram индекс позволяет  быстро находить элементы с наиболее высоким коэффициентом Жаккара предварительно проиндексировав их.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Биграммный индекс (k = 2) для слов: &amp;ldquo;топор&amp;rdquo;, &amp;ldquo;компот&amp;rdquo; и &amp;ldquo;оптом&amp;rdquo; выглядит следующим образом:&lt;/p&gt;

  &lt;ul&gt;
    &lt;li&gt;то &amp;rarr; оптом, топор&lt;/li&gt;
    &lt;li&gt;оп &amp;rarr; оптом, топор&lt;/li&gt;
    &lt;li&gt;по &amp;rarr; компот, топор&lt;/li&gt;
    &lt;li&gt;ор &amp;rarr; топор&lt;/li&gt;
    &lt;li&gt;ко &amp;rarr; компот&lt;/li&gt;
    &lt;li&gt;ом &amp;rarr; компот, оптом&lt;/li&gt;
    &lt;li&gt;мп &amp;rarr; компот&lt;/li&gt;
    &lt;li&gt;от &amp;rarr; компот&lt;/li&gt;
    &lt;li&gt;пт &amp;rarr; оптом&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;Имея k-gramm индекс алгоритм автодополнения сводится к следующим шагам:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;разбить на k-граммы строку введенную пользователем;&lt;/li&gt;
  &lt;li&gt;для каждой k-граммы получить список слов в которых она встречается (posting list);&lt;/li&gt;
  &lt;li&gt;сделать merge posting list&amp;rsquo;ов с подсчетом какое количество раз каждое слово встречается в них. По факту это количество совпавших k-грамм между строкой пользователя и позицией словаря.&lt;/li&gt;
  &lt;li&gt;отсортировать этот список по убыванию количества совпавших k-грамм;&lt;/li&gt;
  &lt;li&gt;вернуть первые N позиций пользователю.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;section-3&quot;&gt;Достоинства&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;данная подход позволяет эффективно находить строки синтаксически близкие к целевой (с максимальным количеством пересекающихся k-грамм). Существенно быстрее чем редакционное расстояние.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;section-4&quot;&gt;Недостатки&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;как и редакционное расстояние, не позволяет использовать информацию языковой модели, что не позволяет исправлять сложные типы опечаток;&lt;/li&gt;
  &lt;li&gt;более сложна в реализации чем подход с редакционным расстоянием.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tips--tricks&quot;&gt;Tips &amp;amp; tricks&lt;/h2&gt;

&lt;p&gt;Некоторые замечания которые могут быть полезны, если вы захотите имплементировать толерантный автокомплит.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;K-gram индекс можно хранить в любой удобной для вас inverted index системе. Например, Apache Lucene;&lt;/li&gt;
  &lt;li&gt;подходы №1 и №2 не являются взаимоисключающими. Очень часто K-gram индекс из за его эффективности используют как предварительный фильтр для других, более сложных алгоритмов;&lt;/li&gt;
  &lt;li&gt;может иметь смысл предварительно убирать из текста все символы не несущие смысловой нагрузки. Например, дефисы, слэши и т.д;&lt;/li&gt;
  &lt;li&gt;при построении k-gram из слова/фразы имеет смысл добавить в исходную строку какой-нибудь специальный символ (например, &amp;ldquo;$&amp;rdquo;) на границе слов. Это сделает результаты более точным, за счет дополнительной совпадающей k-граммы на границах.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;e&quot;&gt;В заключениe&lt;/h2&gt;
&lt;p&gt;Мы рассмотрели два основных инструмента которые позволяют находить и исправлять опечатки. Они могут быть использованы как в чистом виде, так и в составе более сложных стратегий автодополнения. Надеюсь, эта информация даст вам общее представление о том как сделать более толерантное к опечаткам автодополнение.&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>Оценка классификатора (точность, полнота, F-мера)</title>
      <link>http://bazhenov.me/blog/2012/07/21/classification-performance-evaluation.html</link>
      <pubDate>Sat, 21 Jul 2012 00:00:00 +1100</pubDate>
      <author>Denis Bazhenov (dotsid@gmail.com)</author>
      <guid isPermaLink="true">http://bazhenov.me/blog/2012/07/21/classification-performance-evaluation.html</guid>
      <description>&lt;p&gt;Продолжая тему реализации &lt;a href=&quot;http://bazhenov.me/blog/2012/06/11/naive-bayes.html&quot;&gt;автоматической классификации&lt;/a&gt; необходимо обсудить следующий очень важный вопрос. Как оценивать качество алгоритма? Допустим, вы хотите внести изменения в алгоритм. Откуда вы знаете что эти изменения сделают алгоритм лучше? Конечно же надо проверять алгоритм на реальных данных.&lt;/p&gt;

&lt;!--excerpt--&gt;

&lt;h2 id=&quot;section&quot;&gt;Тестовая выборка&lt;/h2&gt;

&lt;p&gt;Основой проверки является тестовая выборка в которой проставлено соответствие между документами и их классами. В зависимости от ваших конкретных условий получение подобной выборки может быть затруднено, так как зачастую ее составляют люди. Но иногда ее можно получить без большого объема ручной работы, если проявить изобретательность. Каких-то конеретных рецептов, к сожалению, не существует.&lt;/p&gt;

&lt;p&gt;Когда у вас появилась тестовая выборка достаточно натравить классификатор на документы и соотнести его решение с заведомо известным правильным решением. Но для того чтобы принимать решение хуже или лучше справляется с работой новая версия алгоритма &lt;em&gt;нам необходима численная метрика его качества&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;Численная оценка качества алгоритма&lt;/h2&gt;

&lt;h3 id=&quot;accuracy&quot;&gt;Accuracy&lt;/h3&gt;

&lt;p&gt;В простейшем случае такой метрикой может быть доля документов по которым классификатор принял правильное решение.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Accuracy = \frac{P}{N}&lt;/script&gt;

&lt;p&gt;где, &lt;script type=&quot;math/tex&quot;&gt;P&lt;/script&gt; – количество документов по которым классификатор принял правильное решение, а &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; – размер обучающей выборки. Очевидное решение, на котором для начала можно остановиться.&lt;/p&gt;

&lt;p&gt;Тем не менее, у этой метрики есть одна особенность которую необходимо учитывать. Она присваивает всем документам одинаковый вес, что может быть не корректно в случае если распределение документов в обучающей выборке сильно смещено в сторону какого-то одного или нескольких классов. В этом случае у классификатора есть больше информации по этим классам и соответственно в рамках этих классов он будет принимать более адекватные решения. На практике это приводит к тому, что вы имеете accuracy, скажем, 80%, но при этом в рамках какого-то конкретного класса классификатор работает из рук вон плохо не определяя правильно даже треть документов.&lt;/p&gt;

&lt;p&gt;Один выход из этой ситуации заключается в том чтобы обучать классификатор на специально подготовленном, сбалансированном корпусе документов. Минус этого решения в том что вы отбираете у классификатора информацию об отностельной частоте документов. Эта информация при прочих равных может оказаться очень кстати для принятия правильного решения.&lt;/p&gt;

&lt;p&gt;Другой выход заключается в изменении подхода к формальной оценке качества.&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;Точность и полнота&lt;/h3&gt;

&lt;p&gt;Точность (precision) и полнота (recall) являются метриками которые используются при оценке большей части алгоритмов извлечения информации. Иногда они используются сами по себе, иногда в качестве базиса для производных метрик, таких как F-мера или R-Precision. Суть точности и полноты очень проста.&lt;/p&gt;

&lt;p&gt;Точность системы в пределах класса – это доля документов действительно принадлежащих данному классу относительно всех документов которые система отнесла к этому классу. Полнота системы – это доля найденных классфикатором документов принадлежащих классу относительно всех документов этого класса в тестовой выборке.&lt;/p&gt;

&lt;p&gt;Эти значения легко рассчитать на основании таблицы контингентности, которая составляется для каждого класса отдельно.&lt;/p&gt;

&lt;p class=&quot;image&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/classification-performance-evaluation/contingency-table.png&quot; alt=&quot;Таблица контингентности&quot; /&gt;&lt;/p&gt;

&lt;p&gt;В таблице содержится информация сколько раз система приняла верное и сколько раз неверное решение по документам заданного класса. А именно:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;TP&lt;/script&gt; — истино-положительное решение;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;TN&lt;/script&gt; — истино-отрицательное решение;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;FP&lt;/script&gt; — ложно-положительное решение;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;FN&lt;/script&gt; — ложно-отрицательное решение.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Тогда, точность и полнота определяются следующим образом:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; Precision = \frac{TP}{TP+FP} &lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; Recall = \frac{TP}{TP+FN} &lt;/script&gt;

&lt;blockquote&gt;
  &lt;p&gt;Рассмотрим пример. Допустим, у вас есть тестовая выборка в которой 10 сообщений, из них 4 – спам. Обработав все сообщения классификатор пометил 2 сообщения как спам, причем одно действительно является спамом, а второе было помечено в тестовой выборке как нормальное. Мы имеем одно истино-положительное решение, три ложно-отрицательных и одно ложно-положительное. Тогда для класса &amp;ldquo;спам&amp;rdquo; точность классификатора составляет &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{2}&lt;/script&gt; (50% положительных решений правильные), а полнота &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{4}&lt;/script&gt; (классификатор нашел 25% всех спам-сообщений).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;confusion-matrix&quot;&gt;Confusion Matrix&lt;/h2&gt;

&lt;p&gt;На практике значения точности и полноты гораздо более удобней рассчитывать с использованием матрицы неточностей (confusion matrix). В случае если количество классов относительно невелико (не более 100-150 классов), этот подход позволяет довольно наглядно представить результаты работы классификатора.&lt;/p&gt;

&lt;p&gt;Матрица неточностей – это матрица размера N на N, где N — это количество классов. Столбцы этой матрицы резервируются за экспертными решениями, а строки за решениями классификатора. Когда мы классифицируем документ из тестовой выборки мы инкрементируем число стоящее на пересечении строки класса который вернул классификатор и столбца класса к которому действительно относится документ.&lt;/p&gt;

&lt;p class=&quot;image&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/classification-performance-evaluation/confusion-matrix.png&quot; alt=&quot;Матрица неточностей&quot; /&gt;&lt;/p&gt;

&lt;p class=&quot;description&quot;&gt;Матрица неточностей (26 классов, результирующая точность – 0.8, результирующая полнота – 0.91)&lt;/p&gt;

&lt;p&gt;Как видно из примера, большинство документов классификатор определяет верно. Диагональные элементы матрицы явно выражены. Тем не менее в рамках некоторых классов (3, 5, 8, 22) классификатор показывает низкую точность.&lt;/p&gt;

&lt;p&gt;Имея такую матрицу точность и полнота для каждого класса рассчитывается очень просто. Точность равняется отношению соответствующего диагонального элемента матрицы и суммы всей строки класса. Полнота – отношению диагонального элемента матрицы и суммы всего столбца класса. Формально:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Precision_c = \frac{A_{c,c}}{\sum_{i=1}^n A_{c,i}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Recall_c = \frac{A_{c,c}}{\sum_{i=1}^n A_{i,c}}&lt;/script&gt;

&lt;p&gt;Результирующая точность классификатора рассчитывается как арифметическое среднее его точности по всем классам. То же самое с полнотой. Технически этот подход называется macro-averaging.&lt;/p&gt;

&lt;h2 id=&quot;f-&quot;&gt;F-мера&lt;/h2&gt;

&lt;p&gt;Понятно что чем выше точность и полнота, тем лучше. Но в реальной жизни максимальная точность и полнота не достижимы одновременно и приходится искать некий баланс. Поэтому, хотелось бы иметь некую метрику которая объединяла бы в себе информацию о точности и полноте нашего алгоритма. В этом случае нам будет проще принимать решение о том какую реализацию запускать в production (у кого больше тот и круче). Именно такой метрикой является F-мера&lt;sup id=&quot;fnref:f-measure&quot;&gt;&lt;a href=&quot;#fn:f-measure&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;F-мера представляет собой &lt;a href=&quot;http://bazhenov.me/blog/2012/05/05/harmonic-mean.html&quot;&gt;гармоническое среднее&lt;/a&gt; между точностью и полнотой. Она стремится к нулю, если точность или полнота стремится к нулю.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F = 2 \frac{Precision \times Recall}{Precision + Recall}&lt;/script&gt;

&lt;p&gt;Данная формула придает одинаковый вес точности и полноте, поэтому F-мера будет падать одинаково при уменьшении и точности и полноты. Возможно рассчитать F-меру придав различный вес точности и полноте, если вы осознанно отдаете приоритет одной из этих метрик при разработке алгоритма.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;F = \left(\beta^2+1\right)\frac{Precision \times Recall}{\beta^2 Precision + Recall}&lt;/script&gt;

&lt;p&gt;где &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; принимает значения в диапазоне &lt;script type=&quot;math/tex&quot;&gt;0&lt;\beta&lt;1&lt;/script&gt; если вы хотите отдать приоритет точности, а при &lt;script type=&quot;math/tex&quot;&gt;\beta &gt; 1&lt;/script&gt; приоритет отдается полноте. При &lt;script type=&quot;math/tex&quot;&gt;\beta = 1&lt;/script&gt; формула сводится к предыдущей и вы получаете сбалансированную F-меру (также ее называют F&lt;sub&gt;1&lt;/sub&gt;).&lt;/p&gt;

&lt;p class=&quot;image&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/classification-performance-evaluation/F-1.png&quot; alt=&quot;Сбалансированная F-мера&quot; /&gt;&lt;/p&gt;
&lt;p class=&quot;description&quot;&gt;Сбалансированная F-мера&lt;/p&gt;

&lt;p class=&quot;image&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/classification-performance-evaluation/F-1-over-4.png&quot; alt=&quot;F-мера с приоритетом точности&quot; /&gt;&lt;/p&gt;
&lt;p class=&quot;description&quot;&gt;F-мера с приоритетом точности (&lt;script type=&quot;math/tex&quot;&gt;\beta^2 = \frac{1}{4}&lt;/script&gt;)&lt;/p&gt;

&lt;p class=&quot;image&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/classification-performance-evaluation/F-2.png&quot; alt=&quot;F-мера с приоритетом полноты&quot; /&gt;&lt;/p&gt;
&lt;p class=&quot;description&quot;&gt;F-мера с приоритетом полноты (&lt;script type=&quot;math/tex&quot;&gt;\beta^2 = 2&lt;/script&gt;)&lt;/p&gt;

&lt;p&gt;F-мера является хорошим кандидатом на формальную метрику оценки качества классификатора. Она сводит к одному числу две других основополагающих метрики: точность и полноту. Имея в своем распоряжении подобный механизм оценки вам будет гораздо проще принять решение о том являются ли изменения в алгоритме в лучшую сторону или нет.&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;Ссылки по теме&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;http://datamin.ubbcluj.ro/wiki/index.php/Evaluation_methods_in_text_categorization&quot;&gt;Evaluation methods in text categorization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://rushdishams.blogspot.com/2011/08/micro-and-macro-average-of-precision.html&quot;&gt;Micro and macro average of precision&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://ru.wikipedia.org/wiki/Информационный_поиск#.D0.9E.D1.86.D0.B5.D0.BD.D0.BA.D0.B8_.D1.8D.D1.84.D1.84.D0.B5.D0.BA.D1.82.D0.B8.D0.B2.D0.BD.D0.BE.D1.81.D1.82.D0.B8&quot;&gt;Информационный поиск: Оценка эффективности — Wikipedia&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Precision_and_recall&quot;&gt;Precision and Recall — Wikipedia&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://cmp.felk.cvut.cz/~hlavac/TeachPresEn/31PattRecog/13ClassifierPerformance.pdf&quot;&gt;Classifier performance evaluation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:f-measure&quot;&gt;
      &lt;p&gt;иногда встречаются названия: F-score или мера Ван Ризбергена.&lt;a href=&quot;#fnref:f-measure&quot; rel=&quot;reference&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Самый наивный классификатор</title>
      <link>http://bazhenov.me/blog/2012/06/11/naive-bayes.html</link>
      <pubDate>Mon, 11 Jun 2012 00:00:00 +1100</pubDate>
      <author>Denis Bazhenov (dotsid@gmail.com)</author>
      <guid isPermaLink="true">http://bazhenov.me/blog/2012/06/11/naive-bayes.html</guid>
      <description>&lt;p&gt;В &lt;a href=&quot;http://bazhenov.me/blog/2012/06/05/classification.html&quot;&gt;прошлой заметке&lt;/a&gt; я в общих чертах описал задачу классификации, а также традиционные подходы используемые для классификации текстовых документов. В этой заметке я более детально расскажу о том как работает самый простой, но вместе с тем один из самых часто используемых при обработке натуральных языков алгоритм классификации – &lt;a href=&quot;http://ru.wikipedia.org/wiki/Наивный_байесовский_классификатор&quot;&gt;наивный байесовский классификатор&lt;/a&gt;.&lt;/p&gt;

&lt;!-- excerpt --&gt;

&lt;p&gt;Заметка разбита на две части: &lt;a href=&quot;#theory&quot;&gt;теоретическая&lt;/a&gt;, в которой описаны аспекты классификации, и &lt;a href=&quot;#practice&quot;&gt;практическая&lt;/a&gt; часть построения классификатора. Если вы хотите быстро создать прототип классификатора, то обратитесь к практической части заметки, там на приводится пример классификатора. Также на github доступны &lt;a href=&quot;https://github.com/bazhenov/naive-bayes-example&quot;&gt;исходники примера&lt;/a&gt;. Если же вам интересны теоретические принципы работы классификации, то обратитесь к теоретической части заметки.&lt;/p&gt;

&lt;h2 id=&quot;theory&quot;&gt;Теория&lt;/h2&gt;

&lt;p&gt;Осторожно, в этой части заметки много формул.&lt;/p&gt;

&lt;p&gt;В основе NBC (Naïve Bayes Classifier) лежит, как вы уже могли догадаться, &lt;a href=&quot;http://ru.wikipedia.org/wiki/Теорема_Байеса&quot;&gt;теорема Байеса&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;$$ P(c|d) = \frac{ P(d|c)P(c) }{ P(d) } $$&lt;/p&gt;

&lt;p&gt;где,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$ P(c|d) $ — вероятность что документ $d$ принадлежит классу $c$, именно её нам надо рассчитать;&lt;/li&gt;
  &lt;li&gt;$ P(d|c) $ — вероятность встретить документ $d$ среди всех документов класса $c$;&lt;/li&gt;
  &lt;li&gt;$ P(c) $ — безусловная вероятность встретить документ класса $c$ в корпусе документов;&lt;/li&gt;
  &lt;li&gt;$ P(d) $ — безусловная вероятность документа $d$ в корпусе документов.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Её смысл на обывательском уровне можно выразить следующим образом. Теорема Байеса позволяет переставить местами причину и следствие. Зная с какой вероятностью причина приводит к некоему событию, эта теорема позволяет расчитать вероятность того что именно эта причина привела к наблюдаемому событию.&lt;/p&gt;

&lt;p&gt;Цель классификации состоит в том чтобы понять к какому классу принадлежит документ, поэтому нам нужна не сама вероятность, а наиболее вероятный класс. Байесовский классификатор использует &lt;a href=&quot;http://ru.wikipedia.org/wiki/Оценка_апостериорного_максимума&quot;&gt;оценку апостериорного максимума&lt;/a&gt; (Maximum a posteriori estimation) для определения наиболее вероятного класса. Грубо говоря, это класс с максимальной вероятностью.&lt;/p&gt;

&lt;p&gt;$$ c_{map}=\operatorname*{arg\,max}_{c \in C} \frac{ P(d|c)P(c) }{ P(d) }$$&lt;/p&gt;

&lt;p&gt;То есть нам надо рассчитать вероятность для всех классов и выбрать тот класс, который обладает максимальной вероятностью. Обратите внимание, знаменатель (вероятность документа) является константой и никак не может повлиять на ранжирование классов, поэтому в нашей задаче мы можем его игнорировать.&lt;/p&gt;

&lt;p&gt;$$ c_{map}=\operatorname*{arg\,max}_{c \in C} \left[ P(d|c)P(c) \right] $$ &lt;/p&gt;

&lt;p class=&quot;description&quot;&gt;Формула №1&lt;/p&gt;

&lt;p&gt;Далее делается допущение которое и объясняет почему этот алгоритм называют наивным.&lt;/p&gt;

&lt;h3 id=&quot;section&quot;&gt;Предположение условной независимости&lt;/h3&gt;

&lt;p&gt;Если я вам скажу &amp;ldquo;темно как у негра в &amp;hellip;&amp;rdquo;, вы сразу поймете о каком месте чем идет речь, даже если я не закончу фразу. Так происходит потому что &lt;em&gt;в натуральном языке вероятность появления слова сильно зависит от контекста&lt;/em&gt;. Байесовский же классификатор представляет документ как набор слов вероятности которых условно не зависят друг от друга. Этот подход иногда еще называется &lt;a href=&quot;http://en.wikipedia.org/wiki/Bag_of_words_model&quot;&gt;bag of words model&lt;/a&gt;. Исходя из этого предположения условная вероятность документа аппроксимируется произведением условных вероятностей всех слов входящих в документ.&lt;/p&gt;

&lt;p&gt;$$ P(d|c) \approx P(w_1|c)P(w_2|c)&amp;hellip;P(w_n|c) = \prod_{i=1}^n P(w_i|c) $$&lt;/p&gt;

&lt;p&gt;Этот подход также называется Unigram Language Model. Языковые модели играют очень важную роль в задачах обработки натуральных языков, но выходят за пределы этой заметки.&lt;/p&gt;

&lt;p&gt;Подставив полученное выражение в формулу №1 мы получим:&lt;/p&gt;

&lt;p&gt;$$ c_{map}=\operatorname*{arg\,max}_{c \in C} \left[ P(c) \prod_{i=1}^n P(w_i|c) \right] $$&lt;/p&gt;

&lt;h3 id=&quot;section-1&quot;&gt;Проблема арифметического переполнения&lt;/h3&gt;

&lt;p&gt;При достаточно большой длине документа придется перемножать большое количество очень маленьких чисел. Для того чтобы при этом избежать &lt;a href=&quot;http://en.wikipedia.org/wiki/Arithmetic_underflow&quot;&gt;арифметического переполнения снизу&lt;/a&gt; зачастую пользуются свойством логарифма произведения $\log ab = \log a+\log b$. Так как логарифм функция монотонная, ее применение к обоим частям выражения изменит только его численное значение, но не параметры при которых достигается максимум. При этом, логарифм от числа близкого к нулю будет числом отрицательным, но в абсолютном значении существенно большим чем исходное число, что делает логарифмические значения вероятностей более удобными для анализа. Поэтому, мы переписываем нашу формулу с использованием логарифма.&lt;/p&gt;

&lt;p&gt;$$ c_{map}=\operatorname*{arg\,max}_{c \in C} \left[ \log P(c)+\sum_{i=1}^n \log P(w_i|c) \right] $$&lt;/p&gt;

&lt;p class=&quot;description&quot;&gt;Формула №2&lt;/p&gt;

&lt;p&gt;Основание логарифма в данном случае не имеет значения. Вы можете использовать как натуральный, так и любой другой логарифм.&lt;/p&gt;

&lt;h3 id=&quot;section-2&quot;&gt;Оценка параметров Байесовской модели&lt;/h3&gt;

&lt;p&gt;Оценка вероятностей $ P(c) $ и $ P(w_i|c) $ осуществляется на обучающей выборке. Вероятность класса мы можем оценить как:&lt;/p&gt;

&lt;p&gt;$$ P(c)=\frac{D_c}{D} $$&lt;/p&gt;

&lt;p&gt;где, $D_c$ – количество документов принадлежащих классу $c$, а $D$ – общее количество документов в обучающей выборке.&lt;/p&gt;

&lt;p&gt;Оценка вероятности слова в классе может делаться несколькими путями. Здесь я приведу multinomial bayes model.&lt;/p&gt;

&lt;p&gt;$$ P(w_i|c)=\frac{W_{ic}}{ \sum_{i&amp;rsquo;\in V} W_{i&amp;rsquo;c} } $$&lt;/p&gt;

&lt;p class=&quot;description&quot;&gt;Формула №3&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$W_{ic}$ — количество раз сколько $i$-ое слово встречается в документах класса $c$;&lt;/li&gt;
  &lt;li&gt;$V$ — словарь корпуса документов (список всех уникальных слов).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Другими словами, числитель описывает сколько раз слово встречается в документах класса (включая повторы), а знаменатель – это суммарное количество слов во всех документах этого класса.&lt;/p&gt;

&lt;h3 id=&quot;section-3&quot;&gt;Проблема неизвестных слов&lt;/h3&gt;

&lt;p&gt;С формулой №3 есть одна небольшая проблема. Если на этапе классификации вам встретится слово которого вы не видели на этапе обучения, то значения $W_{ic}$, а следственно и $P(w_i|c)$ будут равны нулю. &lt;em&gt;Это приведет к тому что документ с этим словом нельзя будет классифицировать, так как он будет иметь нулевую вероятность по всем классам&lt;/em&gt;. Избавиться от этой проблемы путем анализа б&lt;strong&gt;о&lt;/strong&gt;льшего количества документов не получится. Вы никогда не сможете составить обучающую выборку содержащую все возможные слова включая неологизмы, опечатки, синонимы и т.д. Типичным решением проблемы неизвестных слов является &lt;a href=&quot;http://en.wikipedia.org/wiki/Additive_smoothing&quot;&gt;аддитивное сглаживание&lt;/a&gt; (сглаживание Лапласа). Идея заключается в том что мы притворяемся как будто видели каждое слово на один раз больше, то есть прибавляем единицу к частоте каждого слова.&lt;/p&gt;

&lt;p&gt;$$ P(w_i|c)=\frac{W_{ic}+1}{ \sum_{i&amp;rsquo;\in V} \left( W_{i&amp;rsquo;c} + 1 \right) } = \frac{W_{ic}+1}{ |V| + \sum_{i&amp;rsquo;\in V} W_{i&amp;rsquo;c} } $$&lt;/p&gt;

&lt;p&gt;Логически данный подход смещает оценку вероятностей в сторону менее вероятных исходов. Таким образом, слова которые мы не видели на этапе обучения модели получают пусть маленькую, но все же не нулевую вероятность. Вот как это выглядит на практике. Допустим на этапе обучения мы видели три имени собственных указанное количество раз.&lt;/p&gt;

&lt;table class=&quot;center&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Имя&lt;/th&gt;
      &lt;th&gt;Частота&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Вася&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Петя&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Женя&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;И тут на этапе классификации у нас появляется имя Иннокентий, которое мы не видели на этапе обучения. Тогда оригинальная и смещённая по Лапласу оценка вероятностей будет выглядеть следующим образом.&lt;/p&gt;

&lt;p class=&quot;image&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/naive-bayes/additive-smoothing.png&quot; alt=&quot;Смещёная и несмещённая оценка вероятности&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Из графика видно что смещённая оценка никогда не бывает нулевой, что защищает нас от проблемы неизвестных слов.&lt;/p&gt;

&lt;h3 id=&quot;section-4&quot;&gt;Собираем все вместе&lt;/h3&gt;

&lt;p&gt;Подставив выбранные нами оценки в формулу №2 мы получаем окончательную формулу по которой происходит байесовская классификация.&lt;/p&gt;

&lt;p&gt;$$ c_{map}=\operatorname*{arg\,max}_{c \in C} \left[ \log\frac{D_c}{D} + \sum_{i=1}^n\log{\frac{W_{ic}+1}{ |V|+\sum_{i&amp;rsquo;\in V} W_{i&amp;rsquo;c}}} \right] $$&lt;/p&gt;

&lt;p class=&quot;description&quot;&gt;Формула №4&lt;/p&gt;

&lt;h2 id=&quot;practice&quot;&gt;Реализация классификатора&lt;/h2&gt;

&lt;p&gt;Для реализации Байесовского классификатора нам необходима обучающая выборка в которой проставлены соответствия между текстовыми документами и их классами. Затем нам необходимо собрать следующую статистику из выборки, которая будет использоваться на этапе классификации:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;относительные частоты классов в корпусе документов. То есть, как часто встречаются документы того или иного класса;&lt;/li&gt;
  &lt;li&gt;суммарное количество слов в документах каждого класса;&lt;/li&gt;
  &lt;li&gt;относительные частоты слов в пределах каждого класса;&lt;/li&gt;
  &lt;li&gt;размер словаря выборки. Количество уникальных слов в выборке.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Совокупность этой информации мы будем называть моделью классификатора. Затем на этапе классификации необходимо для каждого класса рассчитать значение следующего выражения и выбрать класс с максимальным значением.&lt;/p&gt;

&lt;p&gt;$$ \log\frac{D_c}{D} + \sum_{i \in Q}\log{\frac{W_{ic}+1}{ |V|+L_{c} }} $$&lt;/p&gt;

&lt;p class=&quot;description&quot;&gt;Упрощенная запись формулы №4&lt;/p&gt;

&lt;p&gt;в этой формуле:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$D_c$ — количество документов в обучающей выборке принадлежащих классу $c$;&lt;/li&gt;
  &lt;li&gt;$D$ — общее количество документов в обучающей выборке;&lt;/li&gt;
  &lt;li&gt;$|V|$ — количество уникальных слов во всех документах обучающей выборки;&lt;/li&gt;
  &lt;li&gt;$L_{c}$ — суммарное количество слов в документах класса $c$ в обучающей выборке;&lt;/li&gt;
  &lt;li&gt;$W_{ic}$ — сколько раз $i$-ое слово встречалось в документах класса $c$ в обучающей выборке;&lt;/li&gt;
  &lt;li&gt;$Q$ – множество слов классифицируемого документа (включая повторы).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Информация необходимая для осознания смысла этого выражения приведена выше в разделе &lt;a href=&quot;#theory&quot;&gt;Теория&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;section-5&quot;&gt;Пример&lt;/h3&gt;

&lt;p&gt;Допустим, у нас есть три документа для которых известны их классы (HAM означает – не спам):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code&gt;[SPAM]&lt;/code&gt; предоставляю услуги бухгалтера;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;[SPAM]&lt;/code&gt; спешите купить виагру;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;[HAM]&lt;/code&gt; надо купить молоко.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Модель классификатора будет выглядеть следующим образом:&lt;/p&gt;

&lt;table class=&quot;center&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&amp;nbsp;&lt;/th&gt;
      &lt;th&gt;spam&lt;/th&gt;
      &lt;th&gt;ham&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;частоты классов&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;суммарное количество слов&lt;/td&gt;
      &lt;td&gt;6&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table class=&quot;center&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&amp;nbsp;&lt;/th&gt;
      &lt;th&gt;spam&lt;/th&gt;
      &lt;th&gt;ham&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;предоставляю&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;услуги&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;бухгалтера&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;спешите&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;купить&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;виагру&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;надо&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;молоко&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Теперь классифицируем фразу &amp;ldquo;надо купить сигареты&amp;rdquo;. Рассчитаем значение выражения для класса SPAM:&lt;/p&gt;

&lt;p&gt;$$ \log\frac{2}{3} + \log\frac{1}{8+6} + \log\frac{2}{8+6} + \log\frac{1}{8+6} \approx -7.629 $$&lt;/p&gt;

&lt;p&gt;Теперь сделаем то же самое для класса HAM:&lt;/p&gt;

&lt;p&gt;$$ \log\frac{1}{3} + \log\frac{2}{8+3} + \log\frac{2}{8+3} + \log\frac{1}{8+3} \approx -6.906 $$&lt;/p&gt;

&lt;p&gt;В данном случае класс HAM выиграл и сообщение не будет помечено как спам.&lt;/p&gt;

&lt;h3 id=&quot;section-6&quot;&gt;Формирование вероятностного пространства&lt;/h3&gt;

&lt;p&gt;В простейшем случае вы выбираете класс который получил максимальную оценку. Но если вы например хотите помечать сообщение как спам только если соответствующая вероятность больше 80%, то сравнение логарифмических оценок вам ничего не даст. Оценки которые выдает алгоритм не удовлетворяют двум формальным свойствам которым должны удовлетворять все вероятностные оценки:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;они все должны быть в диапазоне от нуля до единицы;&lt;/li&gt;
  &lt;li&gt;их сумма должна быть равна единице.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Для того чтобы решить эту задачу, необходимо из логарифмических оценок сформировать вероятностное пространство. А именно: избавиться от логарифмов и нормировать сумму по единице.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; P(c \mid d) = \frac{e^{q_c}}{ \sum_{c' \in C}e^{q_{c'}} } &lt;/script&gt;

&lt;p&gt;Здесь &lt;script type=&quot;math/tex&quot;&gt;q_c&lt;/script&gt; — это логарфмическая оценка алгоритма для класса &lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt;, а возведение &lt;script type=&quot;math/tex&quot;&gt;e&lt;/script&gt; (основание натурального логарфима) в степерь оценки используется для того чтобы избавиться от логарифма (&lt;script type=&quot;math/tex&quot;&gt;a^{\log_a x} = x&lt;/script&gt;). Таким образом, если вы в рассчетах использовали не натуральный логарифм, а десятичный, вам необходимо использовать не &lt;script type=&quot;math/tex&quot;&gt;e&lt;/script&gt;, а 10.&lt;/p&gt;

&lt;p&gt;Для вышеприведенного примера вероятность что сообщение спам равно:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt; \frac{e^{-7.629}}{e^{-7.629} + e^{-6.906}} = 0.327 &lt;/script&gt;

&lt;p&gt;то есть сообщение является спамом с вероятностью 32.7%.&lt;/p&gt;

&lt;h2 id=&quot;section-7&quot;&gt;В заключении&lt;/h2&gt;

&lt;h3 id=&quot;show-me-the-code&quot;&gt;Show me the code&lt;/h3&gt;

&lt;p&gt;На github &lt;a href=&quot;https://github.com/bazhenov/naive-bayes-example&quot;&gt;доступен пример&lt;/a&gt; описанного классификатора реализованный на Scala. Имплементация занимает 50 с лишним строк кода, разобраться с ним у вас не составит труда, просто посмотрите тест &lt;code&gt;ClassifierSpec&lt;/code&gt;. Для запуска тестов необходим &lt;a href=&quot;http://maven.apache.org/&quot;&gt;Maven&lt;/a&gt;.&lt;/p&gt;

&lt;pre class=&quot;shell&quot;&gt;&lt;code&gt;$ mvn test
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;section-8&quot;&gt;Вопросы оставшиеся за бортом&lt;/h3&gt;

&lt;p&gt;Сушествует целый ряд вопросов который остался без рассмотрения на текущий момент:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://bazhenov.me/blog/2012/07/21/classification-performance-evaluation.html&quot;&gt;как тестировать качество алгоритмов классификации&lt;/a&gt;;&lt;/li&gt;
  &lt;li&gt;какими системными проблемами обладает алгоритм наивной байесовской классификации;&lt;/li&gt;
  &lt;li&gt;какие существуют подходы увеличения точности алгоритма.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Учитывая объем данного поста, эти вопросы я смело оставляю для будущих заметок.&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title>О задачах классификации</title>
      <link>http://bazhenov.me/blog/2012/06/05/classification.html</link>
      <pubDate>Tue, 05 Jun 2012 00:00:00 +1100</pubDate>
      <author>Denis Bazhenov (dotsid@gmail.com)</author>
      <guid isPermaLink="true">http://bazhenov.me/blog/2012/06/05/classification.html</guid>
      <description>&lt;p&gt;В этом и следующих постах, я хочу на пальцах описать процесс создания простого классификатора текстовых документов, а также рассказать о некоторых нетипичных с обывательской точки зрения подходах используемых при классификации документов.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://ru.wikipedia.org/wiki/Классификация_документов&quot;&gt;Классификатор&lt;/a&gt; – это алгоритм соотносящий некие входные данные с одним или несколькими классами. В отличие от алгоритмов &lt;a href=&quot;http://ru.wikipedia.org/wiki/Кластерный_анализ&quot;&gt;кластеризации&lt;/a&gt; эти классы должны быть определены заранее. &lt;/p&gt;

&lt;p&gt;Возможно, кому-то это определение покажется слишком общими или академическим, поэтому лучше наверное рассмотреть задачу классификации на примерах. А примеров хоть отбавляй.&lt;/p&gt;

&lt;!-- excerpt --&gt;

&lt;h2 id=&quot;section&quot;&gt;Они повсюду&lt;/h2&gt;

&lt;p&gt;Пожалуй самый яркий пример автоматической классификации – это фильтрация спама. Каждый день на мой ящик падает десятки если не сотни спам-писем, которые автоматически отфильтровываются из моего inbox&amp;rsquo;а.&lt;/p&gt;

&lt;p class=&quot;image photo&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/classification/spam.png&quot; alt=&quot;Mail Spam&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Современные коммерческие системы способны успешно фильтровать спам с точностью превышающей 99%&lt;sup id=&quot;fnref:google-spam-filter-performance&quot;&gt;&lt;a href=&quot;#fn:google-spam-filter-performance&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;. Другим довольно типичным примером классификации служит автоматическое определение тематики того или иного текста. Некоторые новостные аггрераторы используют подобный подход для группировки новостей в направления: экономика, политика, общественная жизнь и т.д.&lt;/p&gt;

&lt;p&gt;Зачастую классификация является фундаментом на котором строятся алгоритмы решения более сложных задач.
Например, классификация используется при создании рекомендательных систем и в частности при реализации &lt;a href=&quot;http://ru.wikipedia.org/wiki/Коллаборативная_фильтрация&quot;&gt;коллаборативной фильтрации&lt;/a&gt;.&lt;/p&gt;

&lt;p class=&quot;image photo&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/classification/recommendation.png&quot; alt=&quot;Amazon recommendations&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Safari Reader Mode является еще одним примером где используются алгоритмы классификации для достижения конечной цели. Суть этого режима работы браузера заключается в том что он позволяет автоматически убрать со страницы всю шелуху не имеющую отношения к сути контента страницы&lt;sup id=&quot;fnref:boilerplate-paper&quot;&gt;&lt;a href=&quot;#fn:boilerplate-paper&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p class=&quot;image photo&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/classification/reader-mode.png&quot; alt=&quot;Safari Reader Mode&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Так же классификация используется в задачах face detection&amp;rsquo;а и face recognition&amp;rsquo;а.&lt;/p&gt;

&lt;p class=&quot;image photo&quot;&gt;&lt;img src=&quot;http://bazhenov.me/images/classification/face-detection.png&quot; alt=&quot;Face Detection in Aperture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Классификация используется как инструмент для решения множества других задач:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;снятие омонимии при обработке натуральных языков;&lt;/li&gt;
  &lt;li&gt;в поисковых системах – для ограничения области поиска в целях повышения точности (вертикальный поиск);&lt;/li&gt;
  &lt;li&gt;автоматическое определение языка на котором написан текст;&lt;/li&gt;
  &lt;li&gt;анализ тональности (определение эмоциональной окраски текста).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Этот список можно продолжать еще долго. Например, в медицине алгоритмы классификации используются для реконструирования 3D модели головного мозга по серии МРТ снимков&lt;sup id=&quot;fnref:mri-3d&quot;&gt;&lt;a href=&quot;#fn:mri-3d&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;, а также для диагностики пациентов страдающих синдромом Альцгеймера&lt;sup id=&quot;fnref:alzheimer&quot;&gt;&lt;a href=&quot;#fn:alzheimer&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;h2 id=&quot;section-1&quot;&gt;Традиционные подходы&lt;/h2&gt;

&lt;h3 id=&quot;rule-based-classification&quot;&gt;Rule based classification&lt;/h3&gt;

&lt;p&gt;Если говорить о задаче классификации текстов, то пожалуй ее традиционным решением является классификация основная на правилах (rule based classification). Вы имплементируете правила определения класса документа по его тексту в виде &lt;code&gt;if-then-else&lt;/code&gt; выражений (код на Scala).&lt;/p&gt;

&lt;pre class=&quot;code&quot;&gt;&lt;code&gt;def classify(text: String) =
	if (text.contains(&quot;виагра&quot;) || text.contains(&quot;бухгалтер&quot;)) &quot;SPAM&quot; else &quot;NOT SPAM&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Этот подход может быть хорошим вариантом если вы работаете с небольшой коллекцией документов которую вы способны охватить и тщательно проанализировать. Просто потому что вы четко контролируете правила по которым классификатор принимает решения. Но есть у этого подхода и очевидные минусы:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;для того чтобы выбрать значимые для классификации слова необходимо обладать экспертными знаниями в предметной области. Есть ли у вас например соображения по поводу ключевых слов которые хорошо отличают документы посвященные финансовой тематике от документов экономической? У меня очень смутные;&lt;/li&gt;
  &lt;li&gt;отнюдь не всегда факт наличия или отсутствия какого-либо одного слова является решающим фактором для принятия решения.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Поподробней остановлюсь на последнем пункте. Если вернуться к задаче определения спама и немного подумать о том какие слова являются хорошими классификационными признаками (classifying feature), то станет понятно что нет такого слова наличие которого гарантировало бы что сообщение является спамом. Возможно, в пределах компании производящей &lt;a href=&quot;http://ru.wikipedia.org/wiki/Силденафил&quot;&gt;силденафил&lt;/a&gt; в промышленных масштабах слово &amp;ldquo;виагра&amp;rdquo; не является показательным признаком спам-сообщения, кто знает.&lt;/p&gt;

&lt;p&gt;В общем, суть такова: &lt;em&gt;любое из известных спам-слов пусть редко но встречается в повседневной жизни.&lt;/em&gt; Поэтому, принимать окончательно решение основываясь на факте наличия или отсутствия какого-либо одного слова идея контрпродуктивная. Мы можем усложнять правила добавляя вложенные &lt;code&gt;if&lt;/code&gt;&amp;lsquo;ы. Но довольно быстро вы поймете что возможности человека в формулировании таких правил очень ограничены, потому что &lt;em&gt;сложность правил растет экспоненциально с количеством выбранных для классификации слов&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;weight-based-classification&quot;&gt;Weight based classification&lt;/h3&gt;

&lt;p&gt;Мы можем пойти другим путем. Мы можем для каждого слова выбрать некий вес, который будет означать насколько вероятно что сообщение с этим словом является спамом (0 – никогда не является спамом, 1 – всегда спам).&lt;/p&gt;

&lt;table class=&quot;center&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;&amp;nbsp;&lt;/th&gt;
      &lt;th&gt;spam&lt;/th&gt;
      &lt;th&gt;not spam&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;бухгалтер&lt;/td&gt;
      &lt;td&gt;0.99&lt;/td&gt;
      &lt;td&gt;0.01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;виагра&lt;/td&gt;
      &lt;td&gt;0.99&lt;/td&gt;
      &lt;td&gt;0.01&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;выгодное&lt;/td&gt;
      &lt;td&gt;0.70&lt;/td&gt;
      &lt;td&gt;0.30&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;github&lt;/td&gt;
      &lt;td&gt;0.01&lt;/td&gt;
      &lt;td&gt;0.99&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;В этой таблице перечислены гипотетические веса для четырех слов. Сумма значений в строке должна быть равна единице. Тогда наша классификация может выглядеть следующим образом:&lt;/p&gt;

&lt;pre class=&quot;code&quot;&gt;&lt;code&gt;def classify(text: String) = {
	val weights = Map(&quot;бухгалтер&quot;-&amp;gt;0.9, &quot;виагра&quot;-&amp;gt;0.99, &quot;выгодное&quot;-&amp;gt;0.7, &quot;github&quot;-&amp;gt;0.01)
	val words = text.split(' ').filter(weights.contains(_))
	val P_spam = words.map(weights(_)).reduce(_ * _)
	val P_not_spam = words.map(1 - weights(_)).reduce(_ * _)
	if (P_spam &amp;gt; P_not_spam) &quot;SPAM&quot; else &quot;NOT SPAM&quot;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Мы берем каждое слово и определяем суммарный вес документа отдельно для класса &amp;ldquo;спам&amp;rdquo; и класса &amp;ldquo;не спам&amp;rdquo;. Суммарный вес определяется как произведение весов всех известных слов документа. Слова для которых у нас нет веса мы пропускаем при классификации. Какой суммарный вес оказался больше тот класс и побеждает.&lt;/p&gt;

&lt;p&gt;Это более разумный подход, так как он более гибок и принимает решение на основании всех известных слов в тексте. Так же его гораздо проще сопровождать чем полотна &lt;code&gt;if&lt;/code&gt;&amp;lsquo;ов. &lt;/p&gt;

&lt;h2 id=&quot;section-2&quot;&gt;Метод машинного обучения&lt;/h2&gt;

&lt;p&gt;А теперь очень важное замечание. &lt;em&gt;Если у нас будет некий способ автоматически подобрать оптимальные веса слов, то данный подход можно считать методом &lt;a href=&quot;http://ru.wikipedia.org/wiki/Машинное_обучение&quot;&gt;машинного обучения&lt;/a&gt;&lt;/em&gt;. Сильно упрощенный, возможно даже гипертрофированный, но по своей сути это именно метод машинного обучения.&lt;/p&gt;

&lt;p&gt;Если быть более точным, то описанный мною метод является зародышем &lt;a href=&quot;http://ru.wikipedia.org/wiki/Наивный_байесовский_классификатор&quot;&gt;наивного байесовского классификатора&lt;/a&gt;. Но не позволяйте названию обмануть вас, NBC (Naïve Bayes Classifier) если не самый, то один из самых часто используемых классификаторов. Тому есть ряд причин:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;он прост в имплементации и тестировании;&lt;/li&gt;
  &lt;li&gt;процесс обучения довольно эффективен по сравнению с другими более сложными классификаторами;&lt;/li&gt;
  &lt;li&gt;на небольших корпусах документов разница между NBC и другими гораздо более сложными алгоритмами классификации зачастую несущественна, а иногда NBC может оказаться и более точным&lt;sup id=&quot;fnref:on-bayes-optimality&quot;&gt;&lt;a href=&quot;#fn:on-bayes-optimality&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;В последующих заметках я более детально опишу вопросы связанные с созданием и тестированием подобного классификатора. Подписывайтесь на &lt;a href=&quot;http://bazhenov.me/rss.xml&quot;&gt;RSS&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;section-3&quot;&gt;Ссылки по теме&lt;/h2&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:google-spam-filter-performance&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://googlesystem.blogspot.com/2007/10/how-gmail-blocks-spam.html&quot;&gt;How Gmail Blocks Spam&lt;/a&gt;&lt;a href=&quot;#fnref:google-spam-filter-performance&quot; rel=&quot;reference&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:boilerplate-paper&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.l3s.de/~kohlschuetter/publications/wsdm187-kohlschuetter.pdf&quot;&gt;Boilerplate Detection using Shallow Text Features&lt;/a&gt;&lt;a href=&quot;#fnref:boilerplate-paper&quot; rel=&quot;reference&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:mri-3d&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;amp;arnumber=1352574&amp;amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F9356%2F29717%2F01352574.pdf%3Farnumber%3D1352574&quot;&gt;Automatic classification of MRI images for three-dimensional volume reconstruction by using general regression neural networks&lt;/a&gt;&lt;a href=&quot;#fnref:mri-3d&quot; rel=&quot;reference&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:alzheimer&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.ncbi.nlm.nih.gov/pubmed/20542124&quot;&gt;Automatic classification of patients with Alzheimer&amp;rsquo;s disease from structural MRI: a comparison of ten methods using the ADNI database&lt;/a&gt;&lt;a href=&quot;#fnref:alzheimer&quot; rel=&quot;reference&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:on-bayes-optimality&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;http://www.cc.gatech.edu/fac/Charles.Isbell/classes/reading/papers/bayes-opt.pdf&quot;&gt;On the Optimality of the Simple Bayesian Classifier under Zero-One Loss&lt;/a&gt;&lt;a href=&quot;#fnref:on-bayes-optimality&quot; rel=&quot;reference&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel> 
</rss>