<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="ru" lang="ru-ru">
<head>
	 <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
	 <meta http-equiv="content-type" content="text/html; charset=utf-8" />
	 <title>Наивный байесовский классификатор</title>
	 <meta name="author" content="Denis Bazhenov" />
	 <link href="http://feeds.feedburner.com/severe-reality" rel="alternate" title="Severe Reality" type="application/atom+xml" />
	 <link href='http://fonts.googleapis.com/css?family=Open+Sans:300,400,600&subset=latin,cyrillic-ext,cyrillic' rel='stylesheet' type='text/css'>
	 
	 <link rel="openid2.provider" href="https://www.google.com/accounts/o8/ud?source=profiles" > 
	 <link rel="openid2.local_id" href="http://www.google.com/profiles/dotsid" >
	 
	 <!-- syntax highlighting CSS -->
	 <link rel="stylesheet" href="/css/syntax.css" type="text/css" />
	 <!-- Homepage CSS -->
	 <link rel="stylesheet" href="/css/screen.css" type="text/css" media="screen, projection" />
	 <link rel="stylesheet" href="http://yandex.st/highlightjs/6.1/styles/idea.min.css">

	<script src="http://code.jquery.com/jquery-1.7.1.min.js"></script>
</head>
<body>

	<div class="site">
		
			<header>
				<div class="content">
					<a href="/">Суровая реальность</a>
				</div>
			</header>
		

		<div class="post content">
	<h1>Наивный байесовский классификатор</h1>
	<div class="meta">
		<span class="date">11 June 2012</span>
		<span class="tags">
			
				<a href="/tags.html#information retrieval">information retrieval</a>
			
				<a href="/tags.html#classification">classification</a>
			
				<a href="/tags.html#algorithms">algorithms</a>
			
		</span>
		
	</div>
	
	
	<p>В <a href="/blog/2012/06/05/classification.html">прошлой заметке</a> я в общих чертах описал задачу классификации, а также традиционные подходы используемые для классификации текстовых документов. В этой заметке я более детально расскажу о том как работает самый простой, но вместе с тем один из самых часто используемых при обработке натуральных языков алгоритм классификации – <a href="http://ru.wikipedia.org/wiki/Наивный_байесовский_классификатор">наивный байесовский классификатор</a>.</p>

<!-- excerpt -->

<p>Заметка разбита на две части: <a href="#theory">теоретическая</a>, в которой описаны аспекты классификации, и <a href="#practice">практическая</a> часть построения классификатора. Если вы хотите быстро создать прототип классификатора, то обратитесь к практической части заметки, там на приводится пример классификатора. Также на github доступны <a href="https://github.com/bazhenov/naive-bayes-example">исходники примера</a>. Если же вам интересны теоретические принципы работы классификации, то обратитесь к теоретической части заметки.</p>

<h2 id="theory">Теория</h2>

<p>Осторожно, в этой части заметки много формул.</p>

<p>В основе NBC (Naïve Bayes Classifier) лежит, как вы уже могли догадаться, <a href="http://ru.wikipedia.org/wiki/Теорема_Байеса">теорема Байеса</a>.</p>

<p>$$ P(c|d) = \frac{ P(d|c)P(c) }{ P(d) } $$</p>

<p>где,</p>

<ul>
  <li>$ P(c|d) $ — вероятность что документ $d$ принадлежит классу $c$, именно её нам надо рассчитать;</li>
  <li>$ P(d|c) $ — вероятность встретить документ $d$ среди всех документов класса $c$;</li>
  <li>$ P(c) $ — безусловная вероятность встретить документ класса $c$ в корпусе документов;</li>
  <li>$ P(d) $ — безусловная вероятность документа $d$ в корпусе документов.</li>
</ul>

<p>Её смысл на обывательском уровне можно выразить следующим образом. Теорема Байеса позволяет переставить местами причину и следствие. Зная с какой вероятностью причина приводит к некоему событию, эта теорема позволяет расчитать вероятность того что именно эта причина привела к наблюдаемому событию.</p>

<p>Цель классификации состоит в том чтобы понять к какому классу принадлежит документ, поэтому нам нужна не сама вероятность, а наиболее вероятный класс. Байесовский классификатор использует <a href="http://ru.wikipedia.org/wiki/Оценка_апостериорного_максимума">оценку апостериорного максимума</a> (Maximum a posteriori estimation) для определения наиболее вероятного класса. Грубо говоря, это класс с максимальной вероятностью.</p>

<p>$$ c_{map}=\operatorname*{arg\,max}_{c \in C} \frac{ P(d|c)P(c) }{ P(d) }$$</p>

<p>То есть нам надо рассчитать вероятность для всех классов и выбрать тот класс, который обладает максимальной вероятностью. Обратите внимание, знаменатель (вероятность документа) является константой и никак не может повлиять на ранжирование классов, поэтому в нашей задаче мы можем его игнорировать.</p>

<p>$$ c_{map}=\operatorname*{arg\,max}_{c \in C} \left[ P(d|c)P(c) \right] $$ </p>

<p class="description">Формула №1</p>

<p>Далее делается допущение которое и объясняет почему этот алгоритм называют наивным.</p>

<h3 id="section">Предположение условной независимости</h3>

<p>Если я вам скажу &ldquo;темно как у негра в &hellip;&rdquo;, вы сразу поймете о каком месте чем идет речь, даже если я не закончу фразу. Так происходит потому что <em>в натуральном языке вероятность появления слова сильно зависит от контекста</em>. Байесовский же классификатор представляет документ как набор слов вероятности которых условно не зависят друг от друга. Этот подход иногда еще называется <a href="http://en.wikipedia.org/wiki/Bag_of_words_model">bag of words model</a>. Исходя из этого предположения условная вероятность документа аппроксимируется произведением условных вероятностей всех слов входящих в документ.</p>

<p>$$ P(d|c) \approx P(w_1|c)P(w_2|c)&hellip;P(w_n|c) = \prod_{i=1}^n P(w_i|c) $$</p>

<p>Этот подход также называется Unigram Language Model. Языковые модели играют очень важную роль в задачах обработки натуральных языков, но выходят за пределы этой заметки.</p>

<p>Подставив полученное выражение в формулу №1 мы получим:</p>

<p>$$ c_{map}=\operatorname*{arg\,max}_{c \in C} \left[ P(c) \prod_{i=1}^n P(w_i|c) \right] $$</p>

<h3 id="section-1">Проблема арифметического переполнения</h3>

<p>При достаточно большой длине документа придется перемножать большое количество очень маленьких чисел. Для того чтобы при этом избежать <a href="http://en.wikipedia.org/wiki/Arithmetic_underflow">арифметического переполнения снизу</a> зачастую пользуются свойством логарифма произведения $\log ab = \log a+\log b$. Так как логарифм функция монотонная, ее применение к обоим частям выражения изменит только его численное значение, но не параметры при которых достигается максимум. При этом, логарифм от числа близкого к нулю будет числом отрицательным, но в абсолютном значении существенно большим чем исходное число, что делает логарифмические значения вероятностей более удобными для анализа. Поэтому, мы переписываем нашу формулу с использованием логарифма.</p>

<p>$$ c_{map}=\operatorname*{arg\,max}_{c \in C} \left[ \log P(c)+\sum_{i=1}^n \log P(w_i|c) \right] $$</p>

<p class="description">Формула №2</p>

<p>Основание логарифма в данном случае не имеет значения. Вы можете использовать как натуральный, так и любой другой логарифм.</p>

<h3 id="section-2">Оценка параметров Байесовской модели</h3>

<p>Оценка вероятностей $ P(c) $ и $ P(w_i|c) $ осуществляется на обучающей выборке. Вероятность класса мы можем оценить как:</p>

<p>$$ P(c)=\frac{D_c}{D} $$</p>

<p>где, $D_c$ – количество документов принадлежащих классу $c$, а $D$ – общее количество документов в обучающей выборке.</p>

<p>Оценка вероятности слова в классе может делаться несколькими путями. Здесь я приведу multinomial bayes model.</p>

<p>$$ P(w_i|c)=\frac{W_{ic}}{ \sum_{i&rsquo;\in V} W_{i&rsquo;c} } $$</p>

<p class="description">Формула №3</p>

<ul>
  <li>$W_{ic}$ — количество раз сколько $i$-ое слово встречается в документах класса $c$;</li>
  <li>$V$ — словарь корпуса документов (список всех уникальных слов).</li>
</ul>

<p>Другими словами, числитель описывает сколько раз слово встречается в документах класса (включая повторы), а знаменатель – это суммарное количество слов во всех документах этого класса.</p>

<h3 id="section-3">Проблема неизвестных слов</h3>

<p>С формулой №3 есть одна небольшая проблема. Если на этапе классификации вам встретится слово которого вы не видели на этапе обучения, то значения $W_{ic}$, а следственно и $P(w_i|c)$ будут равны нулю. <em>Это приведет к тому что документ с этим словом нельзя будет классифицировать, так как он будет иметь нулевую вероятность по всем классам</em>. Избавиться от этой проблемы путем анализа б<strong>о</strong>льшего количества документов не получится. Вы никогда не сможете составить обучающую выборку содержащую все возможные слова включая неологизмы, опечатки, синонимы и т.д. Типичным решением проблемы неизвестных слов является <a href="http://en.wikipedia.org/wiki/Additive_smoothing">аддитивное сглаживание</a> (сглаживание Лапласа). Идея заключается в том что мы притворяемся как будто видели каждое слово на один раз больше, то есть прибавляем единицу к частоте каждого слова.</p>

<p>$$ P(w_i|c)=\frac{W_{ic}+1}{ \sum_{i&rsquo;\in V} \left( W_{i&rsquo;c} + 1 \right) } = \frac{W_{ic}+1}{ |V| + \sum_{i&rsquo;\in V} W_{i&rsquo;c} } $$</p>

<p>Логически данный подход смещает оценку вероятностей в сторону менее вероятных исходов. Таким образом, слова которые мы не видели на этапе обучения модели получают пусть маленькую, но все же не нулевую вероятность. Вот как это выглядит на практике. Допустим на этапе обучения мы видели три имени собственных указанное количество раз.</p>

<table class="center">
  <thead>
    <tr>
      <th>Имя</th>
      <th>Частота</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Вася</td>
      <td>3</td>
    </tr>
    <tr>
      <td>Петя</td>
      <td>2</td>
    </tr>
    <tr>
      <td>Женя</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>И тут на этапе классификации у нас появляется имя Иннокентий, которое мы не видели на этапе обучения. Тогда оригинальная и смещённая по Лапласу оценка вероятностей будет выглядеть следующим образом.</p>

<p class="image"><img src="/images/naive-bayes/additive-smoothing.png" alt="Смещёная и несмещённая оценка вероятности" /></p>

<p>Из графика видно что смещённая оценка никогда не бывает нулевой, что защищает нас от проблемы неизвестных слов.</p>

<h3 id="section-4">Собираем все вместе</h3>

<p>Подставив выбранные нами оценки в формулу №2 мы получаем окончательную формулу по которой происходит байесовская классификация.</p>

<p>$$ c_{map}=\operatorname*{arg\,max}_{c \in C} \left[ \log\frac{D_c}{D} + \sum_{i=1}^n\log{\frac{W_{ic}+1}{ |V|+\sum_{i&rsquo;\in V} W_{i&rsquo;c}}} \right] $$</p>

<p class="description">Формула №4</p>

<h2 id="practice">Реализация классификатора</h2>

<p>Для реализации Байесовского классификатора нам необходима обучающая выборка в которой проставлены соответствия между текстовыми документами и их классами. Затем нам необходимо собрать следующую статистику из выборки, которая будет использоваться на этапе классификации:</p>

<ul>
  <li>относительные частоты классов в корпусе документов. То есть, как часто встречаются документы того или иного класса;</li>
  <li>суммарное количество слов в документах каждого класса;</li>
  <li>относительные частоты слов в пределах каждого класса;</li>
  <li>размер словаря выборки. Количество уникальных слов в выборке.</li>
</ul>

<p>Совокупность этой информации мы будем называть моделью классификатора. Затем на этапе классификации необходимо для каждого класса рассчитать значение следующего выражения и выбрать класс с максимальным значением.</p>

<p>$$ \log\frac{D_c}{D} + \sum_{i \in Q}\log{\frac{W_{ic}+1}{ |V|+L_{c} }} $$</p>

<p class="description">Упрощенная запись формулы №4</p>

<p>в этой формуле:</p>

<ul>
  <li>$D_c$ — количество документов в обучающей выборке принадлежащих классу $c$;</li>
  <li>$D$ — общее количество документов в обучающей выборке;</li>
  <li>$|V|$ — количество уникальных слов во всех документах обучающей выборки;</li>
  <li>$L_{c}$ — суммарное количество слов в документах класса $c$ в обучающей выборке;</li>
  <li>$W_{ic}$ — сколько раз $i$-ое слово встречалось в документах класса $c$ в обучающей выборке;</li>
  <li>$Q$ – множество слов классифицируемого документа (включая повторы).</li>
</ul>

<p>Информация необходимая для осознания смысла этого выражения приведена выше в разделе <a href="#theory">Теория</a>.</p>

<h3 id="section-5">Пример</h3>

<p>Допустим, у нас есть три документа для которых известны их классы (HAM означает – не спам):</p>

<ul>
  <li><code>[SPAM]</code> предоставляю услуги бухгалтера;</li>
  <li><code>[SPAM]</code> спешите купить виагру;</li>
  <li><code>[HAM]</code> надо купить молоко.</li>
</ul>

<p>Модель классификатора будет выглядеть следующим образом:</p>

<table class="center">
  <thead>
    <tr>
      <th>&nbsp;</th>
      <th>spam</th>
      <th>ham</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>частоты классов</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td>суммарное количество слов</td>
      <td>6</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<table class="center">
  <thead>
    <tr>
      <th>&nbsp;</th>
      <th>spam</th>
      <th>ham</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>предоставляю</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>услуги</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>бухгалтера</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>спешите</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>купить</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>виагру</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td>надо</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <td>молоко</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>Теперь классифицируем фразу &ldquo;надо купить сигареты&rdquo;. Рассчитаем значение выражения для класса SPAM:</p>

<p>$$ \log\frac{2}{3} + \log\frac{1}{8+6} + \log\frac{2}{8+6} + \log\frac{1}{8+6} \approx -7.629 $$</p>

<p>Теперь сделаем то же самое для класса HAM:</p>

<p>$$ \log\frac{1}{3} + \log\frac{2}{8+3} + \log\frac{2}{8+3} + \log\frac{1}{8+3} \approx -6.906 $$</p>

<p>В данном случае класс HAM выиграл и сообщение не будет помечено как спам.</p>

<h3 id="section-6">Формирование вероятностного пространства</h3>

<p>В простейшем случае вы выбираете класс который получил максимальную оценку. Но если вы например хотите помечать сообщение как спам только если соответствующая вероятность больше 80%, то сравнение логарифмических оценок вам ничего не даст. Оценки которые выдает алгоритм не удовлетворяют двум формальным свойствам которым должны удовлетворять все вероятностные оценки:</p>

<ul>
  <li>они все должны быть в диапазоне от нуля до единицы;</li>
  <li>их сумма должна быть равна единице.</li>
</ul>

<p>Для того чтобы решить эту задачу, необходимо из логарифмических оценок сформировать вероятностное пространство. А именно: избавиться от логарифмов и нормировать сумму по единице.</p>

<script type="math/tex; mode=display"> P(c \mid d) = \frac{e^{q_c}}{ \sum_{c' \in C}e^{q_{c'}} } </script>

<p>Здесь <script type="math/tex">q_c</script> — это логарфмическая оценка алгоритма для класса <script type="math/tex">c</script>, а возведение <script type="math/tex">e</script> (основание натурального логарфима) в степерь оценки используется для того чтобы избавиться от логарифма (<script type="math/tex">a^{\log_a x} = x</script>). Таким образом, если вы в рассчетах использовали не натуральный логарифм, а десятичный, вам необходимо использовать не <script type="math/tex">e</script>, а 10.</p>

<p>Для вышеприведенного примера вероятность что сообщение спам равно:</p>

<script type="math/tex; mode=display"> \frac{e^{-7.629}}{e^{-7.629} + e^{-6.906}} = 0.327 </script>

<p>то есть сообщение является спамом с вероятностью 32.7%.</p>

<h2 id="section-7">В заключении</h2>

<h3 id="show-me-the-code">Show me the code</h3>

<p>На github <a href="https://github.com/bazhenov/naive-bayes-example">доступен пример</a> описанного классификатора реализованный на Scala. Имплементация занимает 50 с лишним строк кода, разобраться с ним у вас не составит труда, просто посмотрите тест <code>ClassifierSpec</code>. Для запуска тестов необходим <a href="http://maven.apache.org/">Maven</a>.</p>

<pre class="shell"><code>$ mvn test
</code></pre>

<h3 id="section-8">Вопросы оставшиеся за бортом</h3>

<p>Сушествует целый ряд вопросов который остался без рассмотрения на текущий момент:</p>

<ul>
  <li><a href="/blog/2012/07/21/classification-performance-evaluation.html">как тестировать качество алгоритмов классификации</a>;</li>
  <li>какими системными проблемами обладает алгоритм наивной байесовской классификации;</li>
  <li>какие существуют подходы увеличения точности алгоритма.</li>
</ul>

<p>Учитывая объем данного поста, эти вопросы я смело оставляю для будущих заметок.</p>



	<div class="bar">
		<div class="g-plusone" data-size="medium"></div>
		<a href="https://twitter.com/share" class="twitter-share-button"
			data-url="http://bazhenov.me/blog/2012/06/11/naive-bayes.html"
			data-via="denis_bazhenov" data-lang="en">Tweet</a>

		<a href="https://twitter.com/denis_bazhenov" class="twitter-follow-button"
			data-show-count="false">Follow</a>
	</div>

	<div id="disqus_thread"></div>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

<!-- Google +1 button -->
<script type="text/javascript">
	(function() {
		var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
		po.src = 'https://apis.google.com/js/plusone.js';
		var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
	})();
</script>

<!-- Twitter buttons -->
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>

<!-- Disqus -->
<script type="text/javascript">
	/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
	var disqus_shortname='severe-reality';
	var disqus_developer=false;

	/* * * DON'T EDIT BELOW THIS LINE * * */
	(function() {
		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	})();
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
	MathJax.Hub.Config({
		tex2jax: {
			inlineMath: [['$','$'], ['\\(','\\)']],
			processEscapes: true
		}
	});
</script>

		<footer>
			<div class="content">
				<a href="http://feeds.feedburner.com/severe-reality">RSS Subscribe</a>
					|
				<a href="mailto:dotsid@gmail.com">Email</a>
			</div>
		</footer>
					
	</div>
	
	<script src="http://yandex.st/highlightjs/6.1/highlight.min.js"></script>
	<script>
	$(document).ready(function() {
		$('pre.code code').each(function(i, e) {hljs.highlightBlock(e, '  ')});
	});
	</script>
	
	
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-31484732-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

</body>
</html>