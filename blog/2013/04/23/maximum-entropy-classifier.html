<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="ru" lang="ru-ru">
<head>
	 <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
	 <meta http-equiv="content-type" content="text/html; charset=utf-8" />
	 <title>Классификация методом максимальной энтропии</title>
	 <meta name="author" content="Denis Bazhenov" />
	 <link href="http://feeds.feedburner.com/severe-reality" rel="alternate" title="Severe Reality" type="application/atom+xml" />
	 <link href='http://fonts.googleapis.com/css?family=Open+Sans:300,400,600&subset=latin,cyrillic-ext,cyrillic' rel='stylesheet' type='text/css'>
	 
	 <link rel="openid2.provider" href="https://www.google.com/accounts/o8/ud?source=profiles" > 
	 <link rel="openid2.local_id" href="http://www.google.com/profiles/dotsid" >
	 
	 <!-- syntax highlighting CSS -->
	 <link rel="stylesheet" href="/css/syntax.css" type="text/css" />
	 <!-- Homepage CSS -->
	 <link rel="stylesheet" href="/css/screen.css" type="text/css" media="screen, projection" />
	 <link rel="stylesheet" href="http://yandex.st/highlightjs/6.1/styles/idea.min.css">

	<script src="http://code.jquery.com/jquery-1.7.1.min.js"></script>
</head>
<body>

	<div class="site">
		
			<header>
				<div class="content">
					<a href="/">Суровая реальность</a>
				</div>
			</header>
		

		<div class="post content">
	<h1>Классификация методом максимальной энтропии</h1>
	<div class="meta">
		<span class="date">23 April 2013</span>
		<span class="tags">
			
				<a href="/tags.html#information retrieval">information retrieval</a>
			
				<a href="/tags.html#classification">classification</a>
			
				<a href="/tags.html#algorithms">algorithms</a>
			
		</span>
		
	</div>
	
	
	<p><a href="/blog/2012/06/11/naive-bayes.html">Наивный байесовский классификатор</a>, о котором я уже писал, один из самых простых классификационных алгоритмов. В этой заметке я опишу более сложный алгоритм — <a href="http://en.wikipedia.org/wiki/Principle_of_maximum_entropy">метод максимальной энтропии</a>, который, в ряде случаев, может оказаться существенно более точным. К своему удивлению, я не нашел в рунете более менее полного описания этого алгоритма классификации. Поэтому, считаю полезным поделиться этими знаниями.</p>

<!-- excerpt -->

<p>Как и в предыдущем случае, описание разбито на две части: <a href="#theory">теорию</a> и <a href="#practice">практику</a>. В теоретической части описываются вопросы связанные с математической моделью классификатора. В практической части, содержится описание, как максимально быстро получить работающий классификатор для практических нужд на Java. Исходный код работающего примера <a href="https://github.com/bazhenov/maxent-example">доступен на github</a>.</p>

<h2 id="section">Мотивация к использованию</h2>

<p>У байеса есть несомненные преимущества: он эффективен в обучении, может использовать параллелизм современного железа. Но есть у него и минусы. Именно они могут побудить вас использовать более сложные алгоритмы. Таких минусов несколько:</p>

<ul>
  <li>одна из систематических ошибок байеса связана с предположением условной независимости классификационных признаков (слов, в случае классификации документов). На практике, это предположение часто оказывается ложным. Например, пара слов “курильский” и “бобтейл” гораздо чаще встречается вместе, чем “бобтейл” и “турбонаддув” или “курильский” и “кальян”. Не учитывая эту информацию, байесовский классификатор имеет тенденцию отклонения в сторону классов с высоким уровнем зависимости между словами (так называемая, проблема evidence double-counting’а).</li>
  <li>вероятностные оценки, которые дает байес могут быть сильно искажены для редких документов (подробнее об этом ниже).</li>
</ul>

<p>Метод максимальной энтропии очень тесно связан с другим распространенным алгоритмом машинного обучения, – <a href="http://ru.wikipedia.org/wiki/Логистическая_регрессия">логистической регрессией</a>. Люди, знакомые с логистической или линейной регрессией, найдут много знакомого в этом описании.</p>

<h2 id="theory">Теоретические основы</h2>
<p>Байесовский классификатор оперирует на основании таблицы весов, которые он получает на этапе обучения из частотности классификационных признаков в пределах классов, – по одному параметру на каждую пару классификационный признак+класс. То есть, всего таких весов n*k, где n–количество классов, а k–количество классификационных признаков.</p>

<p>На этапе классификации maxent очень похож на байесовский классификатор. Он основывается на такой же таблице весов и экспоненциальной модели классификации, которая используется в байесе для формирования вероятностного пространства из логарифмических оценок.</p>

<p>Для дальнейшего повествования введем следующую нотацию. Допустим, у нас есть n классов c<sub>1</sub>, c<sub>2</sub>,…, c<sub>n</sub>, и k классификационных признаков: F<sup>I</sup>,F<sup>II</sup>,…,F<sup>k</sup>. ) <em>Классификационный признак – это бинарная функция над документом.</em> Например:</p>

<pre><code>F = function(d) { d.contains("бухгалтерия") }
</code></pre>

<p>На основании классификационных признаков генерируются классификационные индикаторы f<sub>1</sub>, f<sub>2</sub>,…, f<sub>k*n</sub>, по одному индикатору на каждую пару признак+класс. <em>Классификационный индикатор – это булева функция над парой документ+класс.</em> Она истинна только когда истинен соответствующий классификационный признак и совпадает переданный класс.</p>

<table class="center">
  <thead>
    <tr>
      <th> </th>
      <th>C<sub>1</sub></th>
      <th>C<sub>2</sub></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>F<sup>I</sup></td>
      <td>f<sub>1</sub></td>
      <td>f<sub>2</sub></td>
    </tr>
    <tr>
      <td>F<sup>II</sup></td>
      <td>f<sub>3</sub></td>
      <td>f<sub>4</sub></td>
    </tr>
    <tr>
      <td>F<sup>III</sup></td>
      <td>f<sub>5</sub></td>
      <td>f<sub>6</sub></td>
    </tr>
  </tbody>
</table>
<p class="description">Взаимосвязь классов, классификационных признаков и индикаторов</p>

<p>Пример классификационного индикатора:</p>

<pre><code>f = function(c, d) { с == "SPAM" &amp;&amp; F(d) }
</code></pre>

<p>Обратите внимание, классификационные индикаторы  нумеруются по сквозному принципу. Индикаторы, дающие единицу как остаток от деления на количество классов, срабатывают (возвращают истину) только на первый класс, кратные двойке на второй и т.д. Такой подход не является обязательным при реализации классификатора, но для осознания теории важно понимать разницу между признаком и индикатором, а также разницей в их нумерации.</p>

<p>Непосредственно классификация происходит по формуле:</p>

<script type="math/tex; mode=display">p(c\mid d,\lambda)=\frac
{\exp\sum_i^{n \times k}{\lambda_i}f_i\left(c,d\right )}
{\sum_{\tilde{c}\in C}{\exp\sum_i^{n \times k}{\lambda_i}f_i\left(\tilde{c},d\right )}}</script>

<p class="description">Классификационная формула метода максимальной энтропии (Формула №1)</p>

<p>В этой формуле:</p>

<ul>
  <li><script type="math/tex">f_i</script> — i-ый классификационный индикатор (значение 0 или 1);</li>
  <li><script type="math/tex">\lambda_i</script> — вес i-го классификационного индикатора <script type="math/tex">f_i</script>;</li>
  <li><script type="math/tex">c</script> — класс-гипотеза; </li>
  <li><script type="math/tex">C</script> — множество всех возможных классов;</li>
  <li><script type="math/tex">d</script> — классифицируемый документ.</li>
</ul>

<p>У каждого индикатора <script type="math/tex">f_i</script> есть свой вес <script type="math/tex">\lambda_i</script>, который описывает взаимосвязь между соответствующим классификационным признаком и классом. Чем больше вес, тем сильнее связь. Таким образом, числитель дроби описывает экспоненту весов для класса-гипотезы, а знаменатель нормирует значение по единице. Самое сложная часть этой формулы – набор весов <script type="math/tex">\lambda</script>, который приходится путем численной оптимизации, о которой мы поговорим позже.</p>

<p>Результатом этого отношения является не просто классификационное решение, а значение вероятности для заданного класса. Один из плюсов maxent-классификации заключается в том, что она гораздо более точно моделирует вероятностное распределение классов.</p>

<p>На этом сходства заканчиваются и начинаются различия. Maxent-классификатор имеет несколько существенных отличий от байесовского:</p>

<ul>
  <li>он является conditional, а не joint-классификатором;</li>
  <li>он основывается на взаимосвязи энтропии вероятностного распределения и его равномерности;</li>
  <li>он использует итеративные алгоритмы подбора параметров модели, которые сложнее в реализации и более ресурсоемкие.</li>
</ul>

<p>Все три различия необходимо рассмотреть более подробно.</p>

<h3 id="joint-vs-conditional-">Joint vs. Conditional-модели</h3>
<p>Байес для классификации использует следующую формулу: p(d|c)p(c), что является совместным распределением (<a href="http://en.wikipedia.org/wiki/Joint_probability_distribution">joint distribution</a>) документов и классов: p(d∩c). В отличии от байеса, классификатор максимальной энтропии моделирует распредение p(c|d) напрямую, используя обучающую выборку для подбора оптимальных параметром распределения. Есть две причины, по которым этот подход может быть предпочтителен.</p>

<p>Во-первых, joint-классификаторы игнорируют вероятность документа как такового. Это правомерно, если вы хотите получить только классификацию, но не вероятностное распределение классов.</p>

<blockquote>
  <p>Классификацией называется список классов отсортированный по убыванию их вероятностей. При этом, вы не знаете их численные значения, только порядок. Классификации, впрочем, достаточно для большинства прикладных задач.</p>
</blockquote>

<p>Вероятностные оценки восстанавливаемые из логарифмических оценок байеса являются апроксимацией. Причем, чем меньше априорная вероятность документа, тем сильнее будет искажена восстановленная оценка.</p>

<p>Во-вторых, conditional-модели позволяют более адекватно моделировать зависимости между словами. Представьте, обучающую выборку из следующих двух документов:</p>

<pre><code>class   text
-----------------------
USA     San Francisco
China   Beijing
</code></pre>

<p>Для такого корпуса будут построена следующие таблицы весов:</p>

<pre><code>        Naïve Bayes                    MaxEnt
                USA  China                  USA  China
--------------------------    ------------------------
San          -0.847 -1.609    San         0.639 -0.859
Francisco    -0.847 -1.609    Francisco   0.639 -0.859
Beijing      -1.946 -0.511    Beijing    -2.234  0.762
--------------------------    ------------------------
Total        -3.640 -3.729    Total      -0.956 -0.956
</code></pre>

<p>Обратите внимание, что у байеса сумма столбцов по двум классам не равна. Оценка в пределах класса <code>USA</code> немного выше. Поэтому, если вы попытаетесь классифицировать документ “San Francisco Beijing”, он попадет в класс <code>USA</code>. Так происходит из-за высокой зависимости слов “San” и “Francisco”. Байес считает их дважды. По этой причине документ и попадает в класс <code>USA</code>.</p>

<p>Maxent-классификатор способен определить такого рода зависимость между классификационными признаками и демпировать их вес, чтобы избежать проблемы double-counting’а. В этом случае, он корректно определит вероятность классов как 50/50%.</p>

<h3 id="section-1">Принцип максимальной энтропии</h3>
<p><a href="http://ru.wikipedia.org/wiki/Информационная_энтропия">Информационная энтропия</a> — это мера неопределенности вероятностного распределения.</p>

<script type="math/tex; mode=display">H(X)=-\sum_{x\in\Omega}p(x)\log p(x)</script>

<p class="description">Формула расчета энтропии для дискретного вероятностного распределения</p>

<p>В бытовом понимании, энтропия говорит о том, насколько тяжело делать предсказания о событиях описываемых вероятностным распределением. Чем больше энтропия, тем тяжелее делать предсказания.</p>

<p>На первый взгляд может показаться, что энтропию надо минимизировать. Давайте разберем небольшой пример, показывающий почему это не так.</p>

<p class="image"><img src="/images/maxent/entropy.png" alt="Энтропия произвольной монеты" /></p>

<p class="description">График энтропии для монеты с произвольным значением p(H)</p>

<p>На этом графике показана энтропия <a href="http://ru.wikipedia.org/wiki/Распределение_Бернулли">распределения Бернулли</a> в зависимости от его единственного параметра. Это распределение описывает процесс кидания монетки с заданной вероятностью выпадения орла или решки. Обратите внимание, в точках 0 и 1 энтропия равна нулю. Нет никакой неопределенности, всегда выпадает одна и та же сторона монеты. Максимальна же энтропия в точке 0.5, когда выпадение орла и решки равновероятно. Этот пример иллюстрирует одну важную особенность энтропии, чем больше энтропия распределения, тем распределение более равномерно.</p>

<p>Равномерность распределения – очень полезное качество в задачах классификации. Из всех распределений соответствующих эмпирическим данным, <em>следует выбирать распределение обладающее наибольшей равномерностью</em> (и, как следствие, наибольшей энтропией). В противном случае, вы <em>выбираете распределение которое содержит информацию, подтверждения которой нет в обучающем корпусе документов.</em> Именно поэтому maxent-классификатор максимизирует энтропию.</p>

<p>Проиллюстрируем это на примере. Допустим, вы кидаете монету три раза и все три раза она падает орлом вверх. Существуют ли объективные причины подозревать монету в нечестности (p(H)≠p(T))? С наивной точки зрения, вероятность выпадения орла составляет 100%, что является экстремумом нечестности. Но трех бросков определенно недостаточно, чтобы статистическая вероятность события была репрезентативна в отношении истинной вероятности. В статистике эта проблема решается при помощи доверительных интервалов.</p>

<blockquote>
  <p>Доверительный интервал – это интервал оцениваемого параметра, такой, что в него попадает истинное значение параметра с заданной вероятностью, которую называют уровень доверия. Если не указано явно, подразумевается, что доверительный интервал обладает уровнем доверия 0.95.</p>
</blockquote>

<p>Если мы посчитаем доверительный интервал вероятности выпадения решки (здесь я для расчета использую <a href="http://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval#Wilson_score_interval">доверительный интервал Вильсона</a>), то он составит: 0-79%, 0-66% и 0-56% для первого, второго и третьего броска соответственно. Даже после третьего броска диапазон истинной вероятности довольно широкий.</p>

<p class="image"><img src="/images/maxent/entropy-constrained.png" alt="Энтропия произвольной монеты после третьего броска" /></p>

<p class="description">Доверительный интервал после трех бросков монеты</p>

<p>Так какую вероятность нам выбрать для моделирования такой монеты, если результаты 3-х бросков всё что у нас есть? Принцип максимальной энтропии говорит – 0.5. Мы не получили достаточно информации, чтобы утверждать, что целевое распределение не равномерно. А пока таковой нет, мы должны отталкиваться от предположения равномерности истинного распределения.</p>

<h3 id="section-2">Выбор параметров модели</h3>
<p>Последний вопрос, который требует обсуждения заключается в том, как выбрать оптимальный набор весов <script type="math/tex">\lambda</script> для нашей формулы классификации?</p>

<p>Существует несколько способов получения оптимального набора параметров. Один из них – максимизация функции правдоподобия. Для заданного набора параметров модели – это произведение вероятностей всех документов по их классам.</p>

<script type="math/tex; mode=display">\log p(C|D,\lambda)
=\sum_{(c,d)\in(C,D)}\log p(c|d,\lambda)
=\sum_{(c,d)\in(C,D)}\log\frac
{\exp\sum_i^{n \times k}{\lambda_i}f_i\left(c,d\right )}
{\sum_{\tilde{c}\in C}{\exp\sum_i^{n \times k}{\lambda_i}f_i\left(\tilde{c},d\right )}}</script>

<p class="description">Функция условного правдоподобия maxent-классификатора над корпусом документов <script type="math/tex">(C,D)</script></p>

<p>Набор параметров при котором достигается максимум значения этой функции и является оптимальным. Имеем типичную задачу численной оптимизации.</p>

<p>Обычно, для того, чтобы найти максимум функции, оперируют её градиентом. Если градиент известен, можно использовать различные алгоритмы для нахождения максимума, самый простой из которых <a href="http://ru.wikipedia.org/wiki/Градиентный_спуск">градиентный спуск</a>. Сам по себе, алгоритм градиентного спуска довольно прост. Он заключается в итеративном рассчете градиента функции в текущей точке и смещения в направлении градиента на некий шаг. Так до тех пор, пока не будет достигнута цель оптимизации. Полное описание этого алгоритма выходит за пределы заметки.</p>

<blockquote>
  <p><a href="http://ru.wikipedia.org/wiki/Градиент">Градиент</a> – это вектор, указывающий направление в котором функция растет наиболее интенсивно.</p>
</blockquote>

<p class="image"><img src="/images/maxent/gradient-descend-visualized.png" alt="Визуализация произвольной функции методов градиентного спуска" /></p>

<p class="description">Визуализация процесса оптимизации произвольной функции двух аргументов<br />(показаны изолинии функции и апроксимационные шаги алгоритма)</p>

<p>Частные производные функции правдоподобия, которые составляют градиент, выглядят следующим образом:</p>

<script type="math/tex; mode=display">\frac{\partial\log p(C|D,\lambda)}{\partial\lambda_i}=
\underbrace{\sum_{(c,d)\in(C,D)}{f_i(c,d)}}_{empirical \: count}-
\underbrace{\sum_{d\in D}{\sum_{c\in C}{p(c|d,\lambda)f_i(c,d)}}}_{predicted\:count}</script>

<p class="description">Частная производная функции условного правдоподобия</p>

<p>В этой формуле я хотел бы обратить ваше внимание на две группы, выделенные фигурными скобками. Первая – это количество срабатываний классификационного признака на обучающем корпусе (empirical count). Вторая – это предсказанное нашей моделью количество количество срабатываний классификационного признака на том же корпусе (predicted count).</p>

<p>В точке экстремума все частные производные, а следственно и градиент, равны нулю. Таким образом, вся эта нетривиальная математика приводит нас к очень простому выводу: <em>оптимальная модель – это модель, предсказания которой совпадают с эмпирическим количеством срабатываний классификационных признаков в обучающем корпусе документов.</em></p>

<p>В заключение, несколько фактов относительно функции правдоподобия, её градиента и процесса оптимизации:</p>

<ul>
  <li>оптимальное распределение и максимум функции правдоподобия гарантированно существуют;</li>
  <li>у функции правдоподобия один глобальным максимум;</li>
  <li>найденные данным методом параметры описывают распределение с максимальной возможной энтропией.</li>
</ul>

<h2 id="practice">Пример использования OpenNLP Maxent</h2>

<p>Корректная реализация собственного классификатора максимальной энтропии требует определенного опыта в области машинного обучения. Поэтому, если вам надо получить конкретные результаты, то лучше воспользоваться готовыми решениями. Для Java, есть два проекта, которые предлагают готовый инструмент: классификатор максимальной энтропии из состава проекта <a href="http://opennlp.apache.org">OpenNLP</a> и логистическая регрессия в <a href="http://mahout.apache.org">Apache Mahout</a>. Здесь я опишу, как использовать OpenNLP. Исходный код примера доступен на <a href="https://github.com/bazhenov/maxent-example">github</a>.</p>

<p>Проще всего будет тем, кто использует Maven для управления зависимостями. Достаточно прописать следующую зависимость в проекте:</p>

<pre class="code"><code>&lt;dependency&gt;
  &lt;groupId&gt;org.apache.opennlp&lt;/groupId&gt;
  &lt;artifactId&gt;opennlp-maxent&lt;/artifactId&gt;
  &lt;version&gt;3.0.2-incubating&lt;/version&gt;
&lt;/dependency&gt;
</code></pre>

<p>Затем, необходимо реализовать класс, который будет извлекать из текста классификационные признаки в формате OpenNLP. Для этого необходимо реализовать интерфейс <code>EventStream</code>. Этот интерфейс представляет собой итератор по объектам типа <code>Event</code>. Каждый <code>Event</code> является обучающим семплом, который содержит в себе массив классификационных признаков и результирующий класс (context и outcome в терминологии OpenNLP). Классификационный признак – это просто напросто уникальная строка.</p>

<p>Обучение запускается методом <code>GIS.trainModel()</code>, который принимает на вход итератор по <code>Event</code>‘ам, а также некоторые параметры процесса обучения, из которых стоит отметить следующие:</p>

<ul>
  <li><code>iterations</code> — лимит количества итераций процесса обучения;</li>
  <li><code>cutoff</code> – минимальное количество срабатываний классификационного признака, при котором признак допускается к обучению. Признаки с частотностью меньше cutoff, игнорируются;</li>
  <li><code>smoothing</code>— булевый флаг включающий сглаживание. Сглаживание в алгоритмах машинного обучения тема отдельная. Вкратце, сглаживание может дать более качественную модель, но за счет более сложного процесса обучения. До тех пор, пока у вас есть техническая возможность лучше его использовать.</li>
</ul>

<p>Результирующую модель можно сохранить в файл при помощи следующего кода:</p>

<pre class="code"><code>SuffixSensitiveGISModelWriter writer = new SuffixSensitiveGISModelWriter(model, new File("maxent.model"));
writer.persist();
</code></pre>

<p>а прочитать из файла при помощи следующего:</p>

<pre class="code"><code>SuffixSensitiveGISModelReader reader = new SuffixSensitiveGISModelReader(new File("maxent.model"));
reader.getModel();
</code></pre>

<p>Если у вас остались вопросы, оставляйте из в комментариях. Подписывайтесь на <a href="http://feeds.feedburner.com/severe-reality">RSS</a> для того чтобы получать обновления. Если вам понравилась эта заметка, плюсуйте или поделитесь ссылкой в социальных сетях.</p>

<h2 id="section-3">Ссылки по теме</h2>

<ol>
  <li><a href="http://www.kamalnigam.com/papers/maxent-ijcaiws99.pdf">Using Maximum Entropy for Text Classification</a> — Kamal Nigam, John Lafferty, Andrew McCallum;</li>
  <li><a href="http://bulba.sdsu.edu/~malouf/papers/conll02.pdf">A comparison of algorithms for maximum entropy parameter estimation</a> — Robert Malouf;</li>
  <li><a href="http://www.stanford.edu/class/cs124/lec/Maximum_Entropy_Classifiers.pdf">Maxent Models and Discriminative Estimation. Generative vs. Discriminative models</a> — Christopher Manning</li>
</ol>



	<div class="bar">
		<div class="g-plusone" data-size="medium"></div>
		<a href="https://twitter.com/share" class="twitter-share-button"
			data-url="http://bazhenov.me/blog/2013/04/23/maximum-entropy-classifier.html"
			data-via="denis_bazhenov" data-lang="en">Tweet</a>

		<a href="https://twitter.com/denis_bazhenov" class="twitter-follow-button"
			data-show-count="false">Follow</a>
	</div>

	<div id="disqus_thread"></div>
	<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</div>

<!-- Google +1 button -->
<script type="text/javascript">
	(function() {
		var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
		po.src = 'https://apis.google.com/js/plusone.js';
		var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
	})();
</script>

<!-- Twitter buttons -->
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>

<!-- Disqus -->
<script type="text/javascript">
	/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
	var disqus_shortname='severe-reality';
	var disqus_developer=false;

	/* * * DON'T EDIT BELOW THIS LINE * * */
	(function() {
		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	})();
</script>

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
	MathJax.Hub.Config({
		tex2jax: {
			inlineMath: [['$','$'], ['\\(','\\)']],
			processEscapes: true
		}
	});
</script>

		<footer>
			<div class="content">
				<a href="http://feeds.feedburner.com/severe-reality">RSS Subscribe</a>
					|
				<a href="mailto:dotsid@gmail.com">Email</a>
			</div>
		</footer>
					
	</div>
	
	<script src="http://yandex.st/highlightjs/6.1/highlight.min.js"></script>
	<script>
	$(document).ready(function() {
		$('pre.code code').each(function(i, e) {hljs.highlightBlock(e, '  ')});
	});
	</script>
	
	
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-31484732-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

</body>
</html>